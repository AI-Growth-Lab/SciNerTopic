id,doi,title,publication_year,abstract
https://openalex.org/W2962739339,https://doi.org/10.18653/v1/n18-1202,Deep Contextualized Word Representations,2018,"We introduce a new type of deep contextualized word representation that models both (1) complex characteristics use (e.g., syntax and semantics), (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our vectors are learned functions the internal states bidirectional language (biLM), which is pre-trained on large text corpus. show representations can be easily added existing significantly improve state art six challenging NLP problems, including question answering, textual entailment sentiment analysis. also present an analysis showing exposing internals network crucial, allowing downstream mix different types semi-supervision signals."
https://openalex.org/W2965373594,https://doi.org/10.48550/arxiv.1907.11692,RoBERTa: A Robustly Optimized BERT Pretraining Approach,2019,"Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training computationally expensive, often done on private datasets of sizes, and, as we will show, hyperparameter choices have impact the final results. We present a replication study BERT (Devlin et al., 2019) that carefully measures many key hyperparameters and training data size. find was significantly undertrained, can match or exceed every published after it. Our best achieves state-of-the-art results GLUE, RACE SQuAD. These highlight importance previously overlooked design choices, raise questions about source recently reported improvements. release our models code."
https://openalex.org/W2493916176,https://doi.org/10.1162/tacl_a_00051,Enriching Word Vectors with Subword Information,2017,"Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is limitation, especially languages with vocabularies and rare words. In this paper, we propose new approach based skipgram model, where represented as bag character n-grams. A representation associated n-gram; words being sum these representations. Our method fast, allowing train quickly allows us compute did not appear in training data. We evaluate our nine different languages, both similarity analogy By comparing recently proposed morphological show vectors achieve state-of-the-art performance"
https://openalex.org/W1902237438,https://doi.org/10.18653/v1/d15-1166,Effective Approaches to Attention-based Neural Machine Translation,2015,"An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation. However, there little work exploring useful architectures for attention-based NMT. This paper examines two simple and effective classes mechanism: a global approach which always attends all words local one that only looks at subset time. We demonstrate effectiveness both approaches WMT tasks between English German in directions. With attention, we achieve significant gain 5.0 BLEU points over non-attentional systems already incorporate known techniques such as dropout. Our ensemble model using different attention yields new state-of-the-art result WMT’15 task with 25.9 points, an improvement 1.0 existing best system backed NMT n-gram reranker. 1"
https://openalex.org/W2962784628,https://doi.org/10.18653/v1/p16-1162,Neural Machine Translation of Rare Words with Subword Units,2016,"Neural machine translation (NMT) models typically operate with a fixed vocabulary, but is an open-vocabulary problem. Previous work addresses the of out-of-vocabulary words by backing off to dictionary. In this paper, we introduce simpler and more effective approach, making NMT model capable encoding rare unknown as sequences subword units. This based on intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds compositional translation), cognates loanwords phonological morphological transformations). We discuss suitability different segmentation techniques, including simple ngram byte pair compression algorithm, empirically show improve over back-off dictionary baseline WMT 15 tasks English!German English!Russian up 1.1 1.3 BLEU, respectively."
https://openalex.org/W1895577753,https://doi.org/10.1109/cvpr.2015.7298935,Show and tell: A neural image caption generator,2015,"Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing. In this paper, we present generative model based on deep recurrent architecture combines recent advances machine translation can be used to generate sentences image. The trained maximize likelihood target description sentence given training Experiments several datasets show accuracy fluency it learns solely from descriptions. Our often quite accurate, which verify both qualitatively quantitatively. For instance, while current state-of-the-art BLEU-1 score (the higher better) Pascal dataset 25, our approach yields 59, compared human performance around 69. We also improvements Flickr30k, 56 66, SBU, 19 28. Lastly, newly released COCO dataset, achieve BLEU-4 27.7, state-of-the-art."
https://openalex.org/W2963748441,https://doi.org/10.18653/v1/d16-1264,"SQuAD: 100,000+ Questions for Machine Comprehension of Text",2016,"We present the Stanford Question Answering Dataset (SQuAD), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on set Wikipedia articles, where answer to each question is segment text from corresponding passage. analyze understand types reasoning required questions, leaning heavily dependency and constituency trees. build strong logistic regression model, which achieves an F1 score 51.0%, significant improvement over simple baseline (20%). However, human performance (86.8%) much higher, indicating that presents good challenge problem for future research. The freely available at https://stanford-qa.com"
https://openalex.org/W1905882502,https://doi.org/10.1109/cvpr.2015.7298932,Deep visual-semantic alignments for generating image descriptions,2015,"We present a model that generates natural language descriptions of images and their regions. Our approach leverages datasets sentence to learn about the inter-modal correspondences between visual data. alignment is based on novel combination Convolutional Neural Networks over image regions, bidirectional Recurrent sentences, structured objective aligns two modalities through multimodal embedding. then describe Multimodal Network architecture uses inferred alignments generate demonstrate our produces state art results in retrieval experiments Flickr8K, Flickr30K MSCOCO datasets. show generated significantly outperform baselines both full new dataset region-level annotations."
https://openalex.org/W2470673105,https://doi.org/10.18653/v1/n16-1174,Hierarchical Attention Networks for Document Classification,2016,"We propose a hierarchical attention network for document classification. Our model has two distinctive characteristics: (i) it structure that mirrors the of documents; (ii) levels mechanisms applied at wordand sentence-level, enabling to attend differentially more and less important content when constructing representation. Experiments conducted on six large scale text classification tasks demonstrate proposed architecture outperform previous methods by substantial margin. Visualization layers illustrates selects qualitatively informative words sentences."
https://openalex.org/W2296283641,https://doi.org/10.18653/v1/n16-1030,Neural Architectures for Named Entity Recognition,2016,"Comunicacio presentada a la 2016 Conference of the North American Chapter Association for Computational Linguistics, celebrada San Diego (CA, EUA) els dies 12 17 de juny 2016."
https://openalex.org/W2277195237,https://doi.org/10.1007/s11263-016-0981-7,Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations,2017,"Despite progress in perceptual tasks such as image classification, computers still perform poorly on cognitive description and question answering. Cognition is core to that involve not just recognizing, but reasoning about our visual world. However, models used tackle the rich content images for are being trained using same datasets designed tasks. To achieve success at tasks, need understand interactions relationships between objects an image. When asked What vehicle person riding?, will identify well riding(man, carriage) pulling(horse, answer correctly riding a horse-drawn carriage. In this paper, we present Visual Genome dataset enable modeling of relationships. We collect dense annotations objects, attributes, within each learn these models. Specifically, contains over 108K where has average $$35$$35 $$26$$26 $$21$$21 pairwise objects. canonicalize relationships, noun phrases region descriptions questions pairs WordNet synsets. Together, represent densest largest descriptions, pairs."
https://openalex.org/W2745461083,https://doi.org/10.1109/cvpr.2018.00636,Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering,2018,"Top-down visual attention mechanisms have been used extensively in image captioning and question answering (VQA) to enable deeper understanding through fine-grained analysis even multiple steps of reasoning. In this work, we propose a combined bottom-up top-down mechanism that enables be calculated at the level objects other salient regions. This is natural basis for considered. Within our approach, (based on Faster R-CNN) proposes regions, each with an associated feature vector, while determines weightings. Applying approach captioning, results MSCOCO test server establish new state-of-the-art task, achieving CIDEr / SPICE BLEU-4 scores 117.9, 21.5 36.9, respectively. Demonstrating broad applicability method, applying same VQA obtain first place 2017 Challenge."
https://openalex.org/W2606974598,https://doi.org/10.18653/v1/p17-1099,Get To The Point: Summarization with Pointer-Generator Networks,2017,"Neural sequence-to-sequence models have provided a viable new approach for abstractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the original text). However, these two shortcomings: liable reproduce factual details inaccurately, tend repeat themselves. In this work we propose novel architecture that augments standard attentional model in orthogonal ways. First, use hybrid pointer-generator network can copy words source via pointing, which aids accurate reproduction of information, while retaining ability produce through generator. Second, coverage keep track what has been summarized, discourages repetition. We apply our CNN / Daily Mail task, outperforming current state-of-the-art by at least 2 ROUGE points."
https://openalex.org/W1933349210,https://doi.org/10.1109/iccv.2015.279,VQA: Visual Question Answering,2015,"We propose the task of free-form and open-ended Visual Question Answering (VQA). Given an image a natural language question about image, is to provide accurate answer. Mirroring real-world scenarios, such as helping visually impaired, both questions answers are open-ended. selectively target different areas including background details underlying context. As result, system that succeeds at VQA typically needs more detailed understanding complex reasoning than producing generic captions. Moreover, amenable automatic evaluation, since many contain only few words or closed set can be provided in multiple-choice format. dataset containing ~0.25M images, ~0.76M questions, ~10M (www.visualqa.org), discuss information it provides. Numerous baselines for compared with human performance."
https://openalex.org/W1840435438,https://doi.org/10.18653/v1/d15-1075,A large annotated corpus for learning natural language inference,2015,"Understanding entailment and contradiction is fundamental to understanding natural language, inference about a valuable testing ground for the development of semantic representations. However, machine learning research in this area has been dramatically limited by lack large-scale resources. To address this, we introduce Stanford Natural Language Inference corpus, new, freely available collection labeled sentence pairs, written humans doing novel grounded task based on image captioning. At 570K it two orders magnitude larger than all other resources its type. This increase scale allows lexicalized classifiers outperform some sophisticated existing models, neural network-based model perform competitively language benchmarks first time."
https://openalex.org/W1552847225,https://doi.org/10.3233/sw-140134,"DBpedia – A large-scale, multilingual knowledge base extracted from Wikipedia",2015,"The DBpedia community project extracts structured, multilingual knowledge from Wikipedia and makes it freely available on the Web using Semantic Linked Data technologies. 111 different language editions of Wikipedia. largest base which is extracted English edition consists over 400 million facts that describe 3.7 things. bases are other 110 together consist 1.46 billion 10 additional maps infoboxes 27 to a single shared ontology consisting 320 classes 1,650 properties. mappings created via world-wide crowd-sourcing effort enable be combined. publishes releases all for download provides SPARQL query access 14 out global network local chapters. In addition regular releases, maintains live updated whenever page in changes. sets RDF links pointing into 30 external data sources thus enables these used with data. Several hundred publish themselves make one central interlinking hubs Open (LOD) cloud. this system report, we give an overview project, including its architecture, technical implementation, maintenance, internationalisation, usage statistics applications."
https://openalex.org/W2963026768,https://doi.org/10.18653/v1/p18-1031,Universal Language Model Fine-tuning for Text Classification,2018,"Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective method that can be applied to any task NLP, introduce techniques are key for fine-tuning a language model. Our significantly outperforms the state-of-the-art on six text classification tasks, reducing error by 18-24% majority of datasets. Furthermore, with only 100 labeled examples, it matches performance scratch times more data. open-source our pretrained models code."
https://openalex.org/W2963846996,https://doi.org/10.18653/v1/n18-1101,A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference,2018,"This paper introduces the Multi-Genre Natural Language Inference (MultiNLI) corpus, a dataset designed for use in development and evaluation of machine learning models sentence understanding. At 433k examples, this resource is one largest corpora available natural language inference (a.k.a. recognizing textual entailment), improving upon resources both its coverage difficulty. MultiNLI accomplishes by offering data from ten distinct genres written spoken English, making it possible to evaluate systems on nearly full complexity language, while supplying an explicit setting evaluating cross-genre domain adaptation. In addition, using existing Stanford NLI corpus shows that represents substantially more difficult task than does despite two showing similar levels inter-annotator agreement."
https://openalex.org/W2963626623,https://doi.org/10.18653/v1/e17-2068,Bag of Tricks for Efficient Text Classification,2017,"This paper explores a simple and efficient baseline for text classification. Our experiments show that our fast classifier fastText is often on par with deep learning classifiers in terms of accuracy, many orders magnitude faster training evaluation. We can train more than one billion words less ten minutes using standard multicore CPU, classify half million sentences among 312K classes minute."
https://openalex.org/W2963355447,https://doi.org/10.3115/v1/p15-1150,Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks,2015,"A Long Short-Term Memory (LSTM) network is a type of recurrent neural architecture which has recently obtained strong results on variety sequence modeling tasks. The only underlying LSTM structure that been explored so far linear chain. However, natural language exhibits syntactic properties would naturally combine words to phrases. We introduce the Tree-LSTM, generalization LSTMs tree-structured topologies. TreeLSTMs outperform all existing systems and baselines two tasks: predicting semantic relatedness sentences (SemEval 2014, Task 1) sentiment classification (Stanford Sentiment Treebank)."
https://openalex.org/W2963216553,https://doi.org/10.18653/v1/p16-1009,Improving Neural Machine Translation Models with Monolingual Data,2016,"Neural Machine Translation (NMT) has obtained state-of-the art performance for several language pairs, while only using parallel data training. Target-side monolingual plays an important role in boosting fluency phrase-based statistical machine translation, and we investigate the use of NMT. In contrast to previous work, which combines NMT models with separately trained models, note that encoder-decoder architectures already have capacity learn same information as a model, explore strategies train without changing neural network architecture. By pairing training automatic back-translation, can treat it additional data, obtain substantial improvements on WMT 15 task English German (+2.8-3.7 BLEU), low-resourced IWSLT 14 Turkish->English (+2.1-3.4 obtaining new state-of-the-art results. We also show fine-tuning in-domain gives English->German."
https://openalex.org/W2996428491,https://doi.org/10.48550/arxiv.1909.11942,"ALBERT: A Lite BERT for Self-supervised Learning of Language
  Representations",2020,"Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems, we present two parameter-reduction techniques lower consumption increase the speed of BERT. Comprehensive empirical evidence shows that our proposed methods lead models scale much better compared original We also use a self-supervised loss focuses modeling inter-sentence coherence, show it consistently helps tasks with multi-sentence inputs. As result, best establishes new state-of-the-art GLUE, RACE, \squad benchmarks while having fewer parameters BERT-large. The code pretrained are available https://github.com/google-research/ALBERT."
https://openalex.org/W2911489562,https://doi.org/10.1093/bioinformatics/btz682,BioBERT: a pre-trained biomedical language representation model for biomedical text mining,2019,"Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows. With progress in natural language processing (NLP), extracting valuable information from literature has gained popularity among researchers, and deep learning boosted development effective models. However, directly applying advancements NLP to often yields unsatisfactory results due a word distribution shift general domain corpora corpora. In this article, we investigate how recently introduced pre-trained model BERT can be adapted for We introduce BioBERT (Bidirectional Encoder Representations Transformers Text Mining), which domain-specific representation on large-scale almost same architecture across tasks, largely outperforms previous state-of-the-art models variety tasks when While obtains performance comparable that models, significantly them following three representative tasks: named entity recognition (0.62% F1 score improvement), relation extraction (2.80% improvement) question answering (12.24% MRR improvement). Our analysis show pre-training helps it understand complex texts. make weights freely available at https://github.com/naver/biobert-pretrained, source code fine-tuning https://github.com/dmis-lab/biobert."
https://openalex.org/W2962965405,https://doi.org/10.18653/v1/d15-1044,A Neural Attention Model for Abstractive Sentence Summarization,2015,"Summarization based on text extraction is inherently limited, but generation-style abstractive methods have proven challenging to build. In this work, we propose a fully data-driven approach sentence summarization. Our method utilizes local attention-based model that generates each word of the summary conditioned input sentence. While structurally simple, it can easily be trained end-to-end and scales large amount training data. The shows significant performance gains DUC-2004 shared task compared with several strong baselines."
https://openalex.org/W2963918774,https://doi.org/10.18653/v1/d17-1070,Supervised Learning of Universal Sentence Representations from Natural Language Inference Data,2017,"Many modern NLP systems rely on word embeddings, previously trained in an unsupervised manner large corpora, as base features. Efforts to obtain embeddings for larger chunks of text, such sentences, have however not been so successful. Several attempts at learning representations sentences reached satisfactory enough performance be widely adopted. In this paper, we show how universal sentence using the supervised data Stanford Natural Language Inference datasets can consistently outperform methods like SkipThought vectors a wide range transfer tasks. Much computer vision uses ImageNet features, which then transferred other tasks, our work tends indicate suitability natural language inference Our encoder is publicly available."
https://openalex.org/W2962902328,https://doi.org/10.18653/v1/p16-1101,End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF,2016,"State-of-the-art sequence labeling systems traditionally require large amounts of taskspecific knowledge in the form handcrafted features and data pre-processing. In this paper, we introduce a novel neutral network architecture that benefits from both word- character-level representations automatically, by using combination bidirectional LSTM, CNN CRF. Our system is truly end-to-end, requiring no feature engineering or preprocessing, thus making it applicable to wide range tasks. We evaluate our on two sets for tasks — Penn Treebank WSJ corpus part-of-speech (POS) tagging CoNLL 2003 named entity recognition (NER). obtain state-of-the-art performance datasets 97.55% accuracy POS 91.21% F1 NER."
https://openalex.org/W2963954913,https://doi.org/10.1109/cvpr.2016.10,Stacked Attention Networks for Image Question Answering,2016,"This paper presents stacked attention networks (SANs) that learn to answer natural language questions from images. SANs use semantic representation of a question as query search for the regions in an image are related answer. We argue answering (QA) often requires multiple steps reasoning. Thus, we develop multiple-layer SAN which times infer progressively. Experiments conducted on four QA data sets demonstrate proposed significantly outperform previous state-of-the-art approaches. The visualization layers illustrates progress locates relevant visual clues lead layer-by-layer."
https://openalex.org/W2963012544,https://doi.org/10.48550/arxiv.1509.01626,Character-level Convolutional Networks for Text Classification,2015,"This article offers an empirical exploration on the use of character-level convolutional networks (ConvNets) for text classification. We constructed several large-scale datasets to show that could achieve state-of-the-art or competitive results. Comparisons are offered against traditional models such as bag words, n-grams and their TFIDF variants, deep learning word-based ConvNets recurrent neural networks."
https://openalex.org/W2963250244,https://doi.org/10.18653/v1/d18-2012,SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing,2018,"This paper describes SentencePiece, a language-independent subword tokenizer and detokenizer designed for Neural-based text processing, including Neural Machine Translation. It provides open-source C++ Python implementations units. While existing segmentation tools assume that the input is pre-tokenized into word sequences, SentencePiece can train models directly from raw sentences, which allows us to make purely end-to-end language independent system. We perform validation experiment of NMT on English-Japanese machine translation, find it possible achieve comparable accuracy direct training sentences. also compare performance with various configurations. available under Apache 2 license at https://github.com/google/sentencepiece."
https://openalex.org/W2964110616,https://doi.org/10.18653/v1/p19-1285,Transformer-XL: Attentive Language Models beyond a Fixed-Length Context,2019,"Transformers have a potential of learning longer-term dependency, but are limited by fixed-length context in the setting language modeling. We propose novel neural architecture Transformer-XL that enables dependency beyond fixed length without disrupting temporal coherence. It consists segment-level recurrence mechanism and positional encoding scheme. Our method not only capturing also resolves fragmentation problem. As result, learns is 80% longer than RNNs 450% vanilla Transformers, achieves better performance on both short long sequences, up to 1,800+ times faster during evaluation. Notably, we improve state-of-the-art results bpc/perplexity 0.99 enwiki8, 1.08 text8, 18.3 WikiText-103, 21.8 One Billion Word, 54.5 Penn Treebank (without finetuning). When trained manages generate reasonably coherent, text articles with thousands tokens. code, pretrained models, hyperparameters available Tensorflow PyTorch."
https://openalex.org/W2963929190,https://doi.org/10.18653/v1/k16-1028,Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond,2016,"In this work, we model abstractive text summarization using Attentional Encoder-Decoder Recurrent Neural Networks, and show that they achieve state-of-the-art performance on two different corpora. We propose several novel models address critical problems in are not adequately modeled by the basic architecture, such as modeling key-words, capturing hierarchy of sentence-to-word structure, emitting words rare or unseen at training time. Our work shows many our proposed contribute to further improvement performance. also a new dataset consisting multi-sentence summaries, establish benchmarks for research."
https://openalex.org/W2963206148,https://doi.org/10.18653/v1/n16-1014,A Diversity-Promoting Objective Function for Neural Conversation Models,2016,"Sequence-to-sequence neural network models for generation of conversational responses tend to generate safe, commonplace (e.g., I don’t know) regardless the input. We suggest that traditional objective function, i.e., likelihood output (response) given input (message) is unsuited response tasks. Instead we propose using Maximum Mutual Information (MMI) as function in models. Experimental results demonstrate proposed MMI produce more diverse, interesting, and appropriate responses, yielding substantive gains BLEU scores on two datasets human evaluations."
https://openalex.org/W2963223306,https://doi.org/10.18653/v1/k16-1002,Generating Sentences from a Continuous Space,2016,"The standard recurrent neural network language model (RNNLM) generates sentences one word at a time and does not work from an explicit global sentence representation. In this work, we introduce study RNN-based variational autoencoder generative that incorporates distributed latent representations of entire sentences. This factorization allows it to explicitly holistic properties such as style, topic, high-level syntactic features. Samples the prior over these remarkably produce diverse well-formed through simple deterministic decoding. By examining paths space, are able generate coherent novel interpolate between known We present techniques for solving difficult learning problem presented by model, demonstrate its effectiveness in imputing missing words, explore many interesting model's negative results on use modeling."
https://openalex.org/W2964288706,,Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs,2015,"Abstract: Deep Convolutional Neural Networks (DCNNs) have recently shown state of the art performance in high level vision tasks, such as image classification and object detection. This work brings together methods from DCNNs probabilistic graphical models for addressing task pixel-level (also called semantic segmentation). We show that responses at final layer are not sufficiently localized accurate segmentation. is due to very invariance properties make good tasks. overcome this poor localization property deep networks by combining DCNN with a fully connected Conditional Random Field (CRF). Qualitatively, our DeepLab system able localize segment boundaries accuracy which beyond previous methods. Quantitatively, method sets new state-of-art PASCAL VOC-2012 segmentation task, reaching 71.6% IOU test set. how these results can be obtained efficiently: Careful network re-purposing novel application 'hole' algorithm wavelet community allow dense computation neural net 8 frames per second on modern GPU."
https://openalex.org/W2963625095,https://doi.org/10.1162/tacl_a_00104,Named Entity Recognition with Bidirectional LSTM-CNNs,2016,"Named entity recognition is a challenging task that has traditionally required large amounts of knowledge in the form feature engineering and lexicons to achieve high performance. In this paper, we present novel neural network architecture automatically detects word- character-level features using hybrid bidirectional LSTM CNN architecture, eliminating need for most engineering. We also propose method encoding partial lexicon matches networks compare it existing approaches. Extensive evaluation shows that, given only tokenized text publicly available word embeddings, our system competitive on CoNLL-2003 dataset surpasses previously reported state art performance OntoNotes 5.0 by 2.13 F1 points. By two constructed from publicly-available sources, establish new with an score 91.62 86.28 OntoNotes, surpassing systems employ heavy engineering, proprietary lexicons, rich linking information."
https://openalex.org/W3035390927,https://doi.org/10.18653/v1/2020.acl-main.747,Unsupervised Cross-lingual Representation Learning at Scale,2020,"This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train Transformer-based masked model on one hundred languages, using more than two terabytes filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms BERT (mBERT) variety benchmarks, including +14.6% average accuracy XNLI, +13% F1 score MLQA, and +2.4% NER. XLM-R performs particularly well low-resource improving 15.7% in XNLI Swahili 11.4% Urdu over previous XLM models. also present detailed empirical analysis the key factors are required achieve these gains, trade-offs between (1) positive capacity dilution (2) high low resource languages scale. Finally, we show, first time, possibility modeling without sacrificing per-language performance; is very competitive with strong monolingual GLUE benchmarks. will make our code publicly available."
https://openalex.org/W2184957013,https://doi.org/10.1609/aaai.v29i1.9491,Learning Entity and Relation Embeddings for Knowledge Graph Completion,2015,"Knowledge graph completion aims to perform link prediction between entities. In this paper, we consider the approach of knowledge embeddings. Recently, models such as TransE and TransH build entity relation embeddings by regarding a translation from head tail entity. We note that these simply put both entities relations within same semantic space. fact, an may have multiple aspects various focus on different entities, which makes common space insufficient for modeling. propose TransR in separate spaces. Afterwards, learn first projecting corresponding then building translations projected experiments, evaluate our three tasks including prediction, triple classification relational fact extraction. Experimental results show significant consistent improvements compared state-of-the-art baselines TransH."
https://openalex.org/W2970641574,https://doi.org/10.18653/v1/d19-1410,Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks,2019,"BERT (Devlin et al., 2018) and RoBERTa (Liu 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes massive computational overhead: Finding most similar pair in collection of 10,000 about 50 million inference computations (~65 hours) with BERT. The construction makes unsuitable for search as well unsupervised clustering. In this publication, we present Sentence-BERT (SBERT), modification pretrained network use siamese triplet structures to derive semantically meaningful sentence embeddings can be compared using cosine-similarity. This reduces effort finding from 65 hours / 5 seconds SBERT, while maintaining accuracy We evaluate SBERT SRoBERTa common STS transfer learning tasks, where outperforms other methods."
https://openalex.org/W2561715562,https://doi.org/10.1109/cvpr.2017.215,CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning,2017,"When building artificial intelligence systems that can reason and answer questions about visual data, we need diagnostic tests to analyze our progress discover short-comings. Existing benchmarks for question answering help, but have strong biases models exploit correctly without reasoning. They also conflate multiple sources of error, making it hard pinpoint model weaknesses. We present a dataset range reasoning abilities. It contains minimal has detailed annotations describing the kind each requires. use this variety modern systems, providing novel insights into their abilities limitations."
https://openalex.org/W2962883855,https://doi.org/10.1609/aaai.v30i1.9883,Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models,2016,"We investigate the task of building open domain, conversational dialogue systems based on large corpora using generative models. Generative models produce system responses that are autonomously generated word-by-word, opening up possibility for realistic, flexible interactions. In support this goal, we extend recently proposed hierarchical recurrent encoder-decoder neural network to and demonstrate model is competitive with state-of-the-art language back-off n-gram limitations similar approaches, show how its performance can be improved by bootstrapping learning from a larger question-answer pair corpus pretrained word embeddings."
https://openalex.org/W1566289585,https://doi.org/10.1109/iccv.2015.11,Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books,2015,"Books are a rich source of both fine-grained information, how character, an object or scene looks like, as well high-level semantics, what someone is thinking, feeling and these states evolve through story. This paper aims to align books their movie releases in order provide descriptive explanations for visual content that go semantically far beyond the captions available current datasets. To movies we exploit neural sentence embedding trained unsupervised way from large corpus books, video-text computing similarities between clips sentences book. We propose context-aware CNN combine information multiple sources. demonstrate good quantitative performance movie/book alignment show several qualitative examples showcase diversity tasks our model can be used for."
https://openalex.org/W2963532001,https://doi.org/10.18653/v1/w18-6319,A Call for Clarity in Reporting BLEU Scores,2018,"The field of machine translation faces an under-recognized problem because inconsistency in the reporting scores from its dominant metric. Although people refer to “the” BLEU score, is fact a parameterized metric whose values can vary wildly with changes these parameters. These parameters are often not reported or hard find, and consequently, between papers cannot be directly compared. I quantify this variation, finding differences as high 1.8 commonly used configurations. main culprit different tokenization normalization schemes applied reference. Pointing success parsing community, suggest researchers settle upon scheme by annual Conference on Machine Translation (WMT), which does allow for user-supplied reference processing, provide new tool, SACREBLEU, facilitate this."
https://openalex.org/W2302086703,https://doi.org/10.1109/cvpr.2016.503,Image Captioning with Semantic Attention,2016,"Automatically generating a natural language description of an image has attracted interests recently both because its importance in practical applications and it connects two major artificial intelligence fields: computer vision processing. Existing approaches are either top-down, which start from gist convert into words, or bottom-up, come up with words describing various aspects then combine them. In this paper, we propose new algorithm that combines through model semantic attention. Our learns to selectively attend concept proposals fuse them hidden states outputs recurrent neural networks. The selection fusion form feedback connecting the top-down bottom-up computation. We evaluate our on public benchmarks: Microsoft COCO Flickr30K. Experimental results show significantly outperforms state-of-the-art consistently across different evaluation metrics."
https://openalex.org/W1615991656,https://doi.org/10.1162/tacl_a_00134,Improving Distributional Similarity with Lessons Learned from Word Embeddings,2015,"Recent trends suggest that neural-network-inspired word embedding models outperform traditional count-based distributional on similarity and analogy detection tasks. We reveal much of the performance gains embeddings are due to certain system design choices hyperparameter optimizations, rather than algorithms themselves. Furthermore, we show these modifications can be transferred models, yielding similar gains. In contrast prior reports, observe mostly local or insignificant differences between methods, with no global advantage any single approach over others."
https://openalex.org/W2560730294,https://doi.org/10.1109/cvpr.2017.670,Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering,2017,"Problems at the intersection of vision and language are significant importance both as challenging research questions for rich set applications they enable. However, inherent structure in our world bias tend to be a simpler signal learning than visual modalities, resulting models that ignore information, leading an inflated sense their capability. We propose counter these priors task Visual Question Answering (VQA) make (the V VQA) matter! Specifically, we balance popular VQA dataset (Antol et al., ICCV 2015) by collecting complementary images such every question balanced is associated with not just single image, but rather pair similar result two different answers question. Our construction more original has approximately twice number image-question pairs. complete publicly available http://visualqa.org/ part 2nd iteration Dataset Challenge (VQA v2.0). further benchmark state-of-art on dataset. All perform significantly worse dataset, suggesting have indeed learned exploit priors. This finding provides first concrete empirical evidence what seems qualitative among practitioners. Finally, data collection protocol identifying enables us develop novel interpretable model, which addition providing answer given (image, question) pair, also counter-example based explanation. it identifies image believes same can help building trust machines users."
https://openalex.org/W2562607067,https://doi.org/10.18653/v1/d16-1058,Attention-based LSTM for Aspect-level Sentiment Classification,2016,"Aspect-level sentiment classification is a fine-grained task in analysis. Since it provides more complete and in-depth results, aspect-level analysis has received much attention these years. In this paper, we reveal that the polarity of sentence not only determined by content but also highly related to concerned aspect. For instance, “The appetizers are ok, service slow.”, for aspect taste, positive while service, negative. Therefore, worthwhile explore connection between an sentence. To end, propose Attention-based Long Short-Term Memory Network classification. The mechanism can concentrate on different parts when aspects taken as input. We experiment SemEval 2014 dataset results show our model achieves state-ofthe-art performance"
https://openalex.org/W1931639407,https://doi.org/10.1109/cvpr.2015.7298754,From captions to visual concepts and back,2015,"This paper presents a novel approach for automatically generating image descriptions: visual detectors, language models, and multimodal similarity models learnt directly from dataset of captions. We use multiple instance learning to train detectors words that commonly occur in captions, including many different parts speech such as nouns, verbs, adjectives. The word detector outputs serve conditional inputs maximum-entropy model. model learns set over 400,000 descriptions capture the statistics usage. global semantics by re-ranking caption candidates using sentence-level features deep Our system is state-of-the-art on official Microsoft COCO benchmark, producing BLEU-4 score 29.1%. When human judges compare captions ones written other people our held-out test set, have equal or better quality 34% time."
https://openalex.org/W2964165364,https://doi.org/10.18653/v1/p16-1154,Incorporating Copying Mechanism in Sequence-to-Sequence Learning,2016,"We address an important problem in sequence-to-sequence (Seq2Seq) learning referred to as copying, which certain segments the input sequence are selectively replicated output sequence. A similar phenomenon is observable human language communication. For example, humans tend repeat entity names or even long phrases conversation. The challenge with regard copying Seq2Seq that new machinery needed decide when perform operation. In this paper, we incorporate into neural network-based and propose a model called CopyNet encoder-decoder structure. can nicely integrate regular way of word generation decoder mechanism choose sub-sequences put them at proper places Our empirical study on both synthetic data sets real world demonstrates efficacy CopyNet. outperform RNN-based remarkable margins text summarization tasks."
https://openalex.org/W2413794162,https://doi.org/10.18653/v1/d16-1244,A Decomposable Attention Model for Natural Language Inference,2016,"We propose a simple neural architecture for natural language inference. Our approach uses attention to decompose the problem into subproblems that can be solved separately, thus making it trivially parallelizable. On Stanford Natural Language Inference (SNLI) dataset, we obtain state-of-the-art results with almost an order of magnitude fewer parameters than previous work and without relying on any word-order information. Adding intra-sentence takes minimum amount account yields further improvements."
https://openalex.org/W2250966211,https://doi.org/10.18653/v1/d15-1167,Document Modeling with Gated Recurrent Neural Network for Sentiment Classification,2015,"Document level sentiment classification remains a challenge: encoding the intrinsic relations between sentences in semantic meaning of document. To address this, we introduce neural network model to learn vector-based document representation unified, bottom-up fashion. The first learns sentence with convolutional or long short-term memory. Afterwards, semantics and their are adaptively encoded gated recurrent network. We conduct on four large-scale review datasets from IMDB Yelp Dataset Challenge. Experimental results show that: (1) our shows superior performances over several state-of-the-art algorithms; (2) dramatically outperforms standard modeling for classification. 1"
https://openalex.org/W2139501017,https://doi.org/10.1109/iccv.2015.515,Sequence to Sequence -- Video to Text,2015,"Real-world videos often have complex dynamics, methods for generating open-domain video descriptions should be senstive to temporal structure and allow both input (sequence of frames) output words) variable length. To approach this problem we propose a novel end-to-end sequence-to-sequence model generate captions videos. For exploit recurrent neural networks, specifically LSTMs, which demonstrated state-of-the-art performance in image caption generation. Our LSTM is trained on video-sentence pairs learns associate sequence frames words order description the event clip. naturally able learn as well generated sentences, i.e. language model. We evaluate several variants our that different visual features standard set YouTube two movie datasets (M-VAD MPII-MD)."
https://openalex.org/W2550821151,https://doi.org/10.1162/tacl_a_00065,Google’s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation,2017,"We propose a simple solution to use single Neural Machine Translation (NMT) model translate between multiple languages. Our requires no changes the architecture from standard NMT system but instead introduces an artificial token at beginning of input sentence specify required target language. Using shared wordpiece vocabulary, our approach enables Multilingual systems using model. On WMT’14 benchmarks, multilingual achieves comparable performance for English→French and surpasses state-of-theart results English→German. Similarly, state-of-the-art French→English German→English on WMT’15 respectively. production corpora, models up twelve language pairs allow better translation many individual pairs. can also learn perform implicit bridging never seen explicitly during training, showing that transfer learning zero-shot is possible neural translation. Finally, we show analyses hints universal interlingua representation in some interesting examples when mixing"
https://openalex.org/W1854884267,https://doi.org/10.1162/coli_a_00237,SimLex-999: Evaluating Semantic Models With (Genuine) Similarity Estimation,2015,"We present SimLex-999, a gold standard resource for evaluating distributional semantic models that improves on existing resources in several important ways. First, contrast to standards such as WordSim-353 and MEN, it explicitly quantifies similarity rather than association or relatedness so pairs of entities are associated but not actually similar (Freud, psychology) have low rating. show that, via this focus similarity, SimLex-999 incentivizes the development with different, arguably wider, range applications those which reflect conceptual association. Second, contains concrete abstract adjective, noun, verb pairs, together an independent rating concreteness (free) strength each pair. This diversity enables fine-grained analyses performance concepts different types, consequently greater insight into how architectures can be improved. Further, unlike evaluations, automatic approaches reached surpassed inter-annotator agreement ceiling, state-of-the-art perform well below ceiling SimLex-999. There is therefore plenty scope quantify future improvements models, guiding next generation representation-learning architectures."
https://openalex.org/W2575842049,https://doi.org/10.1109/cvpr.2017.345,Knowing When to Look: Adaptive Attention via a Visual Sentinel for Image Captioning,2017,"Attention-based neural encoder-decoder frameworks have been widely adopted for image captioning. Most methods force visual attention to be active every generated word. However, the decoder likely requires little no information from predict non-visual words such as and of. Other that may seem can often predicted reliably just language model e.g., sign after behind a red stop or phone following talking on cell. In this paper, we propose novel adaptive with sentinel. At each time step, our decides whether attend (and if so, which regions) The where, in order extract meaningful sequential word generation. We test method COCO captioning 2015 challenge dataset Flickr30K. Our approach sets new state-of-the-art by significant margin."
https://openalex.org/W2970771982,https://doi.org/10.18653/v1/d19-1371,SciBERT: A Pretrained Language Model for Scientific Text,2019,"Obtaining large-scale annotated data for NLP tasks in the scientific domain is challenging and expensive. We release SciBERT, a pretrained language model based on BERT (Devlin et. al., 2018) to address lack of high-quality, labeled data. SciBERT leverages unsupervised pretraining large multi-domain corpus publications improve performance downstream tasks. evaluate suite including sequence tagging, sentence classification dependency parsing, with datasets from variety domains. demonstrate statistically significant improvements over achieve new state-of-the-art results several these The code models are available at https://github.com/allenai/scibert/."
https://openalex.org/W2473555522,https://doi.org/10.18653/v1/n16-2013,Hateful Symbols or Hateful People? Predictive Features for Hate Speech Detection on Twitter,2016,"Hate speech in the form of racist and sexist remarks are a common occurrence on social media. For that reason, many media services address problem identifying hate speech, but definition varies markedly is largely manual effort (BBC, 2015; Lomas, 2015). We provide list criteria founded critical race theory, use them to annotate publicly available corpus more than 16k tweets. analyze impact various extra-linguistic features conjunction with character n-grams for hatespeech detection. also present dictionary based most indicative words our data."
https://openalex.org/W2963969878,https://doi.org/10.18653/v1/d17-1215,Adversarial Examples for Evaluating Reading Comprehension Systems,2017,"Standard accuracy metrics indicate that reading comprehension systems are making rapid progress, but the extent to which these truly understand language remains unclear. To reward with real understanding abilities, we propose an adversarial evaluation scheme for Stanford Question Answering Dataset (SQuAD). Our method tests whether can answer questions about paragraphs contain adversarially inserted sentences, automatically generated distract computer without changing correct or misleading humans. In this setting, of sixteen published models drops from average 75% F1 score 36%; when adversary is allowed add ungrammatical sequences words, on four decreases further 7%. We hope our insights will motivate development new more precisely."
https://openalex.org/W1922126009,https://doi.org/10.1007/s11263-015-0823-z,Reading Text in the Wild with Convolutional Neural Networks,2016,"In this work we present an end-to-end system for text spotting--localising and recognising in natural scene images--and based image retrieval. This is on a region proposal mechanism detection deep convolutional neural networks recognition. Our pipeline uses novel combination of complementary generation techniques to ensure high recall, fast subsequent filtering stage improving precision. For the recognition ranking proposals, train very large perform word whole at same time, departing from character classifier systems past. These are trained solely data produced by synthetic engine, requiring no human labelled data. Analysing stages our pipeline, show state-of-the-art performance throughout. We rigorous experiments across number standard spotting benchmarks text-based retrieval datasets, showing improvement over all previous methods. Finally, demonstrate real-world application allow thousands hours news footage be instantly searchable via query."
https://openalex.org/W3104033643,https://doi.org/10.18653/v1/s17-2001,SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation,2017,"Semantic Textual Similarity (STS) measures the meaning similarity of sentences. Applications include machine translation (MT), summarization, generation, question answering (QA), short answer grading, semantic search, dialog and conversational systems. The STS shared task is a venue for assessing current state-of-the-art. 2017 focuses on multilingual cross-lingual pairs with one sub-track exploring MT quality estimation (MTQE) data. obtained strong participation from 31 teams, 17 participating in all language tracks. We summarize performance review selection well performing methods. Analysis highlights common errors, providing insight into limitations existing models. To support ongoing work representations, Benchmark introduced as new training evaluation set carefully selected corpus English data (2012-2017)."
https://openalex.org/W2344975321,https://doi.org/10.1038/nature17637,Natural speech reveals the semantic maps that tile human cerebral cortex,2016,"The meaning of language is represented in regions the cerebral cortex collectively known as 'semantic system'. However, little semantic system has been mapped comprehensively, and selectivity most unknown. Here we systematically map across using voxel-wise modelling functional MRI (fMRI) data collected while subjects listened to hours narrative stories. We show that organized into intricate patterns seem be consistent individuals. then use a novel generative model create detailed atlas. Our results suggest areas within represent information about specific domains, or groups related concepts, our atlas shows which domains are each area. This study demonstrates data-driven methods--commonplace studies human neuroanatomy connectivity--provide powerful efficient means for mapping representations brain."
https://openalex.org/W2963758027,https://doi.org/10.1109/cvpr.2016.494,DenseCap: Fully Convolutional Localization Networks for Dense Captioning,2016,"We introduce the dense captioning task, which requires a computer vision system to both localize and describe salient regions in images natural language. The task generalizes object detection when descriptions consist of single word, Image Captioning one predicted region covers full image. To address localization description jointly we propose Fully Convolutional Localization Network (FCLN) architecture that processes an image with single, efficient forward pass, no external proposals, can be trained end-to-end round optimization. is composed Network, novel layer, Recurrent Neural language model generates label sequences. evaluate our network on Visual Genome dataset, comprises 94,000 4,100,000 region-grounded captions. observe speed accuracy improvements over baselines based current state art approaches generation retrieval settings."
https://openalex.org/W2963506925,https://doi.org/10.18653/v1/w17-3204,Six Challenges for Neural Machine Translation,2017,"We explore six challenges for neural machine translation: domain mismatch, amount of training data, rare words, long sentences, word alignment, and beam search. show both deficiencies improvements over the quality phrase-based statistical translation."
https://openalex.org/W2912924812,https://doi.org/10.1162/tacl_a_00276,Natural Questions: A Benchmark for Question Answering Research,2019,"We present the Natural Questions corpus, a question answering data set. consist of real anonymized, aggregated queries issued to Google search engine. An annotator is presented with along Wikipedia page from top 5 results, and annotates long answer (typically paragraph) short (one or more entities) if on page, marks null no long/short present. The public release consists 307,373 training examples single annotations; 7,830 5-way annotations for development data; further 7,842 annotated sequestered as test data. experiments validating quality also describe analysis 25-way 302 examples, giving insights into human variability annotation task. introduce robust metrics purposes evaluating systems; demonstrate high upper bounds these metrics; establish baseline results using competitive methods drawn related literature."
https://openalex.org/W2963383024,https://doi.org/10.18653/v1/d16-1044,Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding,2016,"Modeling textual or visual information with vector representations trained from large language datasets has been successfully explored in recent years. However, tasks such as question answering require combining these each other. Approaches to multimodal pooling include element-wise product sum, well concatenation of the and representations. We hypothesize that methods are not expressive an outer vectors. As is typically infeasible due its high dimensionality, we instead propose utilizing Multimodal Compact Bilinear (MCB) efficiently expressively combine features. extensively evaluate MCB on grounding tasks. consistently show benefit over ablations without MCB. For answering, present architecture which uses twice, once for predicting attention spatial features again attended representation representation. This model outperforms state-of-the-art Visual7W dataset VQA challenge."
https://openalex.org/W2963966654,https://doi.org/10.1109/cvpr.2018.00143,AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks,2018,"In this paper, we propose an Attentional Generative Adversarial Network (AttnGAN) that allows attention-driven, multi-stage refinement for fine-grained text-to-image generation. With a novel attentional generative network, the AttnGAN can synthesize details at different subregions of image by paying attentions to relevant words in natural language description. addition, deep multimodal similarity model is proposed compute image-text matching loss training generator. The significantly outperforms previous state art, boosting best reported inception score 14.14% on CUB dataset and 170.25% more challenging COCO dataset. A detailed analysis also performed visualizing attention layers AttnGAN. It first time shows layered GAN able automatically select condition word level generating parts image."
https://openalex.org/W2963339397,https://doi.org/10.18653/v1/p17-1147,TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension,2017,"We present TriviaQA, a challenging reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents, six per question on average, that provide high quality distant supervision for answering the questions. show that, in comparison to other recently introduced large-scale datasets, (1) has relatively complex, compositional questions, (2) considerable syntactic lexical variability between questions corresponding answer-evidence sentences, (3) requires more cross sentence reasoning find answers. also two baseline algorithms: feature-based classifier state-of-the-art neural network, performs well SQuAD comprehension. Neither approach comes close human performance (23% 40% vs. 80%), suggesting is testbed worth significant future study."
https://openalex.org/W2963963856,https://doi.org/10.3115/v1/p15-1152,Neural Responding Machine for Short-Text Conversation,2015,"We propose Neural Responding Machine (NRM), a neural network-based response generator for Short-Text Conversation. NRM takes the general encoder-decoder framework: it formalizes generation of as decoding process based on latent representation input text, while both encoding and are realized with recurrent networks (RNN). The is trained large amount one-round conversation data collected from microblogging service. Empirical study shows that can generate grammatically correct content-wise appropriate responses to over 75% outperforming state-of-the-arts in same setting, including retrieval-based SMT-based models."
https://openalex.org/W3011411500,https://doi.org/10.1162/tacl_a_00300,SpanBERT: Improving Pre-training by Representing and Predicting Spans,2020,"We present SpanBERT, a pre-training method that is designed to better represent and predict spans of text. Our approach extends BERT by (1) masking contiguous random spans, rather than tokens, (2) training the span boundary representations entire content masked span, without relying on individual token within it. SpanBERT consistently outperforms our better-tuned baselines, with substantial gains selection tasks such as question answering coreference resolution. In particular, same data model size large , single obtains 94.6% 88.7% F1 SQuAD 1.1 2.0 respectively. also achieve new state art OntoNotes resolution task (79.6% F1), strong performance TACRED relation extraction benchmark, even GLUE. 1"
https://openalex.org/W2970231061,https://doi.org/10.18653/v1/d19-1514,LXMERT: Learning Cross-Modality Encoder Representations from Transformers,2019,"Vision-and-language reasoning requires an understanding of visual concepts, language semantics, and, most importantly, the alignment and relationships between these two modalities. We thus propose LXMERT (Learning Cross-Modality Encoder Representations from Transformers) framework to learn vision-and-language connections. In LXMERT, we build a large-scale Transformer model that consists three encoders: object relationship encoder, cross-modality encoder. Next, endow our with capability connecting vision pre-train large amounts image-and-sentence pairs, via five diverse representative pre-training tasks: masked modeling, prediction (feature regression label classification), matching, image question answering. These tasks help in learning both intra-modality relationships. After fine-tuning pre-trained parameters, achieves state-of-the-art results on answering datasets (i.e., VQA GQA). also show generalizability by adapting it challenging visual-reasoning task, NLVR2, improve previous best result 22% absolute (54% 76%). Lastly, demonstrate detailed ablation studies prove novel components strategies significantly contribute strong results. Code models publicly available at: https://github.com/airsplay/lxmert"
https://openalex.org/W2962826786,https://doi.org/10.1109/icassp.2016.7472618,End-to-end attention-based large vocabulary speech recognition,2016,"Many state-of-the-art Large Vocabulary Continuous Speech Recognition (LVCSR) Systems are hybrids of neural networks and Hidden Markov Models (HMMs). Recently, more direct end-to-end methods have been investigated, in which architectures were trained to model sequences characters [1,2]. To our knowledge, all these approaches relied on Connectionist Temporal Classification [3] modules. We investigate an alternative method for sequence modelling based attention mechanism that allows a Recurrent Neural Network (RNN) learn alignments between input frames output labels. show how this setup can be applied LVCSR by integrating the decoding RNN with n-gram language speeding up its operation constraining selections made reducing source lengths pooling information over time. accuracies similar other HMM-free RNN-based reported Wall Street Journal corpus."
https://openalex.org/W2963609956,https://doi.org/10.21437/interspeech.2017-1452,Tacotron: Towards End-to-End Speech Synthesis,2017,"A text-to-speech synthesis system typically consists of multiple stages, such as a text analysis frontend, an acoustic model and audio module. Building these components often requires extensive domain expertise may contain brittle design choices. In this paper, we present Tacotron, end-to-end generative that synthesizes speech directly from characters. Given <text, audio> pairs, the can be trained completely scratch with random initialization. We several key techniques to make sequence-to-sequence framework perform well for challenging task. Tacotron achieves 3.82 subjective 5-scale mean opinion score on US English, outperforming production parametric in terms naturalness. addition, since generates at frame level, it's substantially faster than sample-level autoregressive methods."
https://openalex.org/W2265846598,https://doi.org/10.1609/aaai.v29i1.9513,Recurrent Convolutional Neural Networks for Text Classification,2015,"Text classification is a foundational task in many NLP applications. Traditional text classifiers often rely on human-designed features, such as dictionaries, knowledge bases and special tree kernels. In contrast to traditional methods, we introduce recurrent convolutional neural network for without features. our model, apply structure capture contextual information far possible when learning word representations, which may considerably less noise compared window-based networks. We also employ max-pooling layer that automatically judges words play key roles the components texts. conduct experiments four commonly used datasets. The experimental results show proposed method outperforms state-of-the-art methods several datasets, particularly document-level"
https://openalex.org/W1521843029,https://doi.org/10.37052/ml.31(1)no1,What's In A Name? Malay Seals As Onomastic Sources,2018,"Most studies of Malay names to date have been based on ethnographic and literary sources. This article presents a new dataset for onomastics, namely Islamic seals from Southeast Asia, inscribed in Arabic script dating the late 16th early 20th century, over half which bear personal name. A high proportion these are sovereigns, thus an exceptionally valuable primary source regnal names. Yet while texts chronicles use is generally avoided favour kinship terms, relational names, titles descriptive epithets, almost invariably with standard Arabic-Islamic feature should be interpreted context image self sealholder wished project outside world, his or her identity, membership universal ummah, was prime importance. Keywords: onomastic, seals, Abstrak Kebanyakan kajian tentang nama Melayu setakat ini berdasarkan sumber daripada bidang etnografi atau persuratan. Makalah mempersembahkan baharu untuk onomastik Melayu, iaitu cap mohor Islam dari Asia Tenggara yang ditulis dalam tulisan Arab, pada abad ke-16 hingga ke-20. Hampir separuh tersebut mencatatkan perseorangan. Kebanyakannya milik sultan dan raja, menjadi amat berharga pemerintah. Dalam hikayat riwayat penggunaan perseorangan biasanya dielakkan, sedangkan digunakan ialah menjelaskan hubungan kekeluargaan, persaudaraan, pangkat, timang- timangan, tetapi pula sentiasa dengan Arab-Islam. Ciri diinterpretasikan konteks imej diri ingin ditampilkan oleh pemilik kepada dunia, sebagai ahli umat Islam, merupakan ciri utama identitinya Kata kunci: onomastik, pemerintah,"
https://openalex.org/W2890904471,https://doi.org/10.1016/j.ifacol.2018.08.474,Digital Twin in manufacturing: A categorical literature review and classification,2018,"Abstract The Digital Twin (DT) is commonly known as a key enabler for the digital transformation, however, in literature no common understanding concerning this term. It used slightly different over disparate disciplines. aim of paper to provide categorical review DT manufacturing and classify existing publication according their level integration DT. Therefore, it distinct between Model (DM), Shadow (DS) Twin. results are showing, that highest development stage, DT, scarce, whilst there more about DM DS."
https://openalex.org/W2963925437,https://doi.org/10.18653/v1/n18-2074,Self-Attention with Relative Position Representations,2018,"Relying entirely on an attention mechanism, the Transformer introduced by Vaswani et al. (2017) achieves state-of-the-art results for machine translation. In contrast to recurrent and convolutional neural networks, it does not explicitly model relative or absolute position information in its structure. Instead, requires adding representations of positions inputs. this work we present alternative approach, extending self-attention mechanism efficiently consider positions, distances between sequence elements. On WMT 2014 English-to-German English-to-French translation tasks, approach yields improvements 1.3 BLEU 0.3 over representations, respectively. Notably, observe that combining no further improvement quality. We describe efficient implementation our method cast as instance relation-aware mechanisms can generalize arbitrary graph-labeled"
https://openalex.org/W2972324944,https://doi.org/10.18653/v1/w19-4828,What Does BERT Look at? An Analysis of BERT’s Attention,2019,"Large pre-trained neural networks such as BERT have had great recent success in NLP, motivating a growing body of research investigating what aspects language they are able to learn from unlabeled data. Most analysis has focused on model outputs (e.g., surprisal) or internal vector representations probing classifiers). Complementary these works, we propose methods for analyzing the attention mechanisms models and apply them BERT. BERT’s heads exhibit patterns attending delimiter tokens, specific positional offsets, broadly over whole sentence, with same layer often exhibiting similar behaviors. We further show that certain correspond well linguistic notions syntax coreference. For example, find attend direct objects verbs, determiners nouns, prepositions, coreferent mentions remarkably high accuracy. Lastly, an attention-based classifier use it demonstrate substantial syntactic information is captured attention."
https://openalex.org/W2963749936,https://doi.org/10.1109/cvpr.2017.354,Network Dissection: Quantifying Interpretability of Deep Visual Representations,2017,"We propose a general framework called Network Dissection for quantifying the interpretability of latent representations CNNs by evaluating alignment between individual hidden units and set semantic concepts. Given any CNN model, proposed method draws on data concepts to score semantics at each intermediate convolutional layer. The with are labeled across broad range visual including objects, parts, scenes, textures, materials, colors. use test hypothesis that is an axis-independent property representation space, then we apply compare various networks when trained solve different classification problems. further analyze effect training iterations, initializations, measure dropout batch normalization deep representations. demonstrate can shed light characteristics models methods go beyond measurements their discriminative power."
https://openalex.org/W3034238904,https://doi.org/10.18653/v1/2020.acl-main.740,Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks,2020,"Language models pretrained on text from a wide variety of sources form the foundation today’s NLP. In light success these broad-coverage models, we investigate whether it is still helpful to tailor model domain target task. We present study across four domains (biomedical and computer science publications, news, reviews) eight classification tasks, showing that second phase pretraining in-domain (domain-adaptive pretraining) leads performance gains, under both high- low-resource settings. Moreover, adapting task’s unlabeled data (task-adaptive improves even after domain-adaptive pretraining. Finally, show task corpus augmented using simple selection strategies an effective alternative, especially when resources for might be unavailable. Overall, consistently find multi-phase adaptive offers large gains in performance."
https://openalex.org/W2964167098,https://doi.org/10.18653/v1/p16-1105,End-to-End Relation Extraction using LSTMs on Sequences and Tree Structures,2016,"We present a novel end-to-end neural model to extract entities and relations between them. Our recurrent network based captures both word sequence dependency tree substructure information by stacking bidirectional treestructured LSTM-RNNs on sequential LSTM-RNNs. This allows our jointly represent with shared parameters in single model. further encourage detection of during training use entity relation extraction via pretraining scheduled sampling. improves over the stateof-the-art feature-based end-toend extraction, achieving 12.1% 5.7% relative error reductions F1score ACE2005 ACE2004, respectively. also show that LSTMRNN compares favorably state-of-the-art CNN (in F1-score) nominal classification (SemEval-2010 Task 8). Finally, we an extensive ablation analysis several components."
https://openalex.org/W4211153246,https://doi.org/10.1103/revmodphys.88.035009,CODATA recommended values of the fundamental physical constants: 2014,2016,This report gives the 2014 self-consistent set of values constants and conversion factors physics chemistry recommended by Committee on Data for Science Technology (CODATA). These are based a least-squares adjustment that takes into account all data available up to 31 December 2014. The may also be found World Wide Web at physics.nist.gov/constants.
https://openalex.org/W2889787757,https://doi.org/10.18653/v1/d18-1259,"HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering",2018,"Existing question answering (QA) datasets fail to train QA systems perform complex reasoning and provide explanations for answers. We introduce HotpotQA, a new dataset with 113k Wikipedia-based question-answer pairs four key features: (1) the questions require finding over multiple supporting documents answer; (2) are diverse not constrained any pre-existing knowledge bases or schemas; (3) we sentence-level facts required reasoning, allowing reason strong supervision explain predictions; (4) offer type of factoid comparison test systems’ ability extract relevant necessary comparison. show that HotpotQA is challenging latest systems, enable models improve performance make explainable predictions."
https://openalex.org/W2740168486,https://doi.org/10.18653/v1/w17-1101,A Survey on Hate Speech Detection using Natural Language Processing,2017,"This paper presents a survey on hate speech detection. Given the steadily growing body of social media content, amount online is also increasing. Due to massive scale web, methods that automatically detect are required. Our describes key areas have been explored recognize these types utterances using natural language processing. We discuss limits those approaches."
https://openalex.org/W2963167310,https://doi.org/10.18653/v1/d16-1127,Deep Reinforcement Learning for Dialogue Generation,2016,"Recent neural models of dialogue generation offer great promise for generating responses conversational agents, but tend to be shortsighted, predicting utterances one at a time while ignoring their influence on future outcomes. Modeling the direction is crucial coherent, interesting dialogues, need which led traditional NLP draw reinforcement learning. In this paper, we show how integrate these goals, applying deep learning model reward in chatbot dialogue. The simulates dialogues between two virtual using policy gradient methods sequences that display three useful properties: informativity, coherence, and ease answering (related forward-looking function). We evaluate our diversity, length as well with human judges, showing proposed algorithm generates more interactive manages foster sustained conversation simulation. This work marks first step towards based long-term success dialogues."
https://openalex.org/W3138819813,https://doi.org/10.1145/3236386.3241340,The Mythos of Model Interpretability,2018,Supervised machine-learning models boast remarkable predictive capabilities. But can you trust your model? Will it work in deployment? What else tell about the world?
https://openalex.org/W2962946486,https://doi.org/10.1609/aaai.v33i01.33017370,Graph Convolutional Networks for Text Classification,2019,"Text classification is an important and classical problem in natural language processing. There have been a number of studies that applied convolutional neural networks (convolution on regular grid, e.g., sequence) to classification. However, only limited explored the more flexible graph non-grid, arbitrary graph) for task. In this work, we propose use text We build single corpus based word co-occurrence document relations, then learn Graph Convolutional Network (Text GCN) corpus. Our GCN initialized with one-hot representation document, it jointly learns embeddings both words documents, as supervised by known class labels documents. experimental results multiple benchmark datasets demonstrate vanilla without any external or knowledge outperforms state-of-the-art methods On other hand, also predictive embeddings. addition, show improvement over comparison become prominent lower percentage training data, suggesting robustness less data"
https://openalex.org/W2964241990,https://doi.org/10.3115/v1/n15-1173,Translating Videos to Natural Language Using Deep Recurrent Neural Networks,2015,"Solving the visual symbol grounding problem has long been a goal of artificial intelligence. The field appears to be advancing closer this with recent breakthroughs in deep learning for natural language static images. In paper, we propose translate videos directly sentences using unified neural network both convolutional and recurrent structure. Described video datasets are scarce, most existing methods have applied toy domains small vocabulary possible words. By transferring knowledge from 1.2M+ images category labels 100,000+ captions, our method is able create sentence descriptions open-domain large vocabularies. We compare approach work generation metrics, subject, verb, object prediction accuracy, human evaluation."
https://openalex.org/W2963042536,https://doi.org/10.1613/jair.4992,A Primer on Neural Network Models for Natural Language Processing,2016,"Over the past few years, neural networks have re-emerged as powerful machine-learning models, yielding state-of-the-art results in fields such image recognition and speech processing. More recently, network models started to be applied also textual natural language signals, again with very promising results. This tutorial surveys from perspective of processing research, an attempt bring natural-language researchers up speed techniques. The covers input encoding for tasks, feed-forward networks, convolutional recurrent recursive well computation graph abstraction automatic gradient computation."
https://openalex.org/W2946417913,https://doi.org/10.18653/v1/p19-1452,BERT Rediscovers the Classical NLP Pipeline,2019,"Pre-trained text encoders have rapidly advanced the state of art on many NLP tasks. We focus one such model, BERT, and aim to quantify where linguistic information is captured within network. find that model represents steps traditional pipeline in an interpretable localizable way, regions responsible for each step appear expected sequence: POS tagging, parsing, NER, semantic roles, then coreference. Qualitative analysis reveals can often does adjust this dynamically, revising lower-level decisions basis disambiguating from higher-level representations."
https://openalex.org/W2100664567,https://doi.org/10.3115/v1/p15-1001,On Using Very Large Target Vocabulary for Neural Machine Translation,2015,"Neural machine translation, a recently proposed approach to translation based purely on neural networks, has shown promising results compared the existing approaches such as phrase-based statistical translation. Despite its recent success, limitation in handling larger vocabulary, training complexity well decoding increase proportionally number of target words. In this paper, we propose method importance sampling that allows us use very large vocabulary without increasing complexity. We show can be efficiently done even with model having by selecting only small subset whole vocabulary. The models trained are empirically found outperform baseline LSTM-based models. Furthermore, when ensemble few vocabularies, achieve state-of-the-art performance (measured BLEU) English!German and almost high English!French system."
https://openalex.org/W2970419734,https://doi.org/10.18653/v1/d19-1387,Text Summarization with Pretrained Encoders,2019,"Bidirectional Encoder Representations from Transformers (BERT) represents the latest incarnation of pretrained language models which have recently advanced a wide range natural processing tasks. In this paper, we showcase how BERT can be usefully applied in text summarization and propose general framework for both extractive abstractive models. We introduce novel document-level encoder based on is able to express semantics document obtain representations its sentences. Our model built top by stacking several inter-sentence Transformer layers. For summarization, new fine-tuning schedule adopts different optimizers decoder as means alleviating mismatch between two (the former while latter not). also demonstrate that two-staged approach further boost quality generated summaries. Experiments three datasets show our achieves state-of-the-art results across board settings."
https://openalex.org/W2979826702,https://doi.org/10.18653/v1/2020.emnlp-demos.6,Transformers: State-of-the-Art Natural Language Processing,2020,"Recent advances in modern Natural Language Processing (NLP) research have
been dominated by the combination of Transfer Learning methods with large-scale
Transformer language models. With them came a paradigm shift NLP the
starting point for training model on downstream task moving from blank
specific to general-purpose pretrained architecture. Still, creating
these models remains an expensive and time-consuming process
restricting use these small sub-set wider NLP
community. In this paper, we present Transformers, library for
state-of-the-art NLP, making developments available community by
gathering state-of-the-art under unified
API together ecosystem libraries, examples, tutorials scripts
targeting many tasks. Transformers features carefully crafted
model implementations high-performance weights two main deep
learning frameworks, PyTorch TensorFlow, while supporting all necessary
tools analyze, evaluate tasks such as
text/token classification, questions answering generation among
others. has gained significant organic traction adoption among
both researcher practitioner communities. We are committed at Hugging
Face pursue efforts develop ambition of
creating standard building systems."
https://openalex.org/W1948566616,https://doi.org/10.18653/v1/d15-1199,Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems,2015,"Natural language generation (NLG) is a critical component of spoken dialogue and it has significant impact both on usability perceived quality. Most NLG systems in common use employ rules heuristics tend to generate rigid stylised responses without the natural variation human language. They are also not easily scaled covering multiple domains languages. This paper presents statistical generator based semantically controlled Long Short-term Memory (LSTM) structure. The LSTM can learn from unaligned data by jointly optimising sentence planning surface realisation using simple cross entropy training criterion, be achieved sampling output candidates. With fewer heuristics, an objective evaluation two differing test showed proposed method improved performance compared previous methods. Human judges scored system higher informativeness naturalness overall preferred other systems."
https://openalex.org/W1518951372,https://doi.org/10.3115/v1/n15-1020,A Neural Network Approach to Context-Sensitive Generation of Conversational Responses,2015,"We present a novel response generation system that can be trained end to on large quantities of unstructured Twitter conversations. A neural network architecture is used address sparsity issues arise when integrating contextual information into classic statistical models, allowing the take account previous dialog utterances. Our dynamic-context generative models show consistent gains over both context-sensitive and non-context-sensitive Machine Translation Information Retrieval baselines."
https://openalex.org/W2964352131,https://doi.org/10.18653/v1/p16-1094,A Persona-Based Neural Conversation Model,2016,"We present persona-based models for handling the issue of speaker consistency in neural response generation. A model encodes personas distributed embeddings that capture individual characteristics such as background information and speaking style. dyadic speaker-addressee captures properties interactions between two interlocutors. Our yield qualitative performance improvements both perplexity BLEU scores over baseline sequence-to-sequence models, with similar gains measured by human judges."
https://openalex.org/W2600702321,https://doi.org/10.18653/v1/d17-1159,Encoding Sentences with Graph Convolutional Networks for Semantic Role Labeling,2017,"Semantic role labeling (SRL) is the task of identifying predicate-argument structure a sentence. It typically regarded as an important step in standard NLP pipeline. As semantic representations are closely related to syntactic ones, we exploit information our model. We propose version graph convolutional networks (GCNs), recent class neural operating on graphs, suited model dependency graphs. GCNs over trees used sentence encoders, producing latent feature words observe that GCN layers complementary LSTM ones: when stack both and layers, obtain substantial improvement already state-of-the-art SRL model, resulting best reported scores benchmark (CoNLL-2009) for Chinese English."
https://openalex.org/W2889326796,https://doi.org/10.18653/v1/d18-1045,Understanding Back-Translation at Scale,2018,"An effective method to improve neural machine translation with monolingual data is augment the parallel training corpus back-translations of target language sentences. This work broadens understanding back-translation and investigates a number methods generate synthetic source We find that in all but resource poor settings obtained via sampling or noised beam outputs are most effective. Our analysis shows noisy gives much stronger signal than generated by greedy search. also compare how compares genuine bitext study various domain effects. Finally, we scale hundreds millions sentences achieve new state art 35 BLEU on WMT’14 English-German test set."
https://openalex.org/W2195506630,https://doi.org/10.1038/nn.4186,Cortical tracking of hierarchical linguistic structures in connected speech,2016,"The most critical attribute of human language is its unbounded combinatorial nature: smaller elements can be combined into larger structures on the basis a grammatical system, resulting in hierarchy linguistic units, such as words, phrases and sentences. Mentally parsing representing structures, however, poses challenges for speech comprehension. In speech, hierarchical do not have boundaries that are clearly defined by acoustic cues must therefore internally incrementally constructed during We found that, listening to connected cortical activity different timescales concurrently tracked time course abstract at levels, Notably, neural tracking was dissociated from encoding predictability incoming words. Our results indicate processing underlies grammar-based internal construction structure."
https://openalex.org/W2211192759,https://doi.org/10.1162/tacl_a_00097,ABCNN: Attention-Based Convolutional Neural Network for Modeling Sentence Pairs,2016,"How to model a pair of sentences is critical issue in many NLP tasks such as answer selection (AS), paraphrase identification (PI) and textual entailment (TE). Most prior work (i) deals with one individual task by fine-tuning specific system; (ii) models each sentence’s representation separately, rarely considering the impact other sentence; or (iii) relies fully on manually designed, task-specific linguistic features. This presents general Attention Based Convolutional Neural Network (ABCNN) for modeling sentences. We make three contributions. The ABCNN can be applied wide variety that require sentence pairs. propose attention schemes integrate mutual influence between into CNNs; thus, takes consideration its counterpart. These interdependent representations are more powerful than isolated representations. ABCNNs achieve state-of-the-art performance AS, PI TE tasks. release code at: https://github.com/yinwenpeng/Answer_Selection ."
https://openalex.org/W2962736243,https://doi.org/10.18653/v1/n18-2017,Annotation Artifacts in Natural Language Inference Data,2018,"Large-scale datasets for natural language inference are created by presenting crowd workers with a sentence (premise), and asking them to generate three new sentences (hypotheses) that it entails, contradicts, or is logically neutral respect to. We show that, in significant portion of such data, this protocol leaves clues make possible identify the label looking only at hypothesis, without observing premise. Specifically, we simple text categorization model can correctly classify hypothesis alone about 67% SNLI (Bowman et. al, 2015) 53% MultiNLI (Williams 2017). Our analysis reveals specific linguistic phenomena as negation vagueness highly correlated certain classes. findings suggest success models date has been overestimated, task remains hard open problem."
https://openalex.org/W2964222246,https://doi.org/10.18653/v1/d17-1018,End-to-end Neural Coreference Resolution,2017,"We introduce the first end-to-end coreference resolution model and show that it significantly outperforms all previous work without using a syntactic parser or hand-engineered mention detector. The key idea is to directly consider spans in document as potential mentions learn distributions over possible antecedents for each. computes span embeddings combine context-dependent boundary representations with head-finding attention mechanism. It trained maximize marginal likelihood of gold antecedent from clusters factored enable aggressive pruning mentions. Experiments demonstrate state-of-the-art performance, gain 1.5 F1 on OntoNotes benchmark by 3.1 5-model ensemble, despite fact this approach be successfully no external resources."
https://openalex.org/W2328886022,https://doi.org/10.48550/arxiv.1603.08023,"How NOT To Evaluate Your Dialogue System: An Empirical Study of
  Unsupervised Evaluation Metrics for Dialogue Response Generation",2016,"We investigate evaluation metrics for dialogue response generation systems where supervised labels, such as task completion, are not available. Recent works in have adopted from machine translation to compare a model's generated single target response. show that these correlate very weakly with human judgements the non-technical Twitter domain, and at all technical Ubuntu domain. provide quantitative qualitative results highlighting specific weaknesses existing metrics, recommendations future development of better automatic systems."
https://openalex.org/W2962985038,https://doi.org/10.18653/v1/p17-1171,Reading Wikipedia to Answer Open-Domain Questions,2017,This paper proposes to tackle open- domain question answering using Wikipedia as the unique knowledge source: answer any factoid is a text span in article. task of machine reading at scale combines challenges document retrieval (finding relevant articles) with that comprehension (identifying spans from those articles). Our approach search component based on bigram hashing and TF-IDF matching multi-layer recurrent neural network model trained detect answers paragraphs. experiments multiple existing QA datasets indicate (1) both modules are highly competitive respect counterparts (2) multitask learning distant supervision their combination an effective complete system this challenging task.
https://openalex.org/W2964223283,https://doi.org/10.1162/tacl_a_00266,CoQA: A Conversational Question Answering Challenge,2019,"Humans gather information through conversations involving a series of interconnected questions and answers. For machines to assist in gathering, it is therefore essential enable them answer conversational questions. We introduce CoQA, novel dataset for building Conversational Question Answering systems. Our contains 127k with answers, obtained from 8k about text passages seven diverse domains. The are conversational, the answers free-form their corresponding evidence highlighted passage. analyze CoQA depth show that have challenging phenomena not present existing reading comprehension datasets (e.g., coreference pragmatic reasoning). evaluate strong dialogue models on CoQA. best system obtains an F1 score 65.4%, which 23.4 points behind human performance (88.8%), indicating there ample room improvement. as challenge community at https://stanfordnlp.github.io/coqa ."
https://openalex.org/W2463955103,https://doi.org/10.1109/tpami.2016.2587640,Show and Tell: Lessons Learned from the 2015 MSCOCO Image Captioning Challenge,2017,"Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing. In this paper, we present generative model based on deep recurrent architecture combines recent advances machine translation can be used to generate sentences image. The trained maximize likelihood target description sentence given training Experiments several datasets show accuracy fluency it learns solely from descriptions. Our often quite accurate, which verify both qualitatively quantitatively. Finally, surge interest task, competition was organized 2015 using newly released COCO dataset. We describe analyze various improvements applied our own baseline resulting performance competition, won ex-aequo with team Microsoft Research, provide open source implementation TensorFlow."
https://openalex.org/W2886641317,https://doi.org/10.18653/v1/p18-1238,"Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning",2018,"We present a new dataset of image caption annotations, Conceptual Captions, which contains an order magnitude more images than the MS-COCO (Lin et al., 2014) and represents wider variety both styles. achieve this by extracting filtering annotations from billions webpages. also quantitative evaluations number captioning models show that model architecture based on Inception-ResNetv2 (Szegedy 2016) for image-feature extraction Transformer (Vaswani 2017) sequence modeling achieves best performance when trained Captions dataset."
https://openalex.org/W2251818205,https://doi.org/10.18653/v1/d15-1237,WikiQA: A Challenge Dataset for Open-Domain Question Answering,2015,"We describe the WIKIQA dataset, a new publicly available set of question and sentence pairs, collected annotated for research on open-domain answering. Most previous work answer selection focuses dataset created using TREC-QA data, which includes editor-generated questions candidate sentences selected by matching content words in question. is constructed more natural process than an order magnitude larger dataset. In addition, also there are no correct sentences, enabling researchers to triggering, critical component any QA system. compare several systems task both datasets performance system problem triggering"
https://openalex.org/W2963854351,https://doi.org/10.18653/v1/p19-1441,Multi-Task Deep Neural Networks for Natural Language Understanding,2019,"In this paper, we present a Multi-Task Deep Neural Network (MT-DNN) for learning representations across multiple natural language understanding (NLU) tasks. MT-DNN not only leverages large amounts of cross-task data, but also benefits from regularization effect that leads to more general in order adapt new tasks and domains. extends the model proposed Liu et al. (2015) by incorporating pre-trained bidirectional transformer model, known as BERT (Devlin al., 2018). obtains state-of-the-art results on ten NLU tasks, including SNLI, SciTail, eight out nine GLUE pushing benchmark 82.7% (2.2% absolute improvement). We demonstrate using SNLI SciTail datasets learned allow domain adaptation with substantially fewer in-domain labels than representations. The code models are publicly available at https://github.com/namisan/mt-dnn."
https://openalex.org/W3034999214,https://doi.org/10.18653/v1/2020.acl-main.703,"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",2020,"We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning model to reconstruct the original text. It uses standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due bidirectional encoder), GPT (with left-to-right decoder), other recent schemes. evaluate number of approaches, finding best performance both randomly shuffling order sentences using novel in-filling scheme, where spans are replaced single mask token. particularly effective when fine tuned generation but also works well comprehension tasks. matches RoBERTa on GLUE SQuAD, achieves new state-of-the-art results range abstractive dialogue, question answering, summarization tasks, gains up 3.5 ROUGE. provides 1.1 BLEU increase over back-translation system translation, only target language pretraining. replicate schemes within framework, understand their effect end-task performance."
https://openalex.org/W2135231474,https://doi.org/10.1109/tpami.2014.2366765,Text Detection and Recognition in Imagery: A Survey,2015,"This paper analyzes, compares, and contrasts technical challenges, methods, the performance of text detection recognition research in color imagery. It summarizes fundamental problems enumerates factors that should be considered when addressing these problems. Existing techniques are categorized as either stepwise or integrated sub-problems highlighted including localization, verification, segmentation recognition. Special issues associated with enhancement degraded processing video text, multi-oriented, perspectively distorted multilingual also addressed. The categories sub-categories illustrated, benchmark datasets enumerated, most representative approaches is compared. review provides a comparison analysis remaining field."
https://openalex.org/W2962854379,https://doi.org/10.18653/v1/w15-4640,The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured Multi-Turn Dialogue Systems,2015,"This paper introduces the Ubuntu Dialogue Corpus, a dataset containing almost 1 million multi-turn dialogues, with total of over 7 utterances and 100 words. provides unique resource for research into building dialogue managers based on neural language models that can make use large amounts unlabeled data. The has both property conversations in Dialog State Tracking Challenge datasets, unstructured nature interactions from microblog services such as Twitter. We also describe two learning architectures suitable analyzing this dataset, provide benchmark performance task selecting best next response."
https://openalex.org/W2891177506,https://doi.org/10.18653/v1/d18-2029,Universal Sentence Encoder for English,2018,"We present easy-to-use TensorFlow Hub sentence embedding models having good task transfer performance. Model variants allow for trade-offs between accuracy and compute resources. report the relationship model complexity, resources, Comparisons are made with baselines without learning to that incorporate word-level transfer. Transfer using sentence-level embeddings is shown outperform often those use only show performance minimal training data obtain encouraging results on word association tests (WEAT) of bias."
https://openalex.org/W2549835527,https://doi.org/10.1162/tacl_a_00115,Assessing the Ability of LSTMs to Learn Syntax-Sensitive Dependencies,2016,"The success of long short-term memory (LSTM) neural networks in language processing is typically attributed to their ability capture long-distance statistical regularities. Linguistic regularities are often sensitive syntactic structure; can such dependencies be captured by LSTMs, which do not have explicit structural representations? We begin addressing this question using number agreement English subject-verb dependencies. probe the architecture’s grammatical competence both training objectives with an target (number prediction, grammaticality judgments) and models. In strongly supervised settings, LSTM achieved very high overall accuracy (less than 1% errors), but errors increased when sequential information conflicted. frequency rose sharply language-modeling setting. conclude that LSTMs a non-trivial amount structure given targeted supervision, stronger architectures may required further reduce errors; furthermore, modeling signal insufficient for capturing syntax-sensitive dependencies, should supplemented more direct supervision if need captured."
https://openalex.org/W2465978385,https://doi.org/10.18653/v1/s16-1002,SemEval-2016 Task 5: Aspect Based Sentiment Analysis,2016,"This paper describes the SemEval 2016 shared task on Aspect Based Sentiment Analysis (ABSA), a continuation of respective tasks 2014 and 2015. In its third year, provided 19 training 20 testing datasets for 8 languages 7 domains, as well common evaluation procedure. From these datasets, 25 were sentence-level 14 text-level ABSA; latter was introduced first time subtask in SemEval. The attracted 245 submissions from 29 teams."
https://openalex.org/W2267186426,https://doi.org/10.18653/v1/d16-1053,Long Short-Term Memory-Networks for Machine Reading,2016,"In this paper we address the question of how to render sequence-level networks better at handling structured input. We propose a machine reading simulator which processes text incrementally from left right and performs shallow reasoning with memory attention. The reader extends Long Short-Term Memory architecture network in place single cell. This enables adaptive usage during recurrence neural attention, offering way weakly induce relations among tokens. system is initially designed process sequence but also demonstrate integrate it an encoder-decoder architecture. Experiments on language modeling, sentiment analysis, natural inference show that our model matches or outperforms state art."
https://openalex.org/W2608787653,https://doi.org/10.18653/v1/p17-1152,Enhanced LSTM for Natural Language Inference,2016,"Reasoning and inference are central to human artificial intelligence. Modeling in language is very challenging. With the availability of large annotated data (Bowman et al., 2015), it has recently become feasible train neural network based models, which have shown be effective. In this paper, we present a new state-of-the-art result, achieving accuracy 88.6% on Stanford Natural Language Inference Dataset. Unlike previous top models that use complicated architectures, first demonstrate carefully designing sequential chain LSTMs can outperform all models. Based this, further show by explicitly considering recursive architectures both local modeling composition, achieve additional improvement. Particularly, incorporating syntactic parsing information contributes our best result---it improves performance even when added already strong model."
https://openalex.org/W2536015822,https://doi.org/10.1145/2983323.2983769,A Deep Relevance Matching Model for Ad-hoc Retrieval,2016,"In recent years, deep neural networks have led to exciting breakthroughs in speech recognition, computer vision, and natural language processing (NLP) tasks. However, there been few positive results of models on ad-hoc retrieval This is partially due the fact that many important characteristics task not well addressed yet. Typically, formalized as a matching problem between two pieces text existing work using models, treated equivalent NLP tasks such paraphrase identification, question answering automatic conversation. we argue mainly about relevance while most concern semantic matching, are some fundamental differences these Successful requires proper handling exact signals, query term importance, diverse requirements. this paper, propose novel model (DRMM) for retrieval. Specifically, our employs joint architecture at level matching. By histogram mapping, feed forward network, gating can effectively deal with three factors mentioned above. Experimental representative benchmark collections show significantly outperform well-known state-of-the-art models."
https://openalex.org/W2963389687,https://doi.org/10.1109/cvpr.2016.541,Learning Deep Structure-Preserving Image-Text Embeddings,2016,This paper proposes a method for learning joint embeddings of images and text using two-branch neural network with multiple layers linear projections followed by nonlinearities. The is trained large margin objective that combines cross-view ranking constraints within-view neighborhood structure preservation inspired metric literature. Extensive experiments show our approach gains significant improvements in accuracy image-to-text text-to-image retrieval. Our achieves new state-of-the-art results on the Flickr30K MSCOCO image-sentence datasets shows promise task phrase localization Entities dataset.
https://openalex.org/W2953356739,https://doi.org/10.18653/v1/p19-1139,ERNIE: Enhanced Language Representation with Informative Entities,2019,"Neural language representation models such as BERT pre-trained on large-scale corpora can well capture rich semantic patterns from plain text, and be fine-tuned to consistently improve the performance of various NLP tasks. However, existing rarely consider incorporating knowledge graphs (KGs), which provide structured facts for better understanding. We argue that informative entities in KGs enhance with external knowledge. In this paper, we utilize both textual train an enhanced model (ERNIE), take full advantage lexical, syntactic, information simultaneously. The experimental results have demonstrated ERNIE achieves significant improvements knowledge-driven tasks, meanwhile is comparable state-of-the-art other common source code paper obtained https://github.com/thunlp/ERNIE."
https://openalex.org/W2963825865,https://doi.org/10.18653/v1/p18-1205,"Personalizing Dialogue Agents: I have a dog, do you have pets too?",2018,"Chit-chat models are known to have several problems: they lack specificity, do not display a consistent personality and often very captivating. In this work we present the task of making chit-chat more engaging by conditioning on profile information. We collect data train (i)condition their given information; (ii) information about person talking to, resulting in improved dialogues, as measured next utterance prediction. Since is initially unknown our model trained engage its partner with personal topics, show dialogue can be used predict interlocutors."
https://openalex.org/W2606964149,https://doi.org/10.18653/v1/d17-1082,RACE: Large-scale ReAding Comprehension Dataset From Examinations,2017,"We present RACE, a new dataset for benchmark evaluation of methods in the reading comprehension task. Collected from English exams middle and high school Chinese students age range between 12 to 18, RACE consists near 28,000 passages 100,000 questions generated by human experts (English instructors), covers variety topics which are carefully designed evaluating students' ability understanding reasoning. In particular, proportion that requires reasoning is much larger than other datasets comprehension, there significant gap performance state-of-the-art models (43%) ceiling (95%). hope this can serve as valuable resource research machine comprehension. The freely available at http://www.cs.cmu.edu/~glai1/data/race/ code https://github.com/qizhex/RACE_AR_baselines."
https://openalex.org/W2963109634,https://doi.org/10.1109/cvpr.2016.9,Generation and Comprehension of Unambiguous Object Descriptions,2016,"We propose a method that can generate an unambiguous description (known as referring expression) of specific object or region in image, and which also comprehend interpret such expression to infer is being described. show our outperforms previous methods descriptions objects without taking into account other potentially ambiguous the scene. Our model inspired by recent successes deep learning for image captioning, but while captioning difficult evaluate, task allows easy objective evaluation. present new large-scale dataset expressions, based on MS-COCO. have released toolbox visualization evaluation, see https://github.com/mjhucla/Google_Refexp_toolbox"
https://openalex.org/W2740747242,https://doi.org/10.18653/v1/p17-1018,Gated Self-Matching Networks for Reading Comprehension and Question Answering,2017,"In this paper, we present the gated self-matching networks for reading comprehension style question answering, which aims to answer questions from a given passage. We first match and passage with attention-based recurrent obtain question-aware representation. Then propose attention mechanism refine representation by matching against itself, effectively encodes information whole finally employ pointer locate positions of answers passages. conduct extensive experiments on SQuAD dataset. The single model achieves 71.3% evaluation metrics exact hidden test set, while ensemble further boosts results 75.9%. At time submission our holds place leaderboard both model."
https://openalex.org/W2916132663,https://doi.org/10.18653/v1/s17-2088,SemEval-2017 Task 4: Sentiment Analysis in Twitter,2017,"This paper describes the fifth year of Sentiment Analysis in Twitter task. SemEval-2017 Task 4 continues with a rerun subtasks SemEval-2016 4, which include identifying overall sentiment tweet, towards topic classification on two-point and five-point ordinal scale, quantification distribution across number tweets: again scale. Compared to 2016, we made two changes: (i) introduced new language, Arabic, for all subtasks, (ii) available information from profiles users who posted target tweets. The task be very popular, total 48 teams participating this year."
https://openalex.org/W2963979492,https://doi.org/10.18653/v1/p18-1007,Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates,2018,"Subword units are an effective way to alleviate the open vocabulary problems in neural machine translation (NMT). While sentences usually converted into unique subword sequences, segmentation is potentially ambiguous and multiple segmentations possible even with same vocabulary. The question addressed this paper whether it harness ambiguity as a noise improve robustness of NMT. We present simple regularization method, regularization, which trains model probabilistically sampled during training. In addition, for better sampling, we propose new algorithm based on unigram language model. experiment corpora report consistent improvements especially low resource out-of-domain settings."
https://openalex.org/W2179376720,https://doi.org/10.1080/23273798.2015.1102299,What do we mean by prediction in language comprehension?,2016,"We consider several key aspects of prediction in language comprehension: its computational nature, the representational level(s) at which we predict, whether use higher level representations to predictively pre-activate lower representations, and 'commit' any way our predictions, beyond pre-activation. argue that bulk behavioral neural evidence suggests predict probabilistically multiple levels grains representation. also can, principle, inferences information levels. suggest degree predictive pre-activation might be a function expected utility prediction, which, turn, may depend on comprehenders' goals their estimates relative reliability prior knowledge bottom-up input. Finally, all these properties understanding can naturally explained productively explored within multi-representational hierarchical actively generative architecture whose goal is infer message intended by producer, predictions play crucial role explaining"
https://openalex.org/W2581637843,https://doi.org/10.18653/v1/d17-1230,Adversarial Learning for Neural Dialogue Generation,2017,"We apply adversarial training to open-domain dialogue generation, a system produce sequences that are indistinguishable from human-generated utterances. cast the task as reinforcement learning problem where we jointly train two systems: generative model response sequences, and discriminator—analagous human evaluator in Turing test— distinguish between dialogues machine-generated ones. In this network approach, outputs discriminator used encourage towards more human-like dialogue. Further, investigate models for evaluation uses success fooling an adversary metric, while avoiding number of potential pitfalls. Experimental results on several metrics, including evaluation, demonstrate adversarially-trained generates higher-quality responses than previous baselines"
https://openalex.org/W2250473257,https://doi.org/10.3115/v1/p15-1162,Deep Unordered Composition Rivals Syntactic Methods for Text Classification,2015,"Many existing deep learning models for natural language processing tasks focus on the compositionality of their inputs, which requires many expensive computations. We present a simple neural network that competes with and, in some cases, outperforms such sentiment analysis and factoid question answering while taking only fraction training time. While our model is syntactically-ignorant, we show significant improvements over previous bag-of-words by deepening applying novel variant dropout. Moreover, performs better than syntactic datasets high variance. makes similar errors to syntactically-aware models, indicating consider, nonlinearly transforming input more important tailoring incorporate word order syntax."
https://openalex.org/W1503259811,https://doi.org/10.3115/v1/n15-1184,Retrofitting Word Vectors to Semantic Lexicons,2015,"Vector space word representations are learned from distributional information of words in large corpora. Although such statistics semantically informative, they disregard the valuable that is contained semantic lexicons as WordNet, FrameNet, and Paraphrase Database. This paper proposes a method for refining vector using relational by encouraging linked to have similar representations, it makes no assumptions about how input vectors were constructed. Evaluated on battery standard lexical evaluation tasks several languages, we obtain substantial improvements starting with variety models. Our refinement outperforms prior techniques incorporating into training algorithms."
https://openalex.org/W2251294039,https://doi.org/10.18653/v1/s15-2082,SemEval-2015 Task 12: Aspect Based Sentiment Analysis,2015,"SemEval-2015 Task 12, a continuation of SemEval-2014 4, aimed to foster research beyond sentenceor text-level sentiment classification towards Aspect Based Sentiment Analysis. The goal is identify opinions expressed about specific entities (e.g., laptops) and their aspects price). task provided manually annotated reviews in three domains (restaurants, laptops hotels), common evaluation procedure. It attracted 93 submissions from 16 teams."
https://openalex.org/W2963691697,https://doi.org/10.18653/v1/w18-2501,AllenNLP: A Deep Semantic Natural Language Processing Platform,2018,"Modern natural language processing (NLP) research requires writing code. Ideally this code would provide a precise definition of the approach, easy repeatability results, and basis for extending research. However, many codebases bury high-level parameters under implementation details, are challenging to run debug, difficult enough extend that they more likely be rewritten. This paper describes AllenNLP, library applying deep learning methods NLP addresses these issues with easy-to-use command-line tools, declarative configuration-driven experiments, modular abstractions. AllenNLP has already increased rate experimentation sharing components at Allen Institute Artificial Intelligence, we working have same impact across field."
https://openalex.org/W2251079237,https://doi.org/10.3115/v1/p15-1128,Semantic Parsing via Staged Query Graph Generation: Question Answering with Knowledge Base,2015,"We propose a novel semantic parsing framework for question answering using knowledge base. define query graph that resembles subgraphs of the base and can be directly mapped to logical form. Semantic is reduced generation, formulated as staged search problem. Unlike traditional approaches, our method leverages in an early stage prune space thus simplifies matching By applying advanced entity linking system deep convolutional neural network model matches questions predicate sequences, outperforms previous methods substantially, achieves F1 measure 52.5% on WEBQUESTIONS dataset."
https://openalex.org/W2519818067,https://doi.org/10.1007/978-3-319-46484-8_4,Detecting Text in Natural Image with Connectionist Text Proposal Network,2016,"We propose a novel Connectionist Text Proposal Network (CTPN) that accurately localizes text lines in natural image. The CTPN detects line sequence of fine-scale proposals directly convolutional feature maps. develop vertical anchor mechanism jointly predicts location and text/non-text score each fixed-width proposal, considerably improving localization accuracy. sequential are naturally connected by recurrent neural network, which is seamlessly incorporated into the resulting an end-to-end trainable model. This allows to explore rich context information image, making it powerful detect extremely ambiguous text. works reliably on multi-scale multi-language without further post-processing, departing from previous bottom-up methods requiring multi-step post filtering. It achieves 0.88 0.61 F-measure ICDAR 2013 2015 benchmarks, surpassing recent results [8, 35] large margin. computationally efficient with 0.14 s/image, using very deep VGG16 model [27]. Online demo available: http://textdet.com/."
https://openalex.org/W2580887223,https://doi.org/10.1016/j.ajhg.2017.01.004,InterVar: Clinical Interpretation of Genetic Variants by the 2015 ACMG-AMP Guidelines,2017,"In 2015, the American College of Medical Genetics and Genomics (ACMG) Association for Molecular Pathology (AMP) published updated standards guidelines clinical interpretation sequence variants with respect to human diseases on basis 28 criteria. However, variability between individual interpreters can be extensive because reasons such as different understandings these lack standard algorithms implementing them, yet computational tools semi-automated variant are not available. To address problems, we propose a suite methods criteria have developed tool called InterVar help reviewers interpret significance variants. take pre-annotated or VCF file input generate automated 18 Furthermore, companion web server, wInterVar, enable user-friendly an step manual adjustment step. These especially useful addressing severe congenital very early-onset developmental disorders high penetrance. Using results from few sequencing studies, demonstrate utility in significantly reducing time"
https://openalex.org/W2962749469,https://doi.org/10.1109/cvpr.2016.540,Visual7W: Grounded Question Answering in Images,2016,"We have seen great progress in basic perceptual tasks such as object recognition and detection. However, AI models still fail to match humans high-level vision due the lack of capacities for deeper reasoning. Recently new task visual question answering (QA) has been proposed evaluate a model's capacity deep image understanding. Previous works established loose, global association between QA sentences images. many questions answers, practice, relate local regions establish semantic link textual descriptions by object-level grounding. It enables type with addition answers used previous work. study grounded setting large collection 7W multiple-choice pairs. Furthermore, we human performance several baseline on tasks. Finally, propose novel LSTM model spatial attention tackle"
https://openalex.org/W2970476646,https://doi.org/10.18653/v1/d19-1250,Language Models as Knowledge Bases?,2019,"Recent progress in pretraining language models on large textual corpora led to a surge of improvements for downstream NLP tasks. Whilst learning linguistic knowledge, these may also be storing relational knowledge present the training data, and able answer queries structured as fill-in-the-blank cloze statements. Language have many advantages over bases: they require no schema engineering, allow practitioners query about an open class relations, are easy extend more human supervision train. We in-depth analysis already (without fine-tuning) wide range state-of-the-art pretrained models. find that (i) without fine-tuning, BERT contains competitive with traditional methods some access oracle (ii) does remarkably well open-domain question answering against supervised baseline, (iii) certain types factual learned much readily than others by standard model approaches. The surprisingly strong ability recall any fine-tuning demonstrates their potential unsupervised QA systems. code reproduce our is available at https://github.com/facebookresearch/LAMA."
https://openalex.org/W2769358515,https://doi.org/10.1073/pnas.1720347115,Word embeddings quantify 100 years of gender and ethnic stereotypes,2018,"Significance Word embeddings are a popular machine-learning method that represents each English word by vector, such the geometry between these vectors captures semantic relations corresponding words. We demonstrate can be used as powerful tool to quantify historical trends and social change. As specific applications, we develop metrics based on characterize how gender stereotypes attitudes toward ethnic minorities in United States evolved during 20th 21st centuries starting from 1910. Our framework opens up fruitful intersection machine learning quantitative science."
https://openalex.org/W2963921497,https://doi.org/10.3115/v1/n15-1011,Effective Use of Word Order for Text Categorization with Convolutional Neural Networks,2015,"Convolutional neural network (CNN) is a that can make use of the internal structure data such as 2D image data. This paper studies CNN on text categorization to exploit 1D (namely, word order) for accurate prediction. Instead using low-dimensional vectors input often done, we directly apply high-dimensional data, which leads learning embedding small regions in classification. In addition straightforward adaptation from text, simple but new variation employs bag-of-word conversion convolution layer proposed. An extension combine multiple layers also explored higher accuracy. The experiments demonstrate effectiveness our approach comparison with state-of-the-art methods."
https://openalex.org/W2251743902,https://doi.org/10.3115/v1/p15-1166,Multi-Task Learning for Multiple Language Translation,2015,"In this paper, we investigate the problem of learning a machine translation model that can simultaneously translate sentences from one source language to multiple target languages. Our solution is inspired by recently proposed neural which generalizes as sequence problem. We extend multi-task framework shares representation and separates modeling different translation. be applied situations where either large amounts parallel data or limited available. Experiments show our able achieve significantly higher quality over individually learned in both on sets publicly"
https://openalex.org/W2557764419,https://doi.org/10.18653/v1/w17-2623,NewsQA: A Machine Comprehension Dataset,2017,"We present NewsQA, a challenging machine comprehension dataset of over 100,000 human-generated question-answer pairs. Crowdworkers supply questions and answers based on set 10,000 news articles from CNN, with consisting spans text in the articles. collect this through four-stage process designed to solicit exploratory that require reasoning. Analysis confirms NewsQA demands abilities beyond simple word matching recognizing textual entailment. measure human performance compare it several strong neural models. The gap between humans machines (13.3% F1) indicates significant progress can be made future research. is freely available online."
https://openalex.org/W2889518897,https://doi.org/10.18653/v1/d18-1443,Bottom-Up Abstractive Summarization,2018,"Neural summarization produces outputs that are fluent and readable, but which can be poor at content selection, for instance often copying full sentences from the source document. This work explores use of data-efficient selectors to over-determine phrases in a document should part summary. We this selector as bottom-up attention step constrain model likely phrases. show approach improves ability compress text, while still generating summaries. two-step process is both simpler higher performing than other end-to-end selection models, leading significant improvements on ROUGE CNN-DM NYT corpus. Furthermore, trained with little 1,000 making it easy transfer summarizer new domain."
https://openalex.org/W2951286828,https://doi.org/10.18653/v1/p19-1334,Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference,2019,"A machine learning system can score well on a given test set by relying heuristics that are effective for frequent example types but break down in more challenging cases. We study this issue within natural language inference (NLI), the task of determining whether one sentence entails another. hypothesize statistical NLI models may adopt three fallible syntactic heuristics: lexical overlap heuristic, subsequence and constituent heuristic. To determine have adopted these heuristics, we introduce controlled evaluation called HANS (Heuristic Analysis Systems), which contains many examples where fail. find trained MNLI, including BERT, state-of-the-art model, perform very poorly HANS, suggesting they indeed heuristics. conclude there is substantial room improvement systems, dataset motivate measure progress area."
https://openalex.org/W1951216520,https://doi.org/10.48550/arxiv.1506.02078,Visualizing and Understanding Recurrent Networks,2015,"Recurrent Neural Networks (RNNs), and specifically a variant with Long Short-Term Memory (LSTM), are enjoying renewed interest as result of successful applications in wide range machine learning problems that involve sequential data. However, while LSTMs provide exceptional results practice, the source their performance limitations remain rather poorly understood. Using character-level language models an interpretable testbed, we aim to bridge this gap by providing analysis representations, predictions error types. In particular, our experiments reveal existence cells keep track long-range dependencies such line lengths, quotes brackets. Moreover, comparative finite horizon n-gram traces LSTM improvements structural dependencies. Finally, remaining errors suggests areas for further study."
https://openalex.org/W2792919287,https://doi.org/10.1038/nmeth.4642,Statistics versus machine learning,2018,"Statistics draws population inferences from a sample, and machine learning finds generalizable predictive patterns."
https://openalex.org/W2963096510,https://doi.org/10.18653/v1/p18-1082,Hierarchical Neural Story Generation,2018,"We explore story generation: creative systems that can build coherent and fluent passages of text about a topic. collect large dataset 300K human-written stories paired with writing prompts from an online forum. Our enables hierarchical generation, where the model first generates premise, then transforms it into passage text. gain further improvements novel form fusion improves relevance to prompt, adding new gated multi-scale self-attention mechanism long-range context. Experiments show over strong baselines on both automated human evaluations. Human judges prefer generated by our approach those non-hierarchical factor two one."
https://openalex.org/W2757541972,https://doi.org/10.18653/v1/d17-1047,Recurrent Attention Network on Memory for Aspect Sentiment Analysis,2017,"We propose a novel framework based on neural networks to identify the sentiment of opinion targets in comment/review. Our adopts multiple-attention mechanism capture features separated by long distance, so that it is more robust against irrelevant information. The results multiple attentions are non-linearly combined with recurrent network, which strengthens expressive power our model for handling complications. weighted-memory not only helps us avoid labor-intensive feature engineering work, but also provides tailor-made memory different sentence. examine merit four datasets: two from SemEval2014, i.e. reviews restaurants and laptops; twitter dataset, testing its performance social media data; Chinese news comment language sensitivity. experimental show consistently outperforms state-of-the-art methods types data."
https://openalex.org/W2479423890,https://doi.org/10.1007/978-3-319-46448-0_51,Visual Relationship Detection with Language Priors,2016,"Visual relationships capture a wide variety of interactions between pairs objects in images (e.g. “man riding bicycle” and pushing bicycle”). Consequently, the set possible is extremely large it difficult to obtain sufficient training examples for all relationships. Because this limitation, previous work on visual relationship detection has concentrated predicting only handful Though most are infrequent, their “man” “bicycle”) predicates “riding” “pushing”) independently occur more frequently. We propose model that uses insight train models individually later combines them together predict multiple per image. improve prior by leveraging language priors from semantic word embeddings finetune likelihood predicted relationship. Our can scale thousands types few examples. Additionally, we localize as bounding boxes further demonstrate understanding content based image retrieval."
https://openalex.org/W2301095666,https://doi.org/10.1162/tacl_a_00101,Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature Representations,2016,"We present a simple and effective scheme for dependency parsing which is based on bidirectional-LSTMs (BiLSTMs). Each sentence token associated with BiLSTM vector representing the in its sentential context, feature vectors are constructed by concatenating few vectors. The trained jointly parser objective, resulting very extractors parsing. demonstrate effectiveness of approach applying it to greedy transition-based as well globally optimized graph-based parser. parsers have architectures, match or surpass state-of-the-art accuracies English Chinese."
https://openalex.org/W1773149199,https://doi.org/10.1109/iccv.2015.303,Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models,2015,"The Flickr30k dataset has become a standard benchmark for sentence-based image description. This paper presents Entities, which augments the 158k captions from with 244k coreference chains linking mentions of same entities in images, as well 276k manually annotated bounding boxes corresponding to each entity. Such annotation is essential continued progress automatic description and grounded language understanding. We present experiments demonstrating usefulness our annotations text-to-image reference resolution, or task localizing textual entity an image, bidirectional image-sentence retrieval. These confirm that we can further improve accuracy state-of-the-art retrieval methods by training explicit region-to-phrase correspondence, but at time, they show accurately inferring this correspondence given caption remains really challenging."
https://openalex.org/W2963088995,https://doi.org/10.18653/v1/d16-1163,Transfer Learning for Low-Resource Neural Machine Translation,2016,"The encoder-decoder framework for neural machine translation (NMT) has been shown effective in large data scenarios, but is much less low-resource languages. We present a transfer learning method that significantly improves Bleu scores across range of Our key idea to first train high-resource language pair (the parent model), then some the learned parameters child model) initialize and constrain training. Using our we improve baseline NMT models by an average 5.6 on four pairs. Ensembling unknown word replacement add another 2 which brings performance close strong syntax based (SBMT) system, exceeding its one pair. Additionally, using model re-scoring, can SBMT system 1.3 Bleu, improving state-of-the-art translation."
https://openalex.org/W2964150020,https://doi.org/10.1145/3290353,code2vec: learning distributed representations of code,2019,"We present a neural model for representing snippets of code as continuous distributed vectors (``code embeddings''). The main idea is to represent snippet single fixed-length vector, which can be used predict semantic properties the snippet. To this end, first decomposed collection paths in its abstract syntax tree. Then, network learns atomic representation each path while simultaneously learning how aggregate set them. demonstrate effectiveness our approach by using it method's name from vector body. evaluate training on dataset 12M methods. show that trained method names files were unobserved during training. Furthermore, we useful capture similarities, combinations, and analogies. A comparison previous techniques over same shows an improvement more than 75%, making successfully based large, cross-project corpus. Our model, visualizations similarities are available interactive online demo at http://code2vec.org. code, data models https://github.com/tech-srl/code2vec."
https://openalex.org/W3034781633,https://doi.org/10.1109/cvpr42600.2020.00674,Self-Supervised Learning of Pretext-Invariant Representations,2020,"The goal of self-supervised learning from images is to construct image representations that are semantically meaningful via pretext tasks do not require semantic annotations. Many lead covariant with transformations. We argue that, instead, ought be invariant under such Specifically, we develop Pretext-Invariant Representation Learning (PIRL, pronounced as `pearl') learns based on tasks. use PIRL a commonly used task involves solving jigsaw puzzles. find substantially improves the quality learned representations. Our approach sets new state-of-the-art in several popular benchmarks for learning. Despite being unsupervised, outperforms supervised pre-training object detection. Altogether, our results demonstrate potential good invariance properties."
https://openalex.org/W2981851019,https://doi.org/10.1109/iccv.2019.00756,VideoBERT: A Joint Model for Video and Language Representation Learning,2019,"Self-supervised learning has become increasingly important to leverage the abundance of unlabeled data available on platforms like YouTube. Whereas most existing approaches learn low-level representations, we propose a joint visual-linguistic model high-level features without any explicit supervision. In particular, inspired by its recent success in language modeling, build upon BERT bidirectional distributions over sequences visual and linguistic tokens, derived from vector quantization video off-the-shelf speech recognition outputs, respectively. We use VideoBERT numerous tasks, including action classification captioning. show that it can be applied directly open-vocabulary classification, confirm large amounts training cross-modal information are critical performance. Furthermore, outperform state-of-the-art captioning, quantitative results verify learns semantic features."
https://openalex.org/W3103061166,https://doi.org/10.1145/3041021.3054223,Deep Learning for Hate Speech Detection in Tweets,2017,"Hate speech detection on Twitter is critical for applications like controversial event extraction, building AI chatterbots, content recommendation, and sentiment analysis. We define this task as being able to classify a tweet racist, sexist or neither. The complexity of the natural language constructs makes very challenging. perform extensive experiments with multiple deep learning architectures learn semantic word embeddings handle complexity. Our benchmark dataset 16K annotated tweets show that such methods outperform state-of-the-art char/word n-gram by ~18 F1 points."
https://openalex.org/W3104717349,https://doi.org/10.1145/2783258.2783307,PTE,2015,"Unsupervised text embedding methods, such as Skip-gram and Paragraph Vector, have been attracting increasing attention due to their simplicity, scalability, effectiveness. However, comparing sophisticated deep learning architectures convolutional neural networks, these methods usually yield inferior results when applied particular machine tasks. One possible reason is that learn the representation of in a fully unsupervised way, without leveraging labeled information available for task. Although low dimensional representations learned are applicable many different tasks, they not particularly tuned any In this paper, we fill gap by proposing semi-supervised method data, which call \textit{predictive embedding} (PTE). Predictive utilizes both unlabeled data text. The levels word co-occurrence first represented large-scale heterogeneous network, then embedded into space through principled efficient algorithm. This only preserves semantic closeness words documents, but also has strong predictive power Compared recent supervised approaches based on comparable or more effective, much efficient, fewer parameters tune."
https://openalex.org/W3105966348,https://doi.org/10.18653/v1/2020.findings-emnlp.372,TinyBERT: Distilling BERT for Natural Language Understanding,2020,"Language model pre-training, such as BERT, has significantly improved the performances of many natural language processing tasks. However, pre-trained models are usually computationally expensive, so it is difficult to efficiently execute them on resource-restricted devices. To accelerate inference and reduce size while maintaining accuracy, we first propose a novel Transformer distillation method that specially designed for knowledge (KD) Transformer-based models. By leveraging this new KD method, plenty encoded in large “teacher” BERT can be effectively transferred small “student” TinyBERT. Then, introduce two-stage learning framework TinyBERT, which performs at both pre-training task-specific stages. This ensures TinyBERT capture general-domain well BERT. TinyBERT4 with 4 layers empirically effective achieves more than 96.8% performance its teacher BERT-Base GLUE benchmark, being 7.5x smaller 9.4x faster inference. also better 4-layer state-of-the-art baselines distillation, only ~28% parameters ~31% time them. Moreover, TinyBERT6 6 on-par BERT-Base."
https://openalex.org/W1871385855,https://doi.org/10.1016/j.cviu.2016.03.013,Bag of visual words and fusion methods for action recognition: Comprehensive study and good practice,2016,"A Comprehensive study on the BoVW pipeline for action recognition task.An evaluation and a generic analysis 13 encoding methods.Intra-normalization supervector based methods.An three fusion methods.Several good practices of task. Video is one important challenging problems in computer vision research. Bag visual words model (BoVW) with local features has been very popular long time obtained state-of-the-art performance several realistic datasets, such as HMDB51, UCF50, UCF101. general to construct global representation from features, which mainly composed five steps; (i) feature extraction, (ii) pre-processing, (iii) codebook generation, (iv) encoding, (v) pooling normalization. Although many efforts have made each step independently different scenarios, their effects are still unknown. Meanwhile, video data exhibits views patterns , static appearance motion dynamics. Multiple descriptors usually extracted represent these views. Fusing crucial boosting final an system. This paper aims provide comprehensive all steps methods, uncover some produce Specifically, we explore two kinds ten eight normalization strategies, methods. We conclude that every contributing rate improper choice may counteract improvement other steps. Furthermore, our study, propose simple yet effective representation, called hybrid supervector, by exploring complementarity frameworks improved dense trajectories. Using this obtain impressive results datasets; HMDB51 (61.9%), UCF50 (92.3%), UCF101 (87.9%)."
https://openalex.org/W2964204621,https://doi.org/10.18653/v1/p18-1198,What you can cram into a single $&amp;!#* vector: Probing sentence embeddings for linguistic properties,2018,"Although much effort has recently been devoted to training high-quality sentence embeddings, we still have a poor understanding of what they are capturing. Downstream tasks, often based on classification, commonly used evaluate the quality representations. The complexity tasks makes it however difficult infer kind information is present in We introduce here 10 probing designed capture simple linguistic features sentences, and use them study embeddings generated by three different encoders trained eight distinct ways, uncovering intriguing properties both methods."
https://openalex.org/W2307381258,https://doi.org/10.18653/v1/p16-1046,Neural Summarization by Extracting Sentences and Words,2016,Traditional approaches to extractive summarization rely heavily on human-engineered features. In this work we propose a data-driven approach based neural networks and continuous sentence We develop general framework for single-document composed of hierarchical document encoder an attention-based extractor. This architecture allows us different classes models which can extract sentences or words. train our large scale corpora containing hundreds thousands document-summary pairs. Experimental results two datasets demonstrate that obtain comparable the state art without any access linguistic annotation.
https://openalex.org/W2963961878,https://doi.org/10.18653/v1/n18-1074,FEVER: a Large-scale Dataset for Fact Extraction and VERification,2018,"In this paper we introduce a new publicly
available dataset for verification against
textual sources, FEVER: Fact Extraction
and VERification. It consists of 185,445
claims generated by altering sentences extracted
from Wikipedia and subsequently
verified without knowledge the sentence
they were derived from. The
claims are classified as SUPPORTED, REFUTED
or NOTENOUGHINFO annotators
achieving 0.6841 in Fleiss κ. For
the first two classes, annotators also
recorded sentence(s) forming necessary
evidence their judgment. To
characterize challenge dataset
presented, develop pipeline approach
and compare it to suitably designed oracles.
The best accuracy achieve on labeling
a claim accompanied correct
evidence is 31.87%, while if ignore the
evidence 50.91%. Thus believe
that FEVER challenging testbed
that will help stimulate progress claim
verification against textual sources."
https://openalex.org/W2118434577,https://doi.org/10.3115/v1/p15-1002,Addressing the Rare Word Problem in Neural Machine Translation,2015,"Neural Machine Translation (NMT) is a new approach to machine translation that has shown promising results are comparable traditional approaches. A significant weakness in conventional NMT systems their inability correctly translate very rare words: end-to-end NMTs tend have relatively small vocabularies with single unk symbol represents every possible out-of-vocabulary (OOV) word. In this paper, we propose and implement an effective technique address problem. We train system on data augmented by the output of word alignment algorithm, allowing emit, for each OOV target sentence, position its corresponding source sentence. This information later utilized post-processing step translates using dictionary. Our experiments WMT’14 English French task show method provides substantial improvement up 2.8 BLEU points over equivalent does not use technique. With 37.5 points, our first surpass best result achieved contest task."
https://openalex.org/W2250184916,https://doi.org/10.18653/v1/w15-4007,Observed versus latent features for knowledge base and text inference,2015,"In this paper we show the surprising effectiveness of a simple observed features model in comparison to latent feature models on two benchmark knowledge base completion datasets, FB15K and WN18. We also compare more challenging dataset derived from FB15K, additionally coupled with textual mentions web-scale corpus. that is most effective at capturing information present for entity pairs relations, combination combines strengths both types."
https://openalex.org/W2963260202,https://doi.org/10.18653/v1/p16-1008,Modeling Coverage for Neural Machine Translation,2016,"Attention mechanism has enhanced state-of-the-art Neural Machine Translation (NMT) by jointly learning to align and translate. It tends ignore past alignment information, however, which often leads over-translation under-translation. To address this problem, we propose coverage-based NMT in paper. We maintain a coverage vector keep track of the attention history. The is fed model help adjust future attention, lets system consider more about untranslated source words. Experiments show that proposed approach significantly improves both translation quality over standard attention-based NMT."
https://openalex.org/W2552414813,https://doi.org/10.1109/cvpr.2017.181,Simple Does It: Weakly Supervised Instance and Semantic Segmentation,2017,"Semantic labelling and instance segmentation are two tasks that require particularly costly annotations. Starting from weak supervision in the form of bounding box detection annotations, we propose a new approach does not modification training procedure. We show when carefully designing input labels given boxes, even single round is enough to improve over previously reported weakly supervised results. Overall, our reaches ~95% quality fully model, both for semantic segmentation."
https://openalex.org/W2250635077,https://doi.org/10.18653/v1/d15-1174,Representing Text for Joint Embedding of Text and Knowledge Bases,2015,"Models that learn to represent textual and knowledge base relations in the same continuous latent space are able perform joint inferences among two kinds of obtain high accuracy on completion (Riedel et al., 2013). In this paper we propose a model captures compositional structure relations, jointly optimizes entity, base, relation representations. The proposed significantly improves performance over does not share parameters with common sub-structure."
https://openalex.org/W2948947170,https://doi.org/10.18653/v1/p19-1356,What Does BERT Learn about the Structure of Language?,2019,"BERT is a recent language representation model that has surprisingly performed well in diverse understanding benchmarks. This result indicates the possibility networks capture structural information about language. In this work, we provide novel support for claim by performing series of experiments to unpack elements English structure learned BERT. We first show BERT's phrasal captures phrase-level lower layers. also intermediate layers encode rich hierarchy linguistic information, with surface features at bottom, syntactic middle and semantic top. turns out require deeper when long-distance dependency required, e.g.~to track subject-verb agreement. Finally, representations compositional way mimics classical, tree-like structures."
https://openalex.org/W2963916161,https://doi.org/10.1109/iccv.2017.83,Dense-Captioning Events in Videos,2017,"Most natural videos contain numerous events. For example, in a video of “man playing piano”, the might also “another man dancing” or “a crowd clapping”. We introduce task dense-captioning events, which involves both detecting and describing events video. propose new model that is able to identify all single pass while simultaneously detected with language. Our introduces variant an existing proposal module designed capture short as well long span minutes. To dependencies between video, our captioning uses contextual information from past future jointly describe ActivityNet Captions, large-scale benchmark for Captions contains 20k amounting 849 hours 100k total descriptions, each its unique start end time. Finally, we report performances retrieval localization."
https://openalex.org/W2964303116,https://doi.org/10.18653/v1/n19-1112,Linguistic Knowledge and Transferability of Contextual Representations,2019,"Contextual word representations derived from large-scale neural language models are successful across a diverse set of NLP tasks, suggesting that they encode useful and transferable features language. To shed light on the linguistic knowledge capture, we study produced by several recent pretrained contextualizers (variants ELMo, OpenAI transformer model, BERT) with suite sixteen probing tasks. We find linear trained top frozen contextual competitive state-of-the-art task-specific in many cases, but fail tasks requiring fine-grained (e.g., conjunct identification). investigate transferability representations, quantify differences individual layers within contextualizers, especially between recurrent networks (RNNs) transformers. For instance, higher RNNs more task-specific, while do not exhibit same monotonic trend. In addition, to better understand what makes transferable, compare model pretraining eleven supervised any given task, closely related task yields performance than (which is average) when dataset fixed. However, data gives best results."
https://openalex.org/W2888482885,https://doi.org/10.18653/v1/d18-1206,"Don’t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization",2018,"We introduce “extreme summarization”, a new single-document summarization task which does not favor extractive strategies and calls for an abstractive modeling approach. The idea is to create short, one-sentence news summary answering the question “What article about?”. collect real-world, large-scale dataset this by harvesting online articles from British Broadcasting Corporation (BBC). propose novel model conditioned on article’s topics based entirely convolutional neural networks. demonstrate experimentally that architecture captures long-range dependencies in document recognizes pertinent content, outperforming oracle system state-of-the-art approaches when evaluated automatically humans."
https://openalex.org/W2952870794,https://doi.org/10.1038/sdata.2016.25,Extensive sequencing of seven human genomes to characterize benchmark reference materials,2016,"The Genome in a Bottle Consortium, hosted by the National Institute of Standards and Technology (NIST) is creating reference materials data for human genome sequencing, as well methods comparison benchmarking. Here, we describe large, diverse set sequencing seven genomes; five are current or candidate NIST Reference Materials. pilot genome, NA12878, has been released RM 8398. We also from two Personal Project trios, one Ashkenazim Jewish ancestry Chinese ancestry. come 12 technologies: BioNano Genomics, Complete Genomics paired-end LFR, Ion Proton exome, Oxford Nanopore, Pacific Biosciences, SOLiD, 10X GemCode WGS, Illumina exome WGS paired-end, mate-pair, synthetic long reads. Cell lines, DNA, these individuals publicly available. Therefore, expect to be useful revealing novel information about improving technologies, SNP, indel, structural variant calling, de novo assembly."
https://openalex.org/W2887782043,https://doi.org/10.1145/3232676,A Survey on Automatic Detection of Hate Speech in Text,2018,"The scientific study of hate speech, from a computer science point view, is recent. This survey organizes and describes the current state field, providing structured overview previous approaches, including core algorithms, methods, main features used. work also discusses complexity concept defined in many platforms contexts, provides unifying definition. area has an unquestionable potential for societal impact, particularly online communities digital media platforms. development systematization shared resources, such as guidelines, annotated datasets multiple languages, crucial step advancing automatic detection speech."
https://openalex.org/W2963780471,https://doi.org/10.18653/v1/p16-1141,Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change,2016,"Understanding how words change their meanings over time is key to models of language and cultural evolution, but historical data on meaning scarce, making theories hard develop test. Word embeddings show promise as a diachronic tool, have not been carefully evaluated. We robust methodology for quantifying semantic by evaluating word (PPMI, SVD, word2vec) against known changes. then use this reveal statistical laws evolution. Using six corpora spanning four languages two centuries, we propose quantitative change: (i) the law conformity---the rate scales with an inverse power-law frequency; (ii) innovation---independent frequency, that are more polysemous higher rates change."
https://openalex.org/W2605076167,https://doi.org/10.1109/cvpr.2017.371,Detecting Oriented Text in Natural Images by Linking Segments,2017,"Most state-of-the-art text detection methods are specific to horizontal Latin and not fast enough for real-time applications. We introduce Segment Linking (SegLink), an oriented method. The main idea is decompose into two locally detectable elements, namely segments links. A segment box covering a part of word or line; link connects adjacent segments, indicating that they belong the same line. Both elements detected densely at multiple scales by end-to-end trained, fully-convolutional neural network. Final detections produced combining connected Compared with previous methods, SegLink improves along dimensions accuracy, speed, ease training. It achieves f-measure 75.0% on standard ICDAR 2015 Incidental (Challenge 4) benchmark, outperforming best large margin. runs over 20 FPS 512x512 images. Moreover, without modification, able detect long lines non-Latin text, such as Chinese."
https://openalex.org/W2964046515,https://doi.org/10.18653/v1/e17-1104,Very Deep Convolutional Networks for Text Classification,2017,"The dominant approach for many NLP tasks are recurrent neural networks, in particular LSTMs, and convolutional networks. However, these architectures rather shallow comparison to the deep networks which have pushed state-of-the-art computer vision. We present a new architecture (VDCNN) text processing operates directly at character level uses only small convolutions pooling operations. able show that performance of this model increases with depth: using up 29 layers, we report improvements over on several public classification tasks. To best our knowledge, is first time very nets been applied processing."
https://openalex.org/W2953641512,https://doi.org/10.1038/s41586-019-1335-8,Unsupervised word embeddings capture latent knowledge from materials science literature,2019,"The overwhelming majority of scientific knowledge is published as text, which difficult to analyse by either traditional statistical analysis or modern machine learning methods. By contrast, the main source machine-interpretable data for materials research community has come from structured property databases1,2, encompass only a small fraction present in literature. Beyond values, publications contain valuable regarding connections and relationships between items interpreted authors. To improve identification use this knowledge, several studies have focused on retrieval information literature using supervised natural language processing3-10, requires large hand-labelled datasets training. Here we show that science can be efficiently encoded information-dense word embeddings11-13 (vector representations words) without human labelling supervision. Without any explicit insertion chemical these embeddings capture complex concepts such underlying structure periodic table structure-property materials. Furthermore, demonstrate an unsupervised method recommend functional applications years before their discovery. This suggests latent future discoveries extent embedded past publications. Our findings highlight possibility extracting massive body collective manner, point towards generalized approach mining"
https://openalex.org/W2963794306,https://doi.org/10.18653/v1/p16-1004,Language to Logical Form with Neural Attention,2016,"Semantic parsing aims at mapping natural language to machine interpretable meaning representations. Traditional approaches rely on high-quality lexicons, manually-built templates, and linguistic features which are either domain- or representation-specific. In this paper we present a general method based an attention-enhanced encoder-decoder model. We encode input utterances into vector representations, generate their logical forms by conditioning the output sequences trees encoding vectors. Experimental results four datasets show that our approach performs competitively without using hand-engineered is easy adapt across domains"
https://openalex.org/W2962717182,https://doi.org/10.18653/v1/p17-1061,Learning Discourse-level Diversity for Neural Dialog Models using Conditional Variational Autoencoders,2017,"While recent neural encoder-decoder models have shown great promise in modeling open-domain conversations, they often generate dull and generic responses. Unlike past work that has focused on diversifying the output of decoder from word-level to alleviate this problem, we present a novel framework based conditional variational autoencoders capture discourse-level diversity encoder. Our model uses latent variables learn distribution over potential conversational intents generates diverse responses using only greedy decoders. We further developed variant is integrated with linguistic prior knowledge for better performance. Finally, training procedure improved through introducing bag-of-word loss. proposed been validated significantly more than baseline approaches exhibit competence decision-making."
https://openalex.org/W2398118205,https://doi.org/10.1109/cvpr.2016.13,Learning Deep Representations of Fine-Grained Visual Descriptions,2016,"State-of-the-art methods for zero-shot visual recognition formulate learning as a joint embedding problem of images and side information. In these formulations the current best complement to features are attributes: manuallyencoded vectors describing shared characteristics among categories. Despite good performance, attributes have limitations: (1) finer-grained requires commensurately more attributes, (2) do not provide natural language interface. We propose overcome limitations by training neural models from scratch, i.e. without pre-training only consuming words characters. Our proposed train end-to-end align with fine-grained category-specific content images. Natural provides flexible compact way encoding salient aspects distinguishing By on raw text, our model can inference text well, providing humans familiar mode both annotation retrieval. achieves strong performance text-based image retrieval significantly outperforms attribute-based state-of-the-art classification Caltech-UCSD Birds 200-2011 dataset."
https://openalex.org/W3037109418,https://doi.org/10.18653/v1/2020.acl-demos.14,Stanza: A Python Natural Language Processing Toolkit for Many Human Languages,2020,"We introduce Stanza, an open-source Python natural language processing toolkit supporting 66 human languages. Compared to existing widely used toolkits, Stanza features a language-agnostic fully neural pipeline for text analysis, including tokenization, multi-word token expansion, lemmatization, part-of-speech and morphological feature tagging, dependency parsing, named entity recognition. have trained on total of 112 datasets, the Universal Dependencies treebanks other multilingual corpora, show that same architecture generalizes well achieves competitive performance all languages tested. Additionally, includes native interface Java Stanford CoreNLP software, which further extends its functionality cover tasks such as coreference resolution relation extraction. Source code, documentation, pretrained models are available at https://stanfordnlp.github.io/stanza."
https://openalex.org/W1899794420,https://doi.org/10.18653/v1/d15-1176,Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation,2015,"We introduce a model for constructing vector representations of words by composing characters using bidirectional LSTMs. Relative to traditional word representation models that have independent vectors each type, our requires only single per character type and fixed set parameters the compositional model. Despite compactness this and, more importantly, arbitrary nature form-function relationship in language, ""composed"" yield state-of-the-art results language modeling part-of-speech tagging. Benefits over baselines are particularly pronounced morphologically rich languages (e.g., Turkish)."
https://openalex.org/W2597655663,https://doi.org/10.48550/arxiv.1703.03130,A Structured Self-attentive Sentence Embedding,2017,"This paper proposes a new model for extracting an interpretable sentence embedding by introducing self-attention. Instead of using vector, we use 2-D matrix to represent the embedding, with each row attending on different part sentence. We also propose self-attention mechanism and special regularization term model. As side effect, comes easy way visualizing what specific parts are encoded into embedding. evaluate our 3 tasks: author profiling, sentiment classification, textual entailment. Results show that yields significant performance gain compared other methods in all tasks."
https://openalex.org/W2963536419,https://doi.org/10.1109/cvpr.2018.00611,Neural Motifs: Scene Graph Parsing with Global Context,2018,"We investigate the problem of producing structured graph representations visual scenes. Our work analyzes role motifs: regularly appearing substructures in scene graphs. present new quantitative insights on such repeated structures Visual Genome dataset. analysis shows that object labels are highly predictive relation but not vice-versa. also find there recurring patterns even larger subgraphs: more than 50% graphs contain motifs involving at least two relations. motivates a baseline: given detections, predict most frequent between pairs with labels, as seen training set. This baseline improves previous state-of-the-art by an average 3.6% relative improvement across evaluation settings. then introduce Stacked Motif Networks, architecture designed to capture higher order further over our strong 7.1% gain. code is available github.com/rowanz/neural-motifs."
https://openalex.org/W2251913848,https://doi.org/10.3115/v1/p15-1034,Leveraging Linguistic Structure For Open Domain Information Extraction,2015,"Relation triples produced by open domain information extraction (open IE) systems are useful for question answering, inference, and other IE tasks. Traditionally these extracted using a large set of patterns; however, this approach is brittle on out-of-domain text long-range dependencies, gives no insight into the substructure arguments. We replace pattern with few patterns canonically structured sentences, shift focus to classifier which learns extract self-contained clauses from longer sentences. then run natural logic inference over short determine maximally specific arguments each candidate triple. show that our outperforms state-of-the-art system end-to-end TAC-KBP 2013 Slot Filling task."
https://openalex.org/W2552161745,https://doi.org/10.1109/iccv.2017.524,Boosting Image Captioning with Attributes,2017,"Automatically describing an image with a natural language has been emerging challenge in both fields of computer vision and processing. In this paper, we present Long Short-Term Memory Attributes (LSTM-A) - novel architecture that integrates attributes into the successful Convolutional Neural Networks (CNNs) plus Recurrent (RNNs) captioning framework, by training them end-to-end manner. To incorporate attributes, construct variants architectures feeding representations RNNs different ways to explore mutual but also fuzzy relationship between them. Extensive experiments are conducted on COCO dataset our framework achieves superior results when compared state-of-the-art deep models. Most remarkably, obtain METEOR/CIDEr-D 25.2%/98.6% testing data widely used publicly available splits (Karpathy & Fei-Fei, 2015) extracting GoogleNet achieve date top-1 performance Leaderboard."
https://openalex.org/W2250999640,https://doi.org/10.3115/v1/p15-1017,Event Extraction via Dynamic Multi-Pooling Convolutional Neural Networks,2015,"Traditional approaches to the task of ACE event extraction primarily rely on elaborately designed features and complicated natural language processing (NLP) tools. These traditional lack generalization, take a large amount human effort are prone error propagation data sparsity problems. This paper proposes novel event-extraction method, which aims automatically extract lexical-level sentence-level without using NLP We introduce word-representation model capture meaningful semantic regularities for words adopt framework based convolutional neural network (CNN) clues. However, CNN can only most important information in sentence may miss valuable facts when considering multiple-event sentences. propose dynamic multi-pooling (DMCNN), uses layer according triggers arguments, reserve more crucial information. The experimental results show that our approach significantly outperforms other state-of-the-art methods."
https://openalex.org/W2289394825,https://doi.org/10.1109/asru.2015.7404837,"The third ‘CHiME’ speech separation and recognition challenge: Dataset, task and baselines",2015,"The CHiME challenge series aims to advance far field speech recognition technology by promoting research at the interface of signal processing and automatic recognition. This paper presents design outcomes 3rd Challenge, which targets performance in a real-world, commercially-motivated scenario: person talking tablet device that has been fitted with six-channel microphone array. describes data collection, task definition baseline systems for simulation, enhancement then an overview 26 were submitted focusing on strategies proved be most successful relative MVDR array DNN acoustic modeling reference system. Challenge findings related role simulated system training evaluation are discussed."
https://openalex.org/W2964164368,https://doi.org/10.24963/ijcai.2017/568,Interactive Attention Networks for Aspect-Level Sentiment Classification,2017,"Aspect-level sentiment classification aims at identifying the polarity of specific target in its context. Previous approaches have realized importance targets and developed various methods with goal precisely modeling their contexts via generating target-specific representations. However, these studies always ignore separate targets. In this paper, we argue that both deserve special treatment need to be learned own representations interactive learning. Then, propose attention networks (IAN) interactively learn attentions targets, generate for separately. With design, IAN model can well represent a collocative context, which is helpful classification. Experimental results on SemEval 2014 Datasets demonstrate effectiveness our model."
https://openalex.org/W3106003309,https://doi.org/10.18653/v1/d17-1169,"Using millions of emoji occurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm",2017,"NLP tasks are often limited by scarcity of manually annotated data. In social media sentiment analysis and related tasks, researchers have therefore used binarized emoticons specific hashtags as forms distant supervision. Our paper shows that extending the supervision to a more diverse set noisy labels, models can learn richer representations. Through emoji prediction on dataset 1246 million tweets containing one 64 common emojis we obtain state-of-the-art performance 8 benchmark datasets within sentiment, emotion sarcasm detection using single pretrained model. analyses confirm diversity our emotional labels yield improvement over previous approaches."
https://openalex.org/W2481240925,https://doi.org/10.1109/tpami.2016.2598339,Deep Visual-Semantic Alignments for Generating Image Descriptions,2017,"We present a model that generates natural language descriptions of images and their regions. Our approach leverages datasets sentence to learn about the inter-modal correspondences between visual data. alignment is based on novel combination Convolutional Neural Networks over image regions, bidirectional Recurrent (RNN) sentences, structured objective aligns two modalities through multimodal embedding. then describe Multimodal Network architecture uses inferred alignments generate demonstrate our produces state art results in retrieval experiments Flickr8K, Flickr30K MSCOCO datasets. show generated outperform baselines both full new dataset region-level annotations. Finally, we conduct large-scale analysis RNN Visual Genome 4.1 million captions highlight differences caption statistics."
https://openalex.org/W2978670439,https://doi.org/10.1162/tacl_a_00290,Neural Network Acceptability Judgments,2019,"This paper investigates the ability of artificial neural networks to judge grammatical acceptability a sentence, with goal testing their linguistic competence. We introduce Corpus Linguistic Acceptability (CoLA), set 10,657 English sentences labeled as or ungrammatical from published linguistics literature. As baselines, we train several recurrent network models on classification, and find that our outperform unsupervised by Lau et al. (2016) CoLA. Error-analysis specific phenomena reveals both al.’s ours learn systematic generalizations like subject-verb-object order. However, all test perform far below human level wide range constructions."
https://openalex.org/W2964010806,https://doi.org/10.18653/v1/d17-1115,Tensor Fusion Network for Multimodal Sentiment Analysis,2017,"Multimodal sentiment analysis is an increasingly popular research area, which extends the conventional language-based definition of to a multimodal setup where other relevant modalities accompany language. In this paper, we pose problem as modeling intra-modality and inter-modality dynamics. We introduce novel model, termed Tensor Fusion Networks, learns both such dynamics end-to-end. The proposed approach tailored for volatile nature spoken language in online videos well accompanying gestures voice. experiments, our model outperforms state-of-the-art approaches unimodal analysis."
https://openalex.org/W2604799547,https://doi.org/10.1613/jair.5477,"Survey of the State of the Art in Natural Language Generation: Core tasks, applications and evaluation",2018,"This paper surveys the current state of art in Natural Language Generation (NLG), defined as task generating text or speech from non-linguistic input. A survey NLG is timely view changes that field has undergone over past two decades, especially relation to new (usually data-driven) methods, well applications technology. therefore aims (a) give an up-to-date synthesis research on core tasks and architectures adopted which such are organised; (b) highlight a number recent topics have arisen partly result growing synergies between other areas artificial intelligence; (c) draw attention challenges evaluation, relating them similar faced NLP, with emphasis different evaluation methods relationships them."
https://openalex.org/W3122775348,https://doi.org/10.1109/taslp.2016.2520371,Deep Sentence Embedding Using Long Short-Term Memory Networks: Analysis and Application to Information Retrieval,2015,"This paper develops a model that addresses sentence embedding, hot topic in current natural language processing research, using recurrent neural networks with Long Short-Term Memory (LSTM) cells. Due to its ability capture long term memory, the LSTM-RNN accumulates increasingly richer information as it goes through sentence, and when reaches last word, hidden layer of network provides semantic representation whole sentence. In this paper, is trained weakly supervised manner on user click-through data logged by commercial web search engine. Visualization analysis are performed understand how embedding process works. The found automatically attenuate unimportant words detects salient keywords Furthermore, these detected activate different cells LSTM-RNN, where belonging similar same cell. As vector can be used many applications. These automatic keyword detection allocation abilities enabled allow perform document retrieval, difficult task, similarity between query documents measured distance their corresponding vectors computed LSTM-RNN. On shown significantly outperform several existing state art methods. We emphasize proposed generates specially useful for retrieval tasks. A comparison well known general method, Paragraph Vector, performed. results show method outperforms task."
https://openalex.org/W2963576560,https://doi.org/10.1109/cvpr.2016.496,Video Paragraph Captioning Using Hierarchical Recurrent Neural Networks,2016,"We present an approach that exploits hierarchical Recurrent Neural Networks (RNNs) to tackle the video captioning problem, i.e., generating one or multiple sentences describe a realistic video. Our framework contains sentence generator and paragraph generator. The produces simple short describes specific interval. It both temporal-and spatial-attention mechanisms selectively focus on visual elements during generation. captures inter-sentence dependency by taking as input sentential embedding produced generator, combining it with history, outputting new initial state for evaluate our two large-scale benchmark datasets: YouTubeClips TACoS-MultiLevel. experiments demonstrate significantly outperforms current state-of-the-art methods BLEU@4 scores 0.499 0.305 respectively."
https://openalex.org/W2964217331,https://doi.org/10.18653/v1/d15-1206,Classifying Relations via Long Short Term Memory Networks along Shortest Dependency Paths,2015,"Relation classification is an important research arena in the field of natural language processing (NLP). In this paper, we present SDP-LSTM, a novel neural network to classify relation two entities sentence. Our architecture leverages shortest dependency path (SDP) between entities; multichannel recurrent networks, with long short term memory (LSTM) units, pick up heterogeneous information along SDP. proposed model has several distinct features: (1) The paths retain most relevant (to classification), while eliminating irrelevant words (2) LSTM networks allow effective integration from sources over paths. (3) A customized dropout strategy regularizes alleviate overfitting. We test our on SemEval 2010 task, and achieve F1-score 83.7%, higher than competing methods literature."
https://openalex.org/W2759820691,https://doi.org/10.18653/v1/d17-1317,Truth of Varying Shades: Analyzing Language in Fake News and Political Fact-Checking,2017,"We present an analytic study on the language of news media in context political fact-checking and fake detection. compare real with that satire, hoaxes, propaganda to find linguistic characteristics untrustworthy text. To probe feasibility automatic fact-checking, we also a case based PolitiFact.com using their factuality judgments 6-point scale. Experiments show while remains be open research question, stylistic cues can help determine truthfulness"
https://openalex.org/W2556605533,https://doi.org/10.1016/j.eswa.2016.10.065,Improving sentiment analysis via sentence type classification using BiLSTM-CRF and CNN,2017,"A divide-and-conquer method classifying sentence types before sentiment analysis.Classifying by the number of opinion targets a contain.A data-driven approach automatically extract features from input sentences. Different sentences express in very different ways. Traditional sentence-level classification research focuses on one-technique-fits-all solution or only centers one special type In this paper, we propose which first classifies into types, then performs analysis separately each type. Specifically, find that tend to be more complex if they contain targets. Thus, apply neural network based sequence model classify opinionated three according appeared sentence. Each group is fed one-dimensional convolutional for classification. Our has been evaluated four datasets and compared with wide range baselines. Experimental results show that: (1) can improve performance analysis; (2) proposed achieves state-of-the-art several benchmarking datasets."
https://openalex.org/W2951939904,https://doi.org/10.48550/arxiv.1703.05192,"Learning to Discover Cross-Domain Relations with Generative Adversarial
  Networks",2017,"While humans easily recognize relations between data from different domains without any supervision, learning to automatically discover them is in general very challenging and needs many ground-truth pairs that illustrate the relations. To avoid costly pairing, we address task of discovering cross-domain given unpaired data. We propose a method based on generative adversarial networks learns (DiscoGAN). Using discovered relations, our proposed network successfully transfers style one domain another while preserving key attributes such as orientation face identity. Source code for official implementation publicly available https://github.com/SKTBrain/DiscoGAN"
https://openalex.org/W1573040851,https://doi.org/10.1109/cvpr.2016.497,Jointly Modeling Embedding and Translation to Bridge Video and Language,2016,"Automatically describing video content with natural language is a fundamental challenge of multimedia. Recurrent Neural Networks (RNN), which models sequence dynamics, has attracted increasing attention on visual interpretation. However, most existing approaches generate word locally given previous words and the content, while relationship between sentence semantics not holistically exploited. As result, generated sentences may be contextually correct but (e.g., subjects, verbs or objects) are true. This paper presents novel unified framework, named Long Short-Term Memory visual-semantic Embedding (LSTM-E), can simultaneously explore learning LSTM embedding. The former aims to maximize probability generating next latter create embedding space for enforcing entire content. Our proposed LSTM-E consists three components: 2-D and/or 3-D deep convolutional neural networks powerful representation, RNN sentences, joint model exploring relationships semantics. experiments YouTube2Text dataset show that our achieves to-date best reported performance in sentences: 45.3% 31.0% terms BLEU@4 METEOR, respectively. We also demonstrate superior predicting Subject-Verb-Object (SVO) triplets several state-of-the-art techniques."
https://openalex.org/W2891555348,https://doi.org/10.18653/v1/d18-1269,XNLI: Evaluating Cross-lingual Sentence Representations,2018,"State-of-the-art natural language processing systems rely on supervision in the form of annotated data to learn competent models. These models are generally trained a single (usually English), and cannot be directly used beyond that language. Since collecting every is not realistic, there has been growing interest cross-lingual understanding (XLU) low-resource cross-language transfer. In this work, we construct an evaluation set for XLU by extending development test sets Multi-Genre Natural Language Inference Corpus (MultiNLI) 14 languages, including languages such as Swahili Urdu. We hope our dataset, dubbed XNLI, will catalyze research sentence providing informative standard task. addition, provide several baselines multilingual understanding, two based machine translation systems, use parallel train aligned bag-of-words LSTM encoders. find XNLI represents practical challenging suite, translating yields best performance among available baselines."
https://openalex.org/W2963807318,https://doi.org/10.18653/v1/w18-6301,Scaling Neural Machine Translation,2018,"Sequence to sequence learning models still require several days reach state of the art performance on large benchmark datasets using a single machine. This paper shows that reduced precision and batch training can speedup by nearly 5x 8-GPU machine with careful tuning implementation. On WMT’14 English-German translation, we match accuracy Vaswani et al. (2017) in under 5 hours when 8 GPUs obtain new 29.3 BLEU after for 85 minutes 128 GPUs. We further improve these results 29.8 much larger Paracrawl dataset. English-French task, state-of-the-art 43.2 8.5"
https://openalex.org/W2171469118,https://doi.org/10.1093/jamia/ocu041,Pharmacovigilance from social media: mining adverse drug reaction mentions using sequence labeling with word embedding cluster features,2015,"Abstract Objective Social media is becoming increasingly popular as a platform for sharing personal health-related information. This information can be utilized public health monitoring tasks, particularly pharmacovigilance, via the use of natural language processing (NLP) techniques. However, in social highly informal, and user-expressed medical concepts are often nontechnical, descriptive, challenging to extract. There has been limited progress addressing these challenges, thus far, advanced machine learning-based NLP techniques have underutilized. Our objective design approach extract mentions adverse drug reactions (ADRs) from informal text media. Methods We introduce ADRMine, concept extraction system that uses conditional random fields (CRFs). ADRMine utilizes variety features, including novel feature modeling words’ semantic similarities. The similarities modeled by clustering words based on unsupervised, pretrained word representation vectors (embeddings) generated unlabeled user posts using deep learning technique. Results outperforms several strong baseline systems ADR task achieving an F-measure 0.82. Feature analysis demonstrates proposed cluster features significantly improve performance. Conclusion It possible complex concepts, with relatively high performance, user-generated content. scalable, suitable mining, it relies large volumes data, diminishing need large, annotated training data sets."
https://openalex.org/W2963486920,https://doi.org/10.1109/cvpr.2018.00717,Zero-Shot Recognition via Semantic Embeddings and Knowledge Graphs,2018,"We consider the problem of zero-shot recognition: learning a visual classifier for category with zero training examples, just using word embedding and its relationship to other categories, which data are provided. The key dealing unfamiliar or novel is transfer knowledge obtained from familiar classes describe class. In this paper, we build upon recently introduced Graph Convolutional Network (GCN) propose an approach that uses both semantic embeddings categorical relationships predict classifiers. Given learned graph (KG), our takes as input each node (representing category). After series convolutions, category. During training, classifiers few categories given learn GCN parameters. At test time, these filters used unseen categories. show robust noise in KG. More importantly, provides significant improvement performance compared current state-of-the-art results (from 2 ~ 3% on some metrics whopping 20% few)."
https://openalex.org/W2765440071,https://doi.org/10.1145/3123266.3123326,Adversarial Cross-Modal Retrieval,2017,"Cross-modal retrieval aims to enable flexible experience across different modalities (e.g., texts vs. images). The core of cross-modal research is learn a common subspace where the items can be directly compared each other. In this paper, we present novel Adversarial Cross-Modal Retrieval (ACMR) method, which seeks an effective based on adversarial learning. learning implemented as interplay between two processes. first process, feature projector, tries generate modality-invariant representation in and confuse other modality classifier, discriminate generated representation. We further impose triplet constraints projector order minimize gap among representations all from with same semantic labels, while maximizing distances semantically images texts. Through joint exploitation above, underlying structure multimedia data better preserved when projected into subspace. Comprehensive experimental results four widely used benchmark datasets show that proposed ACMR method superior it significantly outperforms state-of-the-art methods."
https://openalex.org/W2605035112,https://doi.org/10.18653/v1/n18-1049,Unsupervised Learning of Sentence Embeddings Using Compositional n-Gram Features,2018,"The recent tremendous success of unsupervised word embeddings in a multitude applications raises the obvious question if similar methods could be derived to improve (i.e. semantic representations) sequences as well. We present simple but efficient objective train distributed representations sentences. Our method outperforms state-of-the-art models on most benchmark tasks, highlighting robustness produced general-purpose sentence embeddings."
https://openalex.org/W2963735856,https://doi.org/10.1109/cvpr.2016.493,Natural Language Object Retrieval,2016,"In this paper, we address the task of natural language object retrieval, to localize a target within given image based on query object. Natural retrieval differs from text-based as it involves spatial information about objects scene and global context. To issue, propose novel Spatial Context Recurrent ConvNet (SCRC) model scoring function candidate boxes for integrating configurations scene-level contextual into network. Our processes text, local descriptors, context features through recurrent network, outputs probability text conditioned each box score box, can transfer visual-linguistic knowledge captioning domain our task. Experimental results demonstrate that method effectively utilizes both information, outperforming previous baseline methods significantly different datasets scenarios, exploit large scale vision transfer."
https://openalex.org/W2888302696,https://doi.org/10.18653/v1/d18-1241,QuAC: Question Answering in Context,2018,"We present QuAC, a dataset for Question Answering in Context that contains 14K information-seeking QA dialogs (100K questions total). The involve two crowd workers: (1) student who poses sequence of freeform to learn as much possible about hidden Wikipedia text, and (2) teacher answers the by providing short excerpts from text. QuAC introduces challenges not found existing machine comprehension datasets: its are often more open-ended, unanswerable, or only meaningful within dialog context, we show detailed qualitative evaluation. also report results number reference models, including recently state-of-the-art reading architecture extended model context. Our best underperforms humans 20 F1, suggesting there is significant room future work on this data. Dataset, baseline, leaderboard available at http://quac.ai."
https://openalex.org/W2556468274,https://doi.org/10.18653/v1/d17-1206,A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks,2017,"Transfer and multi-task learning have traditionally focused on either a single source-target pair or very few, similar tasks. Ideally, the linguistic levels of morphology, syntax semantics would benefit each other by being trained in model. We introduce joint many-task model together with strategy for successively growing its depth to solve increasingly complex Higher layers include shortcut connections lower-level task predictions reflect hierarchies. use simple regularization term allow optimizing all weights improve one task's loss without exhibiting catastrophic interference Our end-to-end obtains state-of-the-art competitive results five different tasks from tagging, parsing, relatedness, entailment"
https://openalex.org/W2963890755,https://doi.org/10.1109/cvpr.2016.501,MovieQA: Understanding Stories in Movies through Question-Answering,2016,"We introduce the MovieQA dataset which aims to evaluate automatic story comprehension from both video and text. The consists of 14,944 questions about 408 movies with high semantic diversity. range simpler Who did What Whom, Why How certain events occurred. Each question comes a set five possible answers, correct one four deceiving answers provided by human annotators. Our is unique in that it contains multiple sources information – clips, plots, subtitles, scripts, DVS [32]. analyze our data through various statistics methods. further extend existing QA techniques show question-answering such open-ended semantics hard. make this public along an evaluation benchmark encourage inspiring work challenging domain."
https://openalex.org/W2962718684,https://doi.org/10.18653/v1/d18-1316,Generating Natural Language Adversarial Examples,2018,"Deep neural networks (DNNs) are vulnerable to adversarial examples, perturbations correctly classified examples which can cause the model misclassify. In image domain, these often be made virtually indistinguishable human perception, causing humans and state-of-the-art models disagree. However, in natural language small clearly perceptible, replacement of a single word drastically alter semantics document. Given challenges, we use black-box population-based optimization algorithm generate semantically syntactically similar that fool well-trained sentiment analysis textual entailment with success rates 97% 70%, respectively. We additionally demonstrate 92.3% successful their original label by 20 annotators, perceptibly quite similar. Finally, discuss an attempt training as defense, but fail yield improvement, demonstrating strength diversity our examples. hope findings encourage researchers pursue improving robustness DNNs domain."
https://openalex.org/W2593833795,https://doi.org/10.24963/ijcai.2017/579,Bilateral Multi-Perspective Matching for Natural Language Sentences,2017,"Natural language sentence matching is a fundamental technology for variety of tasks. Previous approaches either match sentences from single direction or only apply granular (word-by-word sentence-by-sentence) matching. In this work, we propose bilateral multi-perspective (BiMPM) model. Given two P and Q, our model first encodes them with BiLSTM encoder. Next, the encoded in directions against Q Q. each direction, time step one matched all time-steps other multiple perspectives. Then, another layer utilized to aggregate results into fix-length vector. Finally, based on vector, decision made through fully connected layer. We evaluate three tasks: paraphrase identification, natural inference answer selection. Experimental standard benchmark datasets show that achieves state-of-the-art performance"
https://openalex.org/W2252211741,https://doi.org/10.18653/v1/d15-1036,Evaluation methods for unsupervised word embeddings,2015,"We present a comprehensive study of evaluation methods for unsupervised embedding techniques that obtain meaningful representations words from text. Different evaluations result in different orderings methods, calling into question the common assumption there is one single optimal vector representation. new directly compare embeddings with respect to specific queries. These reduce bias, provide greater insight, and allow us solicit data-driven relevance judgments rapidly accurately through crowdsourcing."
https://openalex.org/W2963159690,https://doi.org/10.18653/v1/d18-1009,SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference,2018,"Given a partial description like “she opened the hood of car,” humans can reason about situation and anticipate what might come next (”then, she examined engine”). In this paper, we introduce task grounded commonsense inference, unifying natural language inference reasoning. We present SWAG, new dataset with 113k multiple choice questions rich spectrum situations. To address recurring challenges annotation artifacts human biases found in many existing datasets, propose Adversarial Filtering (AF), novel procedure that constructs de-biased by iteratively training an ensemble stylistic classifiers, using them to filter data. account for aggressive adversarial filtering, use state-of-the-art models massively oversample diverse set potential counterfactuals. Empirical results demonstrate while solve resulting problems high accuracy (88%), various competitive struggle on our task. provide comprehensive analysis indicates significant opportunities future research."
https://openalex.org/W2508865106,https://doi.org/10.1609/aaai.v30i1.10350,Siamese Recurrent Architectures for Learning Sentence Similarity,2016,"We present a siamese adaptation of the Long Short-Term Memory (LSTM) network for labeled data comprised pairs variable-length sequences. Our model is applied to assess semantic similarity between sentences, where we exceed state art, outperforming carefully handcrafted features and recently proposed neural systems greater complexity. For these applications, provide word-embedding vectors supplemented with synonymic information LSTMs, which use fixed size vector encode underlying meaning expressed in sentence (irrespective particular wording/syntax). By restricting subsequent operations rely on simple Manhattan metric, compel representations learned by our form highly structured space whose geometry reflects complex relationships. results are latest line findings that showcase LSTMs as powerful language models capable tasks requiring intricate understanding."
https://openalex.org/W2922580172,https://doi.org/10.18653/v1/s19-2010,SemEval-2019 Task 6: Identifying and Categorizing Offensive Language in Social Media (OffensEval),2019,"We present the results and main findings of SemEval-2019 Task 6 on Identifying Categorizing Offensive Language in Social Media (OffensEval). The task was based a new dataset, Identification Dataset (OLID), which contains over 14,000 English tweets, it featured three sub-tasks. In sub-task A, systems were asked to discriminate between offensive non-offensive posts. B, had identify type content post. Finally, C, detect target OffensEval attracted large number participants one most popular tasks SemEval-2019. total, nearly 800 teams signed up participate 115 them submitted results, are presented analyzed this report."
https://openalex.org/W2962843521,https://doi.org/10.18653/v1/s18-2023,Hypothesis Only Baselines in Natural Language Inference,2018,"We propose a hypothesis only baseline for diagnosing Natural Language Inference (NLI). Especially when an NLI dataset assumes inference is occurring based purely on the relationship between context and hypothesis, it follows that assessing entailment relations while ignoring provided degenerate solution. Yet, through experiments 10 distinct datasets, we find this approach, which refer to as hypothesis-only model, able significantly outperform majority-class across number of datasets. Our analysis suggests statistical irregularities may allow model perform in some datasets beyond what should be achievable without access context."
https://openalex.org/W2142192571,https://doi.org/10.1109/iccv.2015.9,Ask Your Neurons: A Neural-Based Approach to Answering Questions about Images,2015,"We address a question answering task on real-world images that is set up as Visual Turing Test. By combining latest advances in image representation and natural language processing, we propose Neural-Image-QA, an end-to-end formulation to this problem for which all parts are trained jointly. In contrast previous efforts, facing multi-modal where the output (answer) conditioned visual input (image question). Our approach Neural-Image-QA doubles performance of best problem. provide additional insights into by analyzing how much information contained only part new human baseline. To study consensus, related ambiguities inherent challenging task, two novel metrics collect answers extends original DAQUAR dataset DAQUAR-Consensus."
https://openalex.org/W2970854433,https://doi.org/10.18653/v1/d19-1077,"Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of BERT",2019,"Pretrained contextual representation models (Peters et al., 2018; Devlin 2018) have pushed forward the state-of-the-art on many NLP tasks. A new release of BERT (Devlin, includes a model simultaneously pretrained 104 languages with impressive performance for zero-shot cross-lingual transfer natural language inference task. This paper explores broader potential mBERT (multilingual) as zero shot 5 tasks covering total 39 from various families: NLI, document classification, NER, POS tagging, and dependency parsing. We compare best-published methods find competitive each Additionally, we investigate most effective strategy utilizing in this manner, determine to what extent generalizes away specific features, measure factors that influence transfer."
https://openalex.org/W2963073938,https://doi.org/10.18653/v1/n16-1024,Recurrent Neural Network Grammars,2016,"Comunicacio presentada a la 2016 Conference of the North American Chapter Association for Computational Linguistics, celebrada San Diego (CA, EUA) els dies 12 17 de juny 2016."
https://openalex.org/W2963247703,https://doi.org/10.18653/v1/n16-1101,"Multi-Way, Multilingual Neural Machine Translation with a Shared Attention Mechanism",2016,"We propose multi-way, multilingual neural machine translation. The proposed approach enables a single translation model to translate between multiple languages, with number of parameters that grows only linearly the languages. This is made possible by having attention mechanism shared across all language pairs. train on ten pairs from WMT'15 simultaneously and observe clear performance improvements over models trained one pair. In particular, we significantly improves quality low-resource"
https://openalex.org/W2964159778,https://doi.org/10.18653/v1/n16-1082,Visualizing and Understanding Neural Models in NLP,2016,"While neural networks have been successfully applied to many NLP tasks the resulting vectorbased models are very difficult interpret. For example it’s not clear how they achieve compositionality, building sentence meaning from meanings of words and phrases. In this paper we describe strategies for visualizing compositionality in NLP, inspired by similar work computer vision. We first plot unit values visualize negation, intensification, concessive clauses, allowing us see wellknown markedness asymmetries negation. then introduce methods a unit’s salience, amount that it contributes final composed first-order derivatives. Our general-purpose may wide applications understanding other semantic properties deep networks."
https://openalex.org/W2962985882,https://doi.org/10.18653/v1/p18-1063,Fast Abstractive Summarization with Reinforce-Selected Sentence Rewriting,2018,"Inspired by how humans summarize long documents, we propose an accurate and fast summarization model that first selects salient sentences then rewrites them abstractively (i.e., compresses paraphrases) to generate a concise overall summary. We use novel sentence-level policy gradient method bridge the non-differentiable computation between these two neural networks in hierarchical way, while maintaining language fluency. Empirically, achieve new state-of-the-art on all metrics (including human evaluation) CNN/Daily Mail dataset, as well significantly higher abstractiveness scores. Moreover, operating at word-level, enable parallel decoding of our generative results substantially faster (10-20x) inference speed 4x training convergence than previous long-paragraph encoder-decoder models. also demonstrate generalization test-only DUC-2002 where scores model."
https://openalex.org/W2962787423,https://doi.org/10.18653/v1/d17-1323,Men Also Like Shopping: Reducing Gender Bias Amplification using Corpus-level Constraints,2017,"Language is increasingly being used to define rich visual recognition problems with supporting image collections sourced from the web. Structured prediction models are in these tasks take advantage of correlations between co-occurring labels and input but risk inadvertently encoding social biases found web corpora. In this work, we study data associated multilabel object classification semantic role labeling. We find that (a) datasets for contain significant gender bias (b) trained on further amplify existing bias. For example, activity cooking over 33% more likely involve females than males a training set, model amplifies disparity 68% at test time. propose inject corpus-level constraints calibrating structured design an algorithm based Lagrangian relaxation collective inference. Our method results almost no performance loss underlying task decreases magnitude amplification by 47.5% 40.5% labeling, respectively."
https://openalex.org/W2962964995,https://doi.org/10.1007/978-3-030-01225-0_13,Stacked Cross Attention for Image-Text Matching,2018,"In this paper, we study the problem of image-text matching. Inferring latent semantic alignment between objects or other salient stuff (e.g. snow, sky, lawn) and corresponding words in sentences allows to capture fine-grained interplay vision language, makes matching more interpretable. Prior work either simply aggregates similarity all possible pairs regions without attending differentially less important regions, uses a multi-step attentional process limited number alignments which is present Stacked Cross Attention discover full using both image sentence as context infer similarity. Our approach achieves state-of-the-art results on MS-COCO Flickr30K datasets. On Flickr30K, our outperforms current best methods by 22.1% relatively text retrieval from query, 18.2% with query (based Recall@1). MS-COCO, improves 17.8% 16.6% Recall@1 5K test set). Code has been made available at: https://github.com/kuanghuei/SCAN."
https://openalex.org/W2546696630,https://doi.org/10.1109/cvpr.2017.232,Dual Attention Networks for Multimodal Reasoning and Matching,2017,"We propose Dual Attention Networks (DANs) which jointly leverage visual and textual attention mechanisms to capture fine-grained interplay between vision language. DANs attend specific regions in images words text through multiple steps gather essential information from both modalities. Based on this framework, we introduce two types of for multimodal reasoning matching, respectively. The model allows attentions steer each other during collaborative inference, is useful tasks such as Visual Question Answering (VQA). In addition, the matching exploits estimate similarity sentences by focusing their shared semantics. Our extensive experiments validate effectiveness combining language, achieving state-of-the-art performance public benchmarks VQA image-text matching."
https://openalex.org/W2963717374,https://doi.org/10.1109/iccv.2017.285,MUTAN: Multimodal Tucker Fusion for Visual Question Answering,2017,"Bilinear models provide an appealing framework for mixing and merging information in Visual Question Answering (VQA) tasks. They help to learn high level associations between question meaning visual concepts the image, but they suffer from huge dimensionality issues.,,We introduce MUTAN, a multimodal tensor-based Tucker decomposition efficiently parametrize bilinear interactions textual representations. Additionally framework, we design low-rank matrix-based explicitly constrain interaction rank. With control complexity of scheme while keeping nice interpretable fusion relations. We show how generalizes some latest VQA architectures, providing state-of-the-art results."
https://openalex.org/W2964266061,https://doi.org/10.18653/v1/p18-1073,A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings,2018,"Recent work has managed to learn cross-lingual word embeddings without parallel data by mapping monolingual a shared space through adversarial training. However, their evaluation focused on favorable conditions, using comparable corpora or closely-related languages, and we show that they often fail in more realistic scenarios. This proposes an alternative approach based fully unsupervised initialization explicitly exploits the structural similarity of embeddings, robust self-learning algorithm iteratively improves this solution. Our method succeeds all tested scenarios obtains best published results standard datasets, even surpassing previous supervised systems. implementation is released as open source project at https://github.com/artetxem/vecmap"
https://openalex.org/W2962944953,https://doi.org/10.18653/v1/p16-1014,Pointing the Unknown Words,2016,"The problem of rare and unknown words is an important issue that can potentially effect the performance many NLP systems, including traditional count-based deep learning models. We propose a novel way to deal with unseen for neural network models using attention. Our model uses two softmax layers in order predict next word conditional language models: one predicts location source sentence, other shortlist vocabulary. At each timestep, decision which layer use adaptively made by MLP conditioned on context. motivate this work from psychological evidence humans naturally have tendency point towards objects context or environment when name object not known. Using our proposed model, we observe improvements tasks, machine translation Europarl English French parallel corpora text summarization Gigaword dataset."
https://openalex.org/W2963866616,https://doi.org/10.1162/tacl_a_00021,Constructing Datasets for Multi-hop Reading Comprehension Across Documents,2018,"Most Reading Comprehension methods limit themselves to queries which can be answered using a single sentence, paragraph, or document. Enabling models combine disjoint pieces of textual evidence would extend the scope machine comprehension methods, but currently no resources exist train and test this capability. We propose novel task encourage development for text understanding across multiple documents investigate limits existing methods. In our task, model learns seek — effectively performing multihop, alias multi-step, inference. devise methodology produce datasets given collection query-answer pairs thematically linked documents. Two from different domains are induced, we identify potential pitfalls circumvention strategies. evaluate two previously proposed competitive find that one integrate information However, both struggle select relevant information; providing guaranteed greatly improves their performance. While outperform several strong baselines, best accuracy reaches 54.5% on an annotated set, compared human performance at 85.0%, leaving ample room improvement."
https://openalex.org/W2221701072,https://doi.org/10.1107/s1600576715016544,<i>SASfit</i>: a tool for small-angle scattering data analysis using a library of analytical expressions,2015,"SASfit is one of the mature programs for small-angle scattering data analysis and has been available many years. This article describes basic processing workflow along with recent developments in program package (version 0.94.6). They include (i) advanced algorithms reduction oversampled sets, (ii) improved confidence assessment optimized model parameters (iii) a flexible plug-in system custom user-provided models. A function mass fractal branched polymers solution provided as an example implementing plug-in. The new release major platforms such Windows, Linux MacOS. To facilitate usage, it includes comprehensive indexed documentation well web-based wiki peer collaboration online videos demonstrating usage. use illustrated by interpretation X-ray curves monomodal gold nanoparticles (NIST reference material 8011) bimodal silica (EU ERM-FD-102)."
https://openalex.org/W2466175319,https://doi.org/10.18653/v1/n16-1098,A Corpus and Cloze Evaluation for Deeper Understanding of Commonsense Stories,2016,"Representation and learning of commonsense knowledge is one the foundational problems in quest to enable deep language understanding. This issue particularly challenging for understanding casual correlational relationships between events. While this topic has received a lot interest NLP community, research been hindered by lack proper evaluation framework. paper attempts address problem with new framework evaluating story script learning: `Story Cloze Test’. test requires system choose correct ending four-sentence story. We created corpus 50k five-sentence stories, ROCStories, evaluation. unique two ways: (1) it captures rich set causal temporal relations daily events, (2) high quality collection everyday life stories that can also be used generation. Experimental shows host baselines state-of-the-art models based on shallow struggle achieve score Story Test. discuss these implications learning, offer suggestions deeper"
https://openalex.org/W2122522916,https://doi.org/10.1371/journal.pone.0144296,Sentiment of Emojis,2015,"There is a new generation of emoticons, called emojis, that increasingly being used in mobile communications and social media. In the past two years, over ten billion emojis were on Twitter. Emojis are Unicode graphic symbols, as shorthand to express concepts ideas. contrast small number well-known emoticons carry clear emotional contents, there hundreds emojis. But what their contents? We provide first emoji sentiment lexicon, Emoji Sentiment Ranking, draw map 751 most frequently The computed from tweets which they occur. engaged 83 human annotators label 1.6 million 13 European languages by polarity (negative, neutral, or positive). About 4% annotated contain analysis allows us several interesting conclusions. It turns out positive, especially popular ones. distribution with without significantly different. inter-annotator agreement higher. tend occur at end tweets, increases distance. observe no significant differences rankings between Ranking. Consequently, we propose our Ranking language-independent resource for automated analysis. Finally, paper provides formalization novel visualization form bar."
https://openalex.org/W2895553377,https://doi.org/10.21105/joss.00774,quanteda: An R package for the quantitative analysis of textual data,2018,"quanteda is an R package providing a comprehensive workflow and toolkit for natural language processing tasks such as corpus management, tokenization, analysis, visualization. It has extensive functions applying dictionary exploring texts using keywords-in-context, computing document feature similarities, discovering multi-word expressions through collocation scoring. Based entirely on sparse operations,it provides highly efficient methods compiling document-feature matrices manipulating these or them in further quantitative analysis. Using C++ multi-threading extensively, also considerably faster more than other Python packages large textual data. The designed users needing to apply texts,from documents final Its capabilities match exceed those provided many end-user software applications, of which are expensive not open source. therefore great benefit researchers, students, analysts with fewer financial resources. While requires programming knowledge, its API enable powerful, analysis minimum steps. By emphasizing consistent design, furthermore, lowers the barriers learning NLP text even proficient programmers."
https://openalex.org/W2963224792,https://doi.org/10.1109/iccv.2017.93,Learning to Reason: End-to-End Module Networks for Visual Question Answering,2017,"Natural language questions are inherently compositional, and many most easily answered by reasoning about their decomposition into modular sub-problems. For example, to answer “is there an equal number of balls boxes?” we can look for balls, boxes, count them, compare the results. The recently proposed Neural Module Network (NMN) architecture [3, 2] implements this approach question answering parsing linguistic substructures assembling question-specific deep networks from smaller modules that each solve one subtask. However, existing NMN implementations rely on brittle off-the-shelf parsers, restricted module configurations these parsers rather than learning them data. In paper, propose End-to-End Networks (N2NMNs), which learn reason directly predicting instance-specific network layouts without aid a parser. Our model learns generate structures (by imitating expert demonstrations) while simultaneously parameters (using downstream task loss). Experimental results new CLEVR dataset targeted at compositional show N2NMNs achieve error reduction nearly 50% relative state-of-theart attentional approaches, discovering interpretable architectures specialized question."
https://openalex.org/W2963563735,https://doi.org/10.18653/v1/p17-1161,Semi-supervised sequence tagging with bidirectional language models,2017,"Pre-trained word embeddings learned from unlabeled text have become a standard component of neural network architectures for NLP tasks. However, in most cases, the recurrent that operates on word-level representations to produce context sensitive is trained relatively little labeled data. In this paper, we demonstrate general semi-supervised approach adding pretrained bidirectional language models systems and apply it sequence labeling We evaluate our model two datasets named entity recognition (NER) chunking, both cases achieve state art results, surpassing previous use other forms transfer or joint learning with additional data task specific gazetteers."
https://openalex.org/W3107826490,https://doi.org/10.48550/arxiv.2001.08210,Multilingual Denoising Pre-training for Neural Machine Translation,2020,"This paper demonstrates that multilingual denoising pre-training produces significant performance gains across a wide variety of machine translation (MT) tasks. We present mBART -- sequence-to-sequence auto-encoder pre-trained on large-scale monolingual corpora in many languages using the BART objective. is one first methods for complete model by full texts multiple languages, while previous approaches have focused only encoder, decoder, or reconstructing parts text. Pre-training allows it to be directly fine tuned supervised (both sentence-level and document-level) unsupervised translation, with no task-specific modifications. demonstrate adding initialization all but highest-resource settings, including up 12 BLEU points low resource MT over 5 document-level models. also show enables new types transfer language pairs bi-text were not corpus, extensive analysis which factors contribute most effective pre-training."
https://openalex.org/W2281397941,https://doi.org/10.1136/bmj.i969,Analysis of matched case-control studies,2016,"There are two common misconceptions about case-control studies: that matching in itself eliminates (controls) confounding by the factors, and if has been performed, then a “matched analysis” is required. However, study does not control for factors; fact it can introduce factors even when did exist source population. Thus, matched design may require controlling analysis. case requires Provided there no problems of sparse data, be obtained, with loss validity possible increase precision, using “standard” (unconditional) analysis, “matched” (conditional) analysis required or appropriate."
https://openalex.org/W2076462394,https://doi.org/10.1007/s10489-014-0629-7,Audio-visual speech recognition using deep learning,2015,"Audio-visual speech recognition (AVSR) system is thought to be one of the most promising solutions for reliable recognition, particularly when audio corrupted by noise. However, cautious selection sensory features crucial attaining high performance. In machine-learning community, deep learning approaches have recently attracted increasing attention because neural networks can effectively extract robust latent that enable various algorithms demonstrate revolutionary generalization capabilities under diverse application conditions. This study introduces a connectionist-hidden Markov model (HMM) noise-robust AVSR. First, denoising autoencoder utilized acquiring features. By preparing training data network with pairs consecutive multiple steps deteriorated and corresponding clean features, trained output denoised from Second, convolutional (CNN) visual raw mouth area images. CNN as images phoneme label outputs, predict labels input Finally, multi-stream HMM (MSHMM) applied integrating acquired HMMs independently respective comparing cases normal mel-frequency cepstral coefficients (MFCCs) are HMM, our unimodal isolated word results approximately 65 % rate gain attained MFCCs 10 dB signal-to-noise-ratio (SNR) signal input. Moreover, multimodal utilizing MSHMM an additional SNR conditions below dB."
https://openalex.org/W1911232923,https://doi.org/10.4324/9781315688121,Syllable-Based Generalizations in English Phonology,2015,Thesis. 1976. Ph.D.--Massachusetts Institute of Technology. Dept. Foreign Literatures and Linguistics.
https://openalex.org/W2734608416,https://doi.org/10.1093/bioinformatics/btx228,Deep learning with word embeddings improves biomedical named entity recognition,2017,"Text mining has become an important tool for biomedical research. The most fundamental text-mining task is the recognition of named entities (NER), such as genes, chemicals and diseases. Current NER methods rely on pre-defined features which try to capture specific surface properties entity types, typical local context, background knowledge, linguistic information. State-of-the-art tools are entity-specific, dictionaries empirically optimal feature sets differ between makes their development costly. Furthermore, often optimized a gold standard corpus, extrapolation quality measures difficult.We show that completely generic method based deep learning statistical word embeddings [called long short-term memory network-conditional random field (LSTM-CRF)] outperforms state-of-the-art entity-specific tools, by large margin. To this end, we compared performance LSTM-CRF 33 data covering five different classes with best-of-class entity-agnostic CRF implementation. On average, F1-score 5% above baselines, mostly due sharp increase in recall.The source code available at https://github.com/glample/tagger links corpora https://corposaurus.github.io/corpora/ .habibima@informatik.hu-berlin.de."
https://openalex.org/W2963126845,https://doi.org/10.18653/v1/n18-1170,Adversarial Example Generation with Syntactically Controlled Paraphrase Networks,2018,"We propose syntactically controlled paraphrase networks (SCPNs) and use them to generate adversarial examples. Given a sentence target syntactic form (e.g., constituency parse), SCPNs are trained produce of the with desired syntax. show it is possible create training data for this task by first doing backtranslation at very large scale, then using parser label transformations that naturally occur during process. Such allows us train neural encoder-decoder model extra inputs specify A combination automated human evaluations paraphrases follow their specifications without decreasing quality when compared baseline (uncontrolled) systems. Furthermore, they more capable generating examples both (1) “fool” pretrained models (2) improve robustness these variation used augment data."
https://openalex.org/W2963206679,https://doi.org/10.18653/v1/d18-1549,Phrase-Based &amp; Neural Unsupervised Machine Translation,2018,"Machine translation systems achieve near human-level performance on some languages, yet their effectiveness strongly relies the availability of large amounts parallel sentences, which hinders applicability to majority language pairs. This work investigates how learn translate when having access only monolingual corpora in each language. We propose two model variants, a neural and phrase-based model. Both versions leverage careful initialization parameters, denoising effect models automatic generation data by iterative back-translation. These are significantly better than methods from literature, while being simpler fewer hyper-parameters. On widely used WMT’14 English-French WMT’16 German-English benchmarks, our respectively obtain 28.1 25.2 BLEU points without using single sentence, outperforming state art more 11 points. low-resource languages like English-Urdu English-Romanian, even results semi-supervised supervised approaches leveraging paucity available bitexts. Our code for NMT PBSMT is publicly available."
https://openalex.org/W2963655793,https://doi.org/10.18653/v1/p16-1002,Data Recombination for Neural Semantic Parsing,2016,"Modeling crisp logical regularities is crucial in semantic parsing, making it difficult for neural models with no task-specific prior knowledge to achieve good results. In this paper, we introduce data recombination, a novel framework injecting such into model. From the training data, induce high-precision synchronous context-free grammar, which captures important conditional independence properties commonly found parsing. We then train sequence-to-sequence recurrent network (RNN) model attention-based copying mechanism on datapoints sampled from thereby teaching about these structural properties. Data recombination improves accuracy of our RNN three parsing datasets, leading new state-of-the-art performance standard GeoQuery dataset comparable supervision."
https://openalex.org/W2516621648,https://doi.org/10.18653/v1/p16-1195,Summarizing Source Code using a Neural Attention Model,2016,"High quality source code is often paired with high level summaries of the computation it performs, for example in documentation or descriptions posted online forums. Such are extremely useful applications such as search but expensive to manually author, hence only done a small fraction all that produced. In this paper, we present first completely datadriven approach generating code. Our model, CODE-NN , uses Long Short Term Memory (LSTM) networks attention produce sentences describe C# snippets and SQL queries. trained on new corpus automatically collected from StackOverflow, which release. Experiments demonstrate strong performance two tasks: (1) summarization, where establish end-to-end learning results outperform baselines, (2) retrieval, our learned model improves state art recently introduced benchmark by large margin."
https://openalex.org/W2923014074,https://doi.org/10.18653/v1/w18-5446,GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding,2018,"Human ability to understand language is general, flexible, and robust. In contrast, most NLU models above the word level are designed for a specific task struggle with out-of-domain data. If we aspire develop understanding beyond detection of superficial correspondences between inputs outputs, then it critical unified model that can execute range linguistic tasks across different domains. To facilitate research in this direction, present General Language Understanding Evaluation (GLUE, gluebenchmark.com): benchmark nine diverse tasks, an auxiliary dataset probing phenomena, online platform evaluating comparing models. For some training data plentiful, but others limited or does not match genre test set. GLUE thus favors represent knowledge way facilitates sample-efficient learning effective knowledge-transfer tasks. While none datasets were created from scratch benchmark, four them feature privately-held data, which used ensure fairly. We evaluate baselines use ELMo (Peters et al., 2018), powerful transfer technique, as well state-of-the-art sentence representation The best still achieve fairly low absolute scores. Analysis our diagnostic yields similarly weak performance over all phenomena tested, exceptions."
https://openalex.org/W2740765036,https://doi.org/10.18653/v1/p17-1044,Deep Semantic Role Labeling: What Works and What’s Next,2017,"We introduce a new deep learning model for semantic role labeling (SRL) that significantly improves the state of art, along with detailed analyses to reveal its strengths and limitations. use highway BiLSTM architecture constrained decoding, while observing number recent best practices initialization regularization. Our 8-layer ensemble achieves 83.2 F1 on theCoNLL 2005 test set 83.4 CoNLL 2012, roughly 10% relative error reduction over previous art. Extensive empirical analysis these gains show (1) models excel at recovering long-distance dependencies but can still make surprisingly obvious errors, (2) there is room syntactic parsers improve results."
https://openalex.org/W2946794439,https://doi.org/10.18653/v1/p19-1580,"Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned",2019,"Multi-head self-attention is a key component of the Transformer, state-of-the-art architecture for neural machine translation. In this work we evaluate contribution made by individual attention heads to overall performance model and analyze roles played them in encoder. We find that most important confident play consistent often linguistically-interpretable roles. When pruning using method based on stochastic gates differentiable relaxation L0 penalty, observe specialized are last be pruned. Our novel removes vast majority without seriously affecting performance. For example, English-Russian WMT dataset, 38 out 48 encoder results drop only 0.15 BLEU."
https://openalex.org/W2741029840,https://doi.org/10.18653/v1/k17-3009,"Tokenizing, POS Tagging, Lemmatizing and Parsing UD 2.0 with UDPipe",2017,"We present an update to UDPipe 1.0 (Straka et al., 2016), a trainable pipeline which performs sentence segmentation,
tokenization, POS tagging, lemmatization and dependency parsing.
We provide models for all 50 languages of UD 2.0, and
furthermore, the can be trained easily using data in CoNLL-U format. For purpose CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text Universal Dependencies, updated 1.1 was used as one baseline systems, finishing 13th system 33 participants. A further improved 1.2 participated shared task, placing 8th best system, while achieving low running times moder-
ately sized models.
The tool is available under open-source Mozilla Public Licence (MPL) provides bindings C++, Python (through
ufal.udpipe PyPI package), Perl (through UFAL::UDPipe CPAN
package), Java C#."
https://openalex.org/W2962717047,https://doi.org/10.18653/v1/p17-1123,Learning to Ask: Neural Question Generation for Reading Comprehension,2017,"We study automatic question generation for sentences from text passages in reading comprehension. introduce an attention-based sequence learning model the task and investigate effect of encoding sentence- vs. paragraph-level information. In contrast to all previous work, our does not rely on hand-crafted rules or a sophisticated NLP pipeline; it is instead trainable end-to-end via sequence-to-sequence learning. Automatic evaluation results show that system significantly outperforms state-of-the-art rule-based system. human evaluations, questions generated by are also rated as being more natural (i.e.,, grammaticality, fluency) difficult answer (in terms syntactic lexical divergence original reasoning needed answer)."
https://openalex.org/W2962801832,https://doi.org/10.18653/v1/w16-2323,Edinburgh Neural Machine Translation Systems for WMT 16,2016,"We participated in the WMT 2016 shared news translation task by building neural systems for four language pairs, each trained both directions: English↔Czech, English↔German, English↔Romanian and English↔Russian. Our are based on an attentional encoder-decoder, using BPE subword segmentation open-vocabulary with a fixed vocabulary. experimented automatic back-translations of monolingual News corpus as additional training data, pervasive dropout, target-bidirectional models. All reported methods give substantial improvements, we see improvements 4.3–11.2 BLEU over our baseline systems. In human evaluation, were (tied) best constrained system 7 out 8 directions which participated.12"
https://openalex.org/W2512924740,https://doi.org/10.18653/v1/w16-2301,Findings of the 2016 Conference on Machine Translation,2016,"This paper presents the results of WMT16 shared tasks, which included five machine translation (MT) tasks (standard news, IT-domain, biomedical, multimodal, pronoun), three evaluation (metrics, tuning, run-time estimation MT quality), and an automatic post-editing task bilingual document alignment task. year, 102 systems from 24 institutions (plus 36 anonymized online systems) were submitted to 12 directions in news The IT-domain received 31 submissions 7 Biomedical 15 5 institutions. Evaluation was both manual (relative ranking 100-point scale assessments). quality had subtasks, with a total 14 teams, submitting 39 entries. 6 11"
https://openalex.org/W2951434086,https://doi.org/10.18653/v1/p19-1612,Latent Retrieval for Weakly Supervised Open Domain Question Answering,2019,"Recent work on open domain question answering (QA) assumes strong supervision of the supporting evidence and/or a blackbox information retrieval (IR) system to retrieve candidates. We argue that both are suboptimal, since gold is not always available, and QA fundamentally different from IR. show for first time it possible jointly learn retriever reader question-answer string pairs without any IR system. In this setting, all Wikipedia treated as latent variable. Since impractical scratch, we pre-train with an Inverse Cloze Task. evaluate versions five datasets. On datasets where questioner already knows answer, traditional such BM25 sufficient. user genuinely seeking learned crucial, outperforming by up 19 points in exact match."
https://openalex.org/W2968124245,https://doi.org/10.48550/arxiv.1908.03557,VisualBERT: A Simple and Performant Baseline for Vision and Language,2019,"We propose VisualBERT, a simple and flexible framework for modeling broad range of vision-and-language tasks. VisualBERT consists stack Transformer layers that implicitly align elements an input text regions in associated image with self-attention. further two visually-grounded language model objectives pre-training on caption data. Experiments four tasks including VQA, VCR, NLVR2, Flickr30K show outperforms or rivals state-of-the-art models while being significantly simpler. Further analysis demonstrates can ground to without any explicit supervision is even sensitive syntactic relationships, tracking, example, associations between verbs corresponding their arguments."
https://openalex.org/W2963804993,https://doi.org/10.18653/v1/n16-1162,Learning Distributed Representations of Sentences from Unlabelled Data,2016,"Unsupervised methods for learning distributed representations of words are ubiquitous in today's NLP research, but far less is known about the best ways to learn phrase or sentence from unlabelled data. This paper a systematic comparison models that such representations. We find optimal approach depends critically on intended application. Deeper, more complex preferable be used supervised systems, shallow log-linear work building representation spaces can decoded with simple spatial distance metrics. also propose two new unsupervised representation-learning objectives designed optimise trade-off between training time, domain portability and performance."
https://openalex.org/W2964090065,https://doi.org/10.18653/v1/p16-2067,Multilingual Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Models and Auxiliary Loss,2016,"Bidirectional long short-term memory (biLSTM) networks have recently proven successful for various NLP sequence modeling tasks, but little is known about their reliance to input representations, target languages, data set size, and label noise. We address these issues evaluate bi-LSTMs with word, character, unicode byte embeddings POS tagging. compare traditional taggers across languages sizes. also present a novel biLSTM model, which combines the tagging loss function an auxiliary that accounts rare words. The model obtains state-of-the-art performance 22 works especially well morphologically complex languages. Our analysis suggests biLSTMs are less sensitive training size corruptions (at small noise levels) than previously assumed."
https://openalex.org/W2963751529,https://doi.org/10.18653/v1/n18-1108,Colorless Green Recurrent Networks Dream Hierarchically,2018,"Recurrent neural networks (RNNs) have achieved impressive results in a variety of linguistic processing tasks, suggesting that they can induce non-trivial properties language. We investigate here to what extent RNNs learn track abstract hierarchical syntactic structure. test whether trained with generic language modeling objective four languages (Italian, English, Hebrew, Russian) predict long-distance number agreement various constructions. include our evaluation nonsensical sentences where cannot rely on semantic or lexical cues (The colorless green ideas I ate the chair sleep furiously), and, for Italian, we compare model performance human intuitions. Our language-model-trained make reliable predictions about agreement, and do not lag much behind performance. thus bring support hypothesis are just shallow-pattern extractors, but also acquire deeper grammatical competence."
https://openalex.org/W2115613106,https://doi.org/10.3115/v1/p15-1107,A Hierarchical Neural Autoencoder for Paragraphs and Documents,2015,"Natural language generation of coherent long texts like paragraphs or longer documents is a challenging problem for recurrent networks models. In this paper, we explore an important step toward task: training LSTM (Longshort term memory) auto-encoder to preserve and reconstruct multi-sentence paragraphs. We introduce model that hierarchically builds embedding paragraph from embeddings sentences words, then decodes the original paragraph. evaluate reconstructed using standard metrics ROUGE Entity Grid, showing neural models are able encode in way syntactic, semantic, discourse coherence. While only first generating text units models, our work has potential significantly impact natural summarization1."
https://openalex.org/W2963101956,https://doi.org/10.1109/cvpr.2019.01094,Auto-Encoding Scene Graphs for Image Captioning,2019,"We propose Scene Graph Auto-Encoder (SGAE) that incorporates the language inductive bias into encoder-decoder image captioning framework for more human-like captions. Intuitively, we humans use to compose collocations and contextual inference in discourse. For example, when see relation `person on bike', it is natural replace `on' with `ride' infer riding bike a road' even `road' not evident. Therefore, exploiting such as prior expected help conventional models less likely overfit dataset focus reasoning. Specifically, scene graph --- directed ($\mathcal{G}$) where an object node connected by adjective nodes relationship represent complex structural layout of both ($\mathcal{I}$) sentence ($\mathcal{S}$). In textual domain, SGAE learn dictionary ($\mathcal{D}$) helps reconstruct sentences $\mathcal{S}\rightarrow \mathcal{G} \rightarrow \mathcal{D} \mathcal{S}$ pipeline, $\mathcal{D}$ encodes desired prior; vision-language shared guide $\mathcal{I}\rightarrow \mathcal{G}\rightarrow pipeline. Thanks representation dictionary, transferred across domains principle. validate effectiveness challenging MS-COCO benchmark, e.g., our SGAE-based single-model achieves new state-of-the-art $127.8$ CIDEr-D Karpathy split, competitive $125.5$ (c40) official server compared other ensemble models."
https://openalex.org/W2963517393,https://doi.org/10.1109/cvpr.2016.452,Robust Scene Text Recognition with Automatic Rectification,2016,"Recognizing text in natural images is a challenging task with many unsolved problems. Different from those documents, words often possess irregular shapes, which are caused by perspective distortion, curved character placement, etc. We propose RARE (Robust recognizer Automatic REctification), recognition model that robust to text. speciallydesigned deep neural network, consists of Spatial Transformer Network (STN) and Sequence Recognition (SRN). In testing, an image firstly rectified via predicted Thin-Plate-Spline (TPS) transformation, into more readable for the following SRN, recognizes through sequence approach. show able recognize several types text, including end-to-end trainable, requiring only associated labels, making it convenient train deploy practical systems. State-of-the-art or highly-competitive performance achieved on benchmarks well demonstrates effectiveness proposed model."
https://openalex.org/W1267646904,https://doi.org/10.1038/npjschz.2015.30,Automated analysis of free speech predicts psychosis onset in high-risk youths,2015,"Psychiatry lacks the objective clinical tests routinely used in other specializations. Novel computerized methods to characterize complex behaviors such as speech could be identify and predict psychiatric illness individuals.In this proof-of-principle study, our aim was test automated analyses combined with Machine Learning later psychosis onset youths at high-risk (CHR) for psychosis.Thirty-four CHR (11 females) had baseline interviews were assessed quarterly up 2.5 years; five transitioned psychosis. Using analysis, transcripts of evaluated semantic syntactic features predicting onset. Speech fed into a convex hull classification algorithm leave-one-subject-out cross-validation assess their predictive value outcome. The canonical correlation between prodromal symptom ratings computed.Derived included Latent Semantic Analysis measure coherence two markers complexity: maximum phrase length use determiners (e.g., which). These predicted development 100% accuracy, outperforming from interviews. significantly correlated symptoms.Findings support utility analysis subtle, clinically relevant mental state changes emergent Recent developments computer science, including natural language processing, provide foundation future psychiatry."
https://openalex.org/W2251329024,https://doi.org/10.18653/v1/d15-1162,An Improved Non-monotonic Transition System for Dependency Parsing,2015,"Transition-based dependency parsers usually use transition systems that monotonically extend partial parse states until they identify a complete tree. Honnibal et al. (2013) showed greedy onebest parsing accuracy can be improved by adding additional non-monotonic transitions permit the parser to “repair” earlier mistakes “over-writing” decisions. This increases size of set trees each state derive, enabling such escape “garden paths” trap monotonic transition-based parsers. We describe new permits derive larger completed than previous work, which allows our from garden paths. A with nonmonotonic system has 91.85% directed attachment accuracy, an improvement 0.6% over comparable using standard arc-eager transitions."
https://openalex.org/W1572786359,https://doi.org/10.1186/s40537-015-0015-2,Sentiment analysis using product review data,2015,"Sentiment analysis or opinion mining is one of the major tasks NLP (Natural Language Processing). has gain much attention in recent years. In this paper, we aim to tackle problem sentiment polarity categorization, which fundamental problems analysis. A general process for categorization proposed with detailed descriptions. Data used study are online product reviews collected from Amazon.com. Experiments both sentence-level and review-level performed promising outcomes. At last, also give insight into our future work on"
https://openalex.org/W2250342921,https://doi.org/10.18653/v1/w15-3049,chrF: character n-gram F-score for automatic MT evaluation,2015,"We propose the use of character n-gram F-score for automatic evaluation machine translation output. Character ngrams have already been used as a part more complex metrics, but their individual potential has not investigated yet. report system-level correlations with human rankings 6-gram F1-score (CHRF) on WMT12, WMT13 and WMT14 data well segment-level correlation 6gram F1 F3-scores (CHRF3) all available target languages. The results are very promising, especially CHRF3 score – from English, this variant showed highest outperforming even best metrics shared task."
https://openalex.org/W2954226438,https://doi.org/10.18653/v1/s19-2007,SemEval-2019 Task 5: Multilingual Detection of Hate Speech Against Immigrants and Women in Twitter,2019,"The paper describes the organization of SemEval 2019 Task 5 about detection hate speech against immigrants and women in Spanish English messages extracted from Twitter. task is organized two related classification subtasks: a main binary subtask for detecting presence speech, finer-grained one devoted to identifying further features hateful contents such as aggressive attitude target harassed, distinguish if incitement an individual rather than group. HatEval has been most popular tasks SemEval-2019 with total 108 submitted runs Subtask A 70 B, 74 different teams. Data provided are described by showing how they have collected annotated. Moreover, provides analysis discussion participant systems results achieved both subtasks."
https://openalex.org/W2963143606,https://doi.org/10.18653/v1/n16-1181,Learning to Compose Neural Networks for Question Answering,2016,"We describe a question answering model that applies to both images and structured knowledge bases. The uses natural language strings automatically assemble neural networks from collection of composable modules. Parameters for these modules are learned jointly with network-assembly parameters via reinforcement learning, only (world, question, answer) triples as supervision. Our approach, which we term dynamic network, achieves state-of-the-art results on benchmark datasets in visual domains."
https://openalex.org/W2911227954,https://doi.org/10.1162/tacl_a_00041,Data Statements for Natural Language Processing: Toward Mitigating System Bias and Enabling Better Science,2018,"In this paper, we propose data statements as a design solution and professional practice for natural language processing technologists, in both research development. Through the adoption widespread use of statements, field can begin to address critical scientific ethical issues that result from certain populations development technology other populations. We present form take explore implications adopting them part regular practice. argue will help alleviate related exclusion bias technology, lead better precision claims about how generalize thus engineering results, protect companies public embarrassment, ultimately meets its users their own preferred linguistic style furthermore does not misrepresent others."
https://openalex.org/W2962977603,https://doi.org/10.18653/v1/n19-1144,Predicting the Type and Target of Offensive Posts in Social Media,2019,"As offensive content has become pervasive in social media, there been much research identifying potentially messages. However, previous work on this topic did not consider the problem as a whole, but rather focused detecting very specific types of content, e.g., hate speech, cyberbulling, or cyber-aggression. In contrast, here we target several different kinds content. particular, model task hierarchically, type and messages media. For purpose, complied Offensive Language Identification Dataset (OLID), new dataset with tweets annotated for using fine-grained three-layer annotation scheme, which make publicly available. We discuss main similarities differences between OLID pre-existing datasets speech identification, aggression detection, similar tasks. further experiment compare performance machine learning models OLID."
https://openalex.org/W2980708516,https://doi.org/10.1007/978-3-030-32381-3_16,How to Fine-Tune BERT for Text Classification?,2019,"Language model pre-training has proven to be useful in learning universal language representations. As a state-of-the-art model, BERT (Bidirectional Encoder Representations from Transformers) achieved amazing results many understanding tasks. In this paper, we conduct exhaustive experiments investigate different fine-tuning methods of on text classification task and provide general solution for fine-tuning. Finally, the proposed obtains new eight widely-studied datasets."
https://openalex.org/W2964345792,https://doi.org/10.1109/cvpr.2018.00142,MAttNet: Modular Attention Network for Referring Expression Comprehension,2018,"In this paper, we address referring expression comprehension: localizing an image region described by a natural language expression. While most recent work treats expressions as single unit, propose to decompose them into three modular components related subject appearance, location, and relationship other objects. This allows us flexibly adapt containing different types of information in end-to-end framework. our model, which call the Modular Attention Network (MAttNet), two attention are utilized: language-based that learns module weights well word/phrase each should focus on; visual modules on relevant components. Module combine scores from all dynamically output overall score. Experiments show MAttNet outperforms previous state-of-art methods large margin both bounding-box-level pixel-level comprehension tasks. Demo code provided."
https://openalex.org/W2740550900,https://doi.org/10.18653/v1/p17-1081,Context-Dependent Sentiment Analysis in User-Generated Videos,2017,"Multimodal sentiment analysis is a developing area of research, which involves the identification sentiments in videos. Current research considers utterances as independent entities, i.e., ignores interdependencies and relations among video. In this paper, we propose LSTM-based model that enables to capture contextual information from their surroundings same video, thus aiding classification process. Our method shows 5-10% performance improvement over state art high robustness generalizability."
https://openalex.org/W2591644541,https://doi.org/10.1109/cvpr.2017.331,Visual Translation Embedding Network for Visual Relation Detection,2017,"Visual relations, such as ""person ride bike"" and ""bike next to car"", offer a comprehensive scene understanding of an image, have already shown their great utility in connecting computer vision natural language. However, due the challenging combinatorial complexity modeling subject-predicate-object relation triplets, very little work has been done localize predict visual relations. Inspired by recent advances relational representation learning knowledge bases convolutional object detection networks, we propose Translation Embedding network (VTransE) for detection. VTransE places objects low-dimensional space where can be modeled simple vector translation, i.e., subject + predicate $\approx$ object. We novel feature extraction layer that enables object-relation transfer fully-convolutional fashion supports training inference single forward/backward pass. To best our knowledge, is first end-to-end network. demonstrate effectiveness over other state-of-the-art methods on two large-scale datasets: Relationship Genome. Note even though purely model, it still competitive Lu's multi-modal model with language priors."
https://openalex.org/W2963115613,https://doi.org/10.1109/cvpr.2019.00688,From Recognition to Cognition: Visual Commonsense Reasoning,2019,"Visual understanding goes well beyond object recognition. With one glance at an image, we can effortlessly imagine the world pixels: for instance, infer people's actions, goals, and mental states. While this task is easy humans, it tremendously difficult today's vision systems, requiring higher-order cognition commonsense reasoning about world. We formalize as Commonsense Reasoning. Given a challenging question machine must answer correctly then provide rationale justifying its answer. Next, introduce new dataset, VCR, consisting of 290k multiple choice QA problems derived from 110k movie scenes. The key recipe generating non-trivial high-quality scale Adversarial Matching, approach to transform rich annotations into questions with minimal bias. Experimental results show that while humans find VCR (over 90% accuracy), state-of-the-art models struggle (~45%). To move towards cognition-level understanding, present engine, Recognition Cognition Networks (R2C), necessary layered inferences grounding, contextualization, reasoning. R2C helps narrow gap between machines (~65%); still, challenge far solved, analysis suggests avenues future work."
https://openalex.org/W2740721704,https://doi.org/10.18653/v1/p17-1052,Deep Pyramid Convolutional Neural Networks for Text Categorization,2017,"This paper proposes a low-complexity word-level deep convolutional neural network (CNN) architecture for text categorization that can efficiently represent long-range associations in text. In the literature, several and complex networks have been proposed this task, assuming availability of relatively large amounts training data. However, associated computational complexity increases as go deeper, which poses serious challenges practical applications. Moreover, it was shown recently shallow CNNs are more accurate much faster than state-of-the-art very nets such character-level even setting Motivated by these findings, we carefully studied deepening to capture global representations text, found simple with best accuracy be obtained increasing depth without cost much. We call pyramid CNN. The model 15 weight layers outperforms previous models on six benchmark datasets sentiment classification topic categorization."
https://openalex.org/W2963191264,https://doi.org/10.1109/cvpr.2016.499,Where to Look: Focus Regions for Visual Question Answering,2016,"We present a method that learns to answer visual questions by selecting image regions relevant the text-based query. Our maps textual queries and features from various into shared space where they are compared for relevance with an inner product. exhibits significant improvements in answering such as what color, it is necessary evaluate specific location, room, selectively identifies informative regions. model tested on recently released VQA [1] dataset, which free-form human-annotated answers."
https://openalex.org/W2339589954,https://doi.org/10.1109/cvpr.2016.451,Multi-oriented Text Detection with Fully Convolutional Networks,2016,"In this paper, we propose a novel approach for text detec- tion in natural images. Both local and global cues are taken into account localizing lines coarse-to-fine pro- cedure. First, Fully Convolutional Network (FCN) model is trained to predict the salient map of regions holistic manner. Then, line hypotheses estimated by combining character components. Fi- nally, another FCN classifier used centroid each character, order remove false hypotheses. The framework general handling multiple ori- entations, languages fonts. proposed method con- sistently achieves state-of-the-art performance on three detection benchmarks: MSRA-TD500, ICDAR2015 ICDAR2013."
https://openalex.org/W2799007037,https://doi.org/10.18653/v1/p18-1079,Semantically Equivalent Adversarial Rules for Debugging NLP models,2018,"Complex machine learning models for NLP are often brittle, making different predictions input instances that extremely similar semantically. To automatically detect this behavior individual instances, we present semantically equivalent adversaries (SEAs) – semantic-preserving perturbations induce changes in the model’s predictions. We generalize these into adversarial rules (SEARs) simple, universal replacement on many instances. demonstrate usefulness and flexibility of SEAs SEARs by detecting bugs black-box state-of-the-art three domains: comprehension, visual question-answering, sentiment analysis. Via user studies, generate high-quality local more than humans, four times as mistakes discovered human experts. also actionable: retraining using data augmentation significantly reduces bugs, while maintaining accuracy."
https://openalex.org/W2962809918,https://doi.org/10.18653/v1/p16-1223,A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task,2016,"Enabling a computer to understand document so that it can answer comprehension questions is central, yet unsolved goal of NLP. A key factor impeding its solution by machine learned systems the limited availability human-annotated data. Hermann et al. (2015) seek solve this problem creating over million training examples pairing CNN and Daily Mail news articles with their summarized bullet points, show neural network then be trained give good performance on task. In paper, we conduct thorough examination new reading Our primary aim what depth language understanding required do well We approach from one side doing careful hand-analysis small subset problems other showing simple, carefully designed obtain accuracies 73.6% 76.6% these two datasets, exceeding current state-of-the-art results 7-10% approaching believe ceiling for"
https://openalex.org/W2410983263,https://doi.org/10.48550/arxiv.1606.01541,Deep Reinforcement Learning for Dialogue Generation,2016,"Recent neural models of dialogue generation offer great promise for generating responses conversational agents, but tend to be shortsighted, predicting utterances one at a time while ignoring their influence on future outcomes. Modeling the direction is crucial coherent, interesting dialogues, need which led traditional NLP draw reinforcement learning. In this paper, we show how integrate these goals, applying deep learning model reward in chatbot dialogue. The simulates dialogues between two virtual using policy gradient methods sequences that display three useful properties: informativity (non-repetitive turns), coherence, and ease answering (related forward-looking function). We evaluate our diversity, length as well with human judges, showing proposed algorithm generates more interactive manages foster sustained conversation simulation. This work marks first step towards based long-term success dialogues."
https://openalex.org/W2963653811,https://doi.org/10.18653/v1/d17-1209,Graph Convolutional Encoders for Syntax-aware Neural Machine Translation,2017,"We present a simple and effective approach to incorporating syntactic structure into neural attention-based encoder-decoder models for machine translation. rely on graph-convolutional networks (GCNs), recent class of developed modeling graph-structured data. Our GCNs use predicted dependency trees source sentences produce representations words (i.e. hidden states the encoder) that are sensitive their neighborhoods. take word as input output, so they can easily be incorporated layers standard encoders (e.g., top bidirectional RNNs or convolutional networks). evaluate effectiveness with English-German English-Czech translation experiments different types observe substantial improvements over syntax-agnostic versions in all considered setups."
https://openalex.org/W2251622960,https://doi.org/10.3115/v1/w15-1506,Relation Extraction: Perspective from Convolutional Neural Networks,2015,"Up to now, relation extraction systems have made extensive use of features generated by linguistic analysis modules. Errors in these lead errors detection and classification. In this work, we depart from traditional approaches with complicated feature engineering introducing a convolutional neural network for that automatically learns sentences minimizes the dependence on external toolkits resources. Our model takes advantages multiple window sizes filters pre-trained word embeddings as an initializer non-static architecture improve performance. We emphasize problem unbalanced corpus. The experimental results show our system significantly outperforms not only best baseline but also state-of-the-art"
https://openalex.org/W2963667126,https://doi.org/10.18653/v1/n18-1169,"Delete, Retrieve, Generate: a Simple Approach to Sentiment and Style Transfer",2018,"We consider the task of text attribute transfer: transforming a sentence to alter specific (e.g., sentiment) while preserving its attribute-independent content changing ""screen is just right size"" too small""). Our training data includes only sentences labeled with their positive or negative), but not pairs that differ in attributes, so we must learn disentangle attributes from an unsupervised way. Previous work using adversarial methods has struggled produce high-quality outputs. In this paper, propose simpler motivated by observation are often marked distinctive phrases ""too strongest method extracts words deleting associated sentence's original value, retrieves new target attribute, and uses neural model fluently combine these into final output. On human evaluation, our best generates grammatical appropriate responses on 22% more inputs than previous system, averaged over three transfer datasets: altering sentiment reviews Yelp, Amazon, image captions be romantic humorous."
https://openalex.org/W2950339735,https://doi.org/10.18653/v1/p19-1470,COMET: Commonsense Transformers for Automatic Knowledge Graph Construction,2019,"We present the first comprehensive study on automatic knowledge base construction for two prevalent commonsense graphs: ATOMIC (Sap et al., 2019) and ConceptNet (Speer 2017). Contrary to many conventional KBs that store with canonical templates, only loosely structured open-text descriptions of knowledge. posit an important step toward completion is development generative models knowledge, propose COMmonsEnse Transformers (COMET) learn generate rich diverse in natural language. Despite challenges modeling, our investigation reveals promising results when implicit from deep pre-trained language transferred explicit graphs. Empirical demonstrate COMET able novel humans rate as high quality, up 77.5% (ATOMIC) 91.7% (ConceptNet) precision at top 1, which approaches human performance these resources. Our findings suggest using KB could soon be a plausible alternative extractive methods."
https://openalex.org/W2531207078,https://doi.org/10.1162/tacl_a_00067,Fully Character-Level Neural Machine Translation without Explicit Segmentation,2017,"Most existing machine translation systems operate at the level of words, relying on explicit segmentation to extract tokens. We introduce a neural (NMT) model that maps source character sequence target without any segmentation. employ character-level convolutional network with max-pooling encoder reduce length representation, allowing be trained speed comparable subword-level models while capturing local regularities. Our character-to-character outperforms recently proposed baseline WMT’15 DE-EN and CS-EN, gives performance FI-EN RU-EN. then demonstrate it is possible share single across multiple languages by training many-to-one task. In this multilingual setting, significantly all language pairs. observe RU-EN, quality even surpasses specifically pair alone, both in terms BLEU score human judgment."
https://openalex.org/W2970986510,https://doi.org/10.18653/v1/d19-1005,Knowledge Enhanced Contextual Word Representations,2019,"Contextual word representations, typically trained on unstructured, unlabeled text, do not contain any explicit grounding to real world entities and are often unable remember facts about those entities. We propose a general method embed multiple knowledge bases (KBs) into large scale models, thereby enhance their representations with structured, human-curated knowledge. For each KB, we first use an integrated entity linker retrieve relevant embeddings, then update contextual via form of word-to-entity attention. In contrast previous approaches, the linkers self-supervised language modeling objective jointly end-to-end in multitask setting that combines small amount linking supervision raw text. After integrating WordNet subset Wikipedia BERT, enhanced BERT (KnowBert) demonstrates improved perplexity, ability recall as measured probing task downstream performance relationship extraction, typing, sense disambiguation. KnowBert’s runtime is comparable BERT’s it scales KBs."
https://openalex.org/W2294774419,https://doi.org/10.3115/v1/n15-1104,Normalized Word Embedding and Orthogonal Transform for Bilingual Word Translation,2015,"Word embedding has been found to be highly powerful translate words from one language another by a simple linear transform. However, we some inconsistence among the objective functions of and transform learning, as well distance measurement. This paper proposes solution which normalizes word vectors on hypersphere constrains an orthogonal The experimental results confirmed that proposed can offer better performance similarity task English-toSpanish translation task."
https://openalex.org/W2752234108,https://doi.org/10.1073/pnas.1702247114,Self-report captures 27 distinct categories of emotion bridged by continuous gradients,2017,"Significance Claims about how reported emotional experiences are geometrically organized within a semantic space have shaped the study of emotion. Using statistical methods to analyze reports states elicited by 2,185 emotionally evocative short videos with richly varying situational content, we uncovered 27 varieties experience. Reported experience is better captured categories such as “amusement” than ratings widely measured affective dimensions valence and arousal. Although found organize dimensional appraisals in coherent powerful fashion, many linked smooth gradients, contrary discrete theories. Our results comprise an approximation geometric structure"
https://openalex.org/W2963526187,https://doi.org/10.18653/v1/n18-2003,Gender Bias in Coreference Resolution: Evaluation and Debiasing Methods,2018,"We introduce a new benchmark, WinoBias, for coreference resolution focused on gender bias. Our corpus contains Winograd-schema style sentences with entities corresponding to people referred by their occupation (e.g. the nurse, doctor, carpenter). demonstrate that rule-based, feature-rich, and neural system all link gendered pronouns pro-stereotypical higher accuracy than anti-stereotypical entities, an average difference of 21.1 in F1 score. Finally, we data-augmentation approach that, combination existing word-embedding debiasing techniques, removes bias demonstrated these systems WinoBias without significantly affecting performance benchmark datasets. dataset code are available at http://winobias.org."
https://openalex.org/W2739046565,https://doi.org/10.18653/v1/d17-1239,Challenges in Data-to-Document Generation,2017,"Recent neural models have shown significant progress on the problem of generating short descriptive texts conditioned a small number database records. In this work, we suggest slightly more difficult data-to-text generation task, and investigate how effective current approaches are task. particular, introduce new, large-scale corpus data records paired with documents, propose series extractive evaluation methods for analyzing performance, obtain baseline results using methods. Experiments show that these produce fluent text, but fail to convincingly approximate human-generated documents. Moreover, even templated baselines exceed performance some metrics, though copy- reconstruction-based extensions lead noticeable improvements."
https://openalex.org/W2949678053,https://doi.org/10.18653/v1/p19-1163,The Risk of Racial Bias in Hate Speech Detection,2019,"We investigate how annotators’ insensitivity to differences in dialect can lead racial bias automatic hate speech detection models, potentially amplifying harm against minority populations. first uncover unexpected correlations between surface markers of African American English (AAE) and ratings toxicity several widely-used datasets. Then, we show that models trained on these corpora acquire propagate biases, such AAE tweets by self-identified Americans are up two times more likely be labelled as offensive compared others. Finally, propose *dialect* *race priming* ways reduce the annotation, showing when annotators made explicitly aware an tweet’s they significantly less label tweet offensive."
https://openalex.org/W3098232790,https://doi.org/10.1007/978-3-319-46448-0_49,Grounding of Textual Phrases in Images by Reconstruction,2016,"Grounding (i.e. localizing) arbitrary, free-form textual phrases in visual content is a challenging problem with many applications for human-computer interaction and image-text reference resolution. Few datasets provide the ground truth spatial localization of phrases, thus it desirable to learn from data no or little grounding supervision. We propose novel approach which learns by reconstructing given phrase using an attention mechanism, can be either latent optimized directly. During training our encodes recurrent network language model then attend relevant image region order reconstruct input phrase. At test time, correct attention, i.e., grounding, evaluated. If supervision available directly applied via loss over mechanism. demonstrate effectiveness on Flickr 30k Entities ReferItGame different levels supervision, ranging partial full Our supervised variant improves large margin state-of-the-art both datasets."
https://openalex.org/W2963649796,https://doi.org/10.1109/iccv.2017.142,"Scene Graph Generation from Objects, Phrases and Region Captions",2017,"Object detection, scene graph generation and region captioning, which are three understanding tasks at different semantic levels, tied together: graphs generated on top of objects detected in an image with their pairwise relationship predicted, while captioning gives a language description the objects, attributes, relations other context information. In this work, to leverage mutual connections across we propose novel neural network model, termed as Multi-level Scene Description Network (denoted MSDN), solve vision jointly end-to-end manner. Object, phrase, caption regions first aligned dynamic based spatial connections. Then feature refining structure is used pass messages levels through graph. We benchmark learned model tasks, show joint learning our proposed method can bring improvements over previous models. Particularly, task, outperforms stateof- art more than 3% margin. Code has been made publicly available."
https://openalex.org/W1981208470,https://doi.org/10.1186/s12859-015-0564-6,An overview of the BIOASQ large-scale biomedical semantic indexing and question answering competition,2015,"This article provides an overview of the first BIOASQ challenge, a competition on large-scale biomedical semantic indexing and question answering (QA), which took place between March September 2013. assesses ability systems to semantically index very large numbers scientific articles, return concise user-understandable answers given natural language questions by combining information from articles ontologies.The 2013 comprised two tasks, Task 1a 1b. In participants were asked automatically annotate new PUBMED documents with MESH headings. Twelve teams participated in 1a, total 46 system runs submitted, one performing consistently better than MTI indexer used NLM suggest headings curators. 1b benchmark datasets containing 29 development 282 test English questions, along gold standard (reference) answers, prepared team experts around Europe had produce answers. Three 1b, 11 runs. The infrastructure, including datasets, evaluation mechanisms, results baseline methods, is publicly available.A available infrastructure for QA has been developed, includes can be evaluate that: assign published or questions; retrieve relevant RDF triples ontologies, snippets Central; ""exact"" paragraph-sized ""ideal"" (summaries). that are promising. performed NLM's indexer. received high scores manual answers; hence, they produced quality summaries as Overall, helped obtain unified view how techniques text classification, indexing, document passage retrieval, answering, summarization combined allow concise, reflecting their real needs."
https://openalex.org/W2906152891,https://doi.org/10.1162/tacl_a_00254,Analysis Methods in Neural Language Processing: A Survey,2019,"Abstract The field of natural language processing has seen impressive progress in recent years, with neural network models replacing many the traditional systems. A plethora new have been proposed, which are thought to be opaque compared their feature-rich counterparts. This led researchers analyze, interpret, and evaluate networks novel more fine-grained ways. In this survey paper, we review analysis methods processing, categorize them according prominent research trends, highlight existing limitations, point potential directions for future work."
https://openalex.org/W2962972512,https://doi.org/10.18653/v1/n18-1158,Ranking Sentences for Extractive Summarization with Reinforcement Learning,2018,Single document summarization is the task of producing a shorter version while preserving its principal information content. In this paper we conceptualize extractive as sentence ranking and propose novel training algorithm which globally optimizes ROUGE evaluation metric through reinforcement learning objective. We use our to train neural model on CNN DailyMail datasets demonstrate experimentally that it outperforms state-of-the-art abstractive systems when evaluated automatically by humans.
https://openalex.org/W2251394420,https://doi.org/10.18653/v1/d15-1303,Deep Convolutional Neural Network Textual Features and Multiple Kernel Learning for Utterance-level Multimodal Sentiment Analysis,2015,"We present a novel way of extracting features from short texts, based on the activation values an inner layer deep convolutional neural network. use extracted in multimodal sentiment analysis video clips representing one sentence each. combined feature vectors textual, visual, and audio modalities to train classifier multiple kernel learning, which is known be good at heterogeneous data. obtain 14% performance improvement over state art parallelizable decision-level data fusion method, much faster, though slightly less accurate."
https://openalex.org/W2750747353,https://doi.org/10.18653/v1/s17-2126,DataStories at SemEval-2017 Task 4: Deep LSTM with Attention for Message-level and Topic-based Sentiment Analysis,2017,"In this paper we present two deep-learning systems that competed at SemEval-2017 Task 4 “Sentiment Analysis in Twitter”. We participated all subtasks for English tweets, involving message-level and topic-based sentiment polarity classification quantification. use Long Short-Term Memory (LSTM) networks augmented with kinds of attention mechanisms, on top word embeddings pre-trained a big collection Twitter messages. Also, text processing tool suitable social network messages, which performs tokenization, normalization, segmentation spell correction. Moreover, our approach uses no hand-crafted features or lexicons. ranked 1st (tie) Subtask A, achieved very competitive results the rest Subtasks. Both are available to research community."
https://openalex.org/W2967615747,https://doi.org/10.1109/cvpr.2019.00959,Character Region Awareness for Text Detection,2019,"Scene text detection methods based on neural networks have emerged recently and shown promising results. Previous trained with rigid word-level bounding boxes exhibit limitations in representing the region an arbitrary shape. In this paper, we propose a new scene method to effectively detect area by exploring each character affinity between characters. To overcome lack of individual level annotations, our proposed framework exploits both given character-level annotations for synthetic images estimated ground-truths real acquired learned interim model. order estimate characters, network is newly representation affinity. Extensive experiments six benchmarks, including TotalText CTW-1500 datasets which contain highly curved texts natural images, demonstrate that significantly outperforms state-of-the-art detectors. According results, guarantees high flexibility detecting complicated such as arbitrarily-oriented, curved, or deformed texts."
https://openalex.org/W2982756474,https://doi.org/10.18653/v1/d19-1221,Universal Adversarial Triggers for Attacking and Analyzing NLP,2019,"Adversarial examples highlight model vulnerabilities and are useful for evaluation interpretation. We define universal adversarial triggers: input-agnostic sequences of tokens that trigger a to produce specific prediction when concatenated any input from dataset. propose gradient-guided search over which finds short (e.g., one word classification four words language modeling) successfully the target prediction. For example, triggers cause SNLI entailment accuracy drop 89.94% 0.55%, 72% “why” questions in SQuAD be answered “to kill american people”, GPT-2 spew racist output even conditioned on non-racial contexts. Furthermore, although optimized using white-box access model, they transfer other models all tasks we consider. Finally, since input-agnostic, provide an analysis global behavior. instance, confirm exploit dataset biases help diagnose heuristics learned by reading comprehension models."
https://openalex.org/W2462305634,https://doi.org/10.18653/v1/s16-1081,"SemEval-2016 Task 1: Semantic Textual Similarity, Monolingual and Cross-Lingual Evaluation",2016,"Comunicacio presentada al 10th International Workshop on Semantic Evaluation (SemEval-2016), celebrat els dies 16 i 17 de juny 2016 a San Diego, California."
https://openalex.org/W2963020213,https://doi.org/10.1162/tacl_a_00049,Cross-Sentence <i>N</i>-ary Relation Extraction with Graph LSTMs,2017,"Past work in relation extraction has focused on binary relations single sentences. Recent NLP inroads high-value domains have sparked interest the more general setting of extracting n-ary that span multiple In this paper, we explore a framework based graph long short-term memory networks (graph LSTMs) can be easily extended to cross-sentence extraction. The formulation provides unified way exploring different LSTM approaches and incorporating various intra-sentential inter-sentential dependencies, such as sequential, syntactic, discourse relations. A robust contextual representation is learned for entities, which serves input classifier. This simplifies handling with arbitrary arity, enables multi-task learning related We evaluate two important precision medicine settings, demonstrating its effectiveness both conventional supervised distant supervision. Cross-sentence produced larger knowledge bases. significantly improved accuracy. thorough analysis yielded useful insight impact linguistic"
https://openalex.org/W2964008635,https://doi.org/10.1609/aaai.v32i1.11330,Style Transfer in Text: Exploration and Evaluation,2018,"The ability to transfer styles of texts or images, is an important measurement the advancement artificial intelligence (AI). However, progress in language style lagged behind other domains, such as computer vision, mainly because lack parallel data and reliable evaluation metrics. In response challenge lacking data, we explore learning from non-parallel data. We propose two models achieve this goal. key idea proposed learn separate content representations using adversarial networks. Considering problem principle metrics, novel metrics that measure aspects transfer: strength preservation. benchmark our on tasks: paper-news title transfer, positive-negative review transfer. Results show preservation metric highly correlate human judgments, are able generate sentences with similar score but higher comparing auto-encoder."
https://openalex.org/W2964018924,https://doi.org/10.1109/cvpr.2016.29,What Value Do Explicit High Level Concepts Have in Vision to Language Problems?,2016,"Much recent progress in Vision-to-Language (V2L) problems has been achieved through a combination of Convolutional Neural Networks (CNNs) and Recurrent (RNNs). This approach does not explicitly represent high-level semantic concepts, but rather seeks to directly from image features text. In this paper we investigate whether direct succeeds due to, or despite, the fact that it avoids explicit representation information. We propose method incorporating concepts into successful CNN-RNN approach, show achieves significant improvement on state-of-the-art both captioning visual question answering. also same mechanism can be used introduce external information doing so further improves performance. achieve best reported results VQA several benchmark datasets, provide an analysis value V2L problems."
https://openalex.org/W3104723404,https://doi.org/10.1613/jair.1.11640,A Survey of Cross-lingual Word Embedding Models,2019,"Cross-lingual representations of words enable us to reason about word meaning in multilingual contexts and are a key facilitator cross-lingual transfer when developing natural language processing models for low-resource languages. In this survey, we provide comprehensive typology embedding models. We compare their data requirements objective functions. The recurring theme the survey is that many presented literature optimize same objectives, seemingly different often equivalent, modulo optimization strategies, hyper-parameters, such. also discuss ways embeddings evaluated, as well future challenges research horizons."
https://openalex.org/W2805744755,https://doi.org/10.18653/v1/s18-1001,SemEval-2018 Task 1: Affect in Tweets,2018,"We present the SemEval-2018 Task 1: Affect in Tweets, which includes an array of subtasks on inferring affectual state a person from their tweet. For each task, we created labeled data English, Arabic, and Spanish tweets. The individual tasks are: 1. emotion intensity regression, 2. ordinal classification, 3. valence (sentiment) 4. 5. classification. Seventy-five teams (about 200 team members) participated shared task. summarize methods, resources, tools used by participating teams, with focus techniques resources that are particularly useful. also analyze systems for consistent bias towards particular race or gender. is made freely available to further improve our understanding how people convey emotions through language."
https://openalex.org/W3019166713,https://doi.org/10.1109/tnnls.2020.2979670,A Survey of the Usages of Deep Learning for Natural Language Processing,2021,"Over the last several years, field of natural language processing has been propelled forward by an explosion in use deep learning models. This article provides a brief introduction to and quick overview architectures methods. It then sifts through plethora recent studies summarizes large assortment relevant contributions. Analyzed research areas include core linguistic issues addition many applications computational linguistics. A discussion current state art is provided along with recommendations for future field."
https://openalex.org/W1915251500,,On using monolingual corpora in neural machine translation,2015,"Recent work on end-to-end neural network-based architectures for machine translation has shown promising results En-Fr and En-De translation. Arguably, one of the major factors behind this success been availability high quality parallel corpora. In work, we investigate how to leverage abundant monolingual corpora Compared a phrase-based hierarchical baseline, obtain up $1.96$ BLEU improvement low-resource language pair Turkish-English, $1.59$ focused domain task Chinese-English chat messages. While our method was initially targeted toward such tasks with less data, show that it also extends resource languages as Cs-En De-En where an $0.39$ $0.47$ scores over baselines, respectively."
https://openalex.org/W2972818416,https://doi.org/10.1109/asru46091.2019.9003750,A Comparative Study on Transformer vs RNN in Speech Applications,2019,"Sequence-to-sequence models have been widely used in end-to-end speech processing, for example, automatic recognition (ASR), translation (ST), and text-to-speech (TTS). This paper focuses on an emergent sequence-to-sequence model called Transformer, which achieves state-of-the-art performance neural machine other natural language processing applications. We undertook intensive studies we experimentally compared analyzed Transformer conventional recurrent networks (RNN) a total of 15 ASR, one multilingual ST, two TTS benchmarks. Our experiments revealed various training tips significant benefits obtained with each task including the surprising superiority 13/15 ASR benchmarks comparison RNN. are preparing to release Kaldi-style reproducible recipes using open source publicly available datasets all tasks community succeed our exciting outcomes."
https://openalex.org/W2963963993,https://doi.org/10.1162/tacl_a_00023,The NarrativeQA Reading Comprehension Challenge,2018,"Reading comprehension (RC)—in contrast to information retrieval—requires integrating and reasoning about events, entities, their relations across a full document. Question answering is conventionally used assess RC ability, in both artificial agents children learning read. However, existing datasets tasks are dominated by questions that can be solved selecting answers using superficial (e.g., local context similarity or global term frequency); they thus fail test for the essential integrative aspect of RC. To encourage progress on deeper language, we present new dataset set which reader must answer stories reading entire books movie scripts. These designed so successfully requires understanding underlying narrative rather than relying shallow pattern matching salience. We show although humans solve easily, standard models struggle presented here. provide an analysis challenges it presents."
https://openalex.org/W2964101860,https://doi.org/10.18653/v1/p17-1163,Neural Belief Tracker: Data-Driven Dialogue State Tracking,2017,"One of the core components modern spoken dialogue systems is belief tracker, which estimates user’s goal at every step dialogue. However, most current approaches have difficulty scaling to larger, more complex domains. This due their dependency on either: a) Spoken Language Understanding models that require large amounts annotated training data; or b) hand-crafted lexicons for capturing some linguistic variation in users’ language. We propose a novel Neural Belief Tracking (NBT) framework overcomes these problems by building recent advances representation learning. NBT reason over pre-trained word vectors, learning compose them into distributed representations user utterances and context. Our evaluation two datasets shows this approach surpasses past limitations, matching performance state-of-the-art rely semantic outperforming when such are not provided."
https://openalex.org/W2970454332,https://doi.org/10.18653/v1/d19-1441,Patient Knowledge Distillation for BERT Model Compression,2019,"Pre-trained language models such as BERT have proven to be highly effective for natural processing (NLP) tasks. However, the high demand computing resources in training hinders their application practice. In order alleviate this resource hunger large-scale model training, we propose a Patient Knowledge Distillation approach compress an original large (teacher) into equally-effective lightweight shallow network (student). Different from previous knowledge distillation methods, which only use output last layer of teacher distillation, our student patiently learns multiple intermediate layers incremental extraction, following two strategies: (i) PKD-Last: learning k layers; and (ii) PKD-Skip: every layers. These patient schemes enable exploitation rich information teacher’s hidden layers, encourage learn imitate through multi-layer process. Empirically, translates improved results on NLP tasks with significant gain efficiency, without sacrificing accuracy."
https://openalex.org/W2558809543,https://doi.org/10.1109/cvpr.2017.475,GuessWhat?! Visual Object Discovery through Multi-modal Dialogue,2017,"We introduce GuessWhat?!, a two-player guessing game as testbed for research on the interplay of computer vision and dialogue systems. The goal is to locate an unknown object in rich image scene by asking sequence questions. Higher-level understanding, like spatial reasoning language grounding, required solve proposed task. Our key contribution collection large-scale dataset consisting 150K human-played games with total 800K visual question-answer pairs 66K images. explain our design decisions collecting oracle questioner tasks that are associated two players game. prototyped deep learning models establish initial baselines introduced tasks."
https://openalex.org/W3090449556,https://doi.org/10.1007/978-3-030-58577-8_7,UNITER: UNiversal Image-TExt Representation Learning,2020,"Joint image-text embedding is the bedrock for most Vision-and-Language (V+L) tasks, where multimodality inputs are simultaneously processed joint visual and textual understanding. In this paper, we introduce UNITER, a UNiversal Image-TExt Representation, learned through large-scale pre-training over four datasets (COCO, Visual Genome, Conceptual Captions, SBU Captions), which can power heterogeneous downstream V+L tasks with multimodal embeddings. We design tasks: Masked Language Modeling (MLM), Region (MRM, three variants), Image-Text Matching (ITM), Word-Region Alignment (WRA). Different from previous work that applies random masking to both modalities, use conditional on (i.e., masked language/region modeling conditioned full observation of image/text). addition ITM global alignment, also propose WRA via Optimal Transport (OT) explicitly encourage fine-grained alignment between words image regions during pre-training. Comprehensive analysis shows OT-based contribute better conduct thorough ablation study find an optimal combination tasks. Extensive experiments show UNITER achieves new state art across six (over nine datasets), including Question Answering, Retrieval, Referring Expression Comprehension, Commonsense Reasoning, Entailment, NLVR\(^2\) (Code available at https://github.com/ChenRocks/UNITER.)."
https://openalex.org/W2607855566,https://doi.org/10.1109/cvpr.2017.352,Detecting Visual Relationships with Deep Relational Networks,2017,"Relationships among objects play a crucial role in image understanding. Despite the great success of deep learning techniques recognizing individual objects, reasoning about relationships remains challenging task. Previous methods often treat this as classification problem, considering each type relationship (e.g. ""ride"") or distinct visual phrase ""person-ride-horse"") category. Such approaches are faced with significant difficulties caused by high diversity appearance for kind large number phrases. We propose an integrated framework to tackle problem. At heart is Deep Relational Network, novel formulation designed specifically exploiting statistical dependencies between and their relationships. On two datasets, proposed method achieves substantial improvement over state-of-the-art."
https://openalex.org/W2963084773,https://doi.org/10.18653/v1/p18-1249,Constituency Parsing with a Self-Attentive Encoder,2018,"We demonstrate that replacing an LSTM encoder with a self-attentive architecture can lead to improvements state-of-the-art discriminative constituency parser. The use of attention makes explicit the manner in which information is propagated between different locations sentence, we both analyze our model and propose potential improvements. For example, find separating positional content improved parsing accuracy. Additionally, evaluate approaches for lexical representation. Our parser achieves new results single models trained on Penn Treebank: 93.55 F1 without any external data, 95.13 when using pre-trained word representations. also outperforms previous best-published accuracy figures 8 9 languages SPMRL dataset."
https://openalex.org/W2558834163,https://doi.org/10.1109/cvpr.2017.127,Semantic Compositional Networks for Visual Captioning,2017,"A Semantic Compositional Network (SCN) is developed for image captioning, in which semantic concepts (i.e., tags) are detected from the image, and probability of each tag used to compose parameters a long short-term memory (LSTM) network. The SCN extends weight matrix LSTM an ensemble tag-dependent matrices. degree member generate caption tied image-dependent corresponding tag. In addition captioning images, we also extend captions video clips. We qualitatively analyze composition SCNs, quantitatively evaluate algorithm on three benchmark datasets: COCO, Flickr30k, Youtube2Text. Experimental results show that proposed method significantly outperforms prior state-of-the-art approaches, across multiple evaluation metrics."
https://openalex.org/W2251427843,https://doi.org/10.18653/v1/d15-1181,Multi-Perspective Sentence Similarity Modeling with Convolutional Neural Networks,2015,"Modeling sentence similarity is complicated by the ambiguity and variability of linguistic expression. To cope with these challenges, we propose a model for comparing sentences that uses multiplicity perspectives. We first each using convolutional neural network extracts features at multiple levels granularity types pooling. then compare our representations several granularities metrics. apply to three tasks, including Microsoft Research paraphrase identification task two SemEval semantic textual tasks. obtain strong performance on all rivaling or exceeding state art without external resources such as WordNet parsers."
https://openalex.org/W2884276923,https://doi.org/10.1145/3196321.3196334,Deep code comment generation,2018,"During software maintenance, code comments help developers comprehend programs and reduce additional time spent on reading navigating source code. Unfortunately, these are often mismatched, missing or outdated in the projects. Developers have to infer functionality from This paper proposes a new approach named DeepCom automatically generate for Java methods. The generated aim understand of applies Natural Language Processing (NLP) techniques learn large corpus generates learned features. We use deep neural network that analyzes structural information methods better generation. conduct experiments large-scale built 9,714 open projects GitHub. evaluate experimental results machine translation metric. Experimental demonstrate our method outperforms state-of-the-art by substantial margin."
https://openalex.org/W2962718483,https://doi.org/10.18653/v1/p18-1078,Simple and Effective Multi-Paragraph Reading Comprehension,2018,"We introduce a method of adapting neural paragraph-level question answering models to the case where entire documents are given as input. Most current cannot scale document or multi-document input, and naively applying these each paragraph independently often results in them being distracted by irrelevant text. show that it is possible significantly improve performance using modified training scheme teaches model ignore non-answer containing paragraphs. Our involves sampling multiple paragraphs from document, an objective function requires produce globally correct output. additionally identify upon number other design decisions arise when working with document-level data. Experiments on TriviaQA SQuAD shows our advances state art, including 10 point gain TriviaQA."
https://openalex.org/W2963617989,https://doi.org/10.18653/v1/p17-1041,A Syntactic Neural Model for General-Purpose Code Generation,2017,"We consider the problem of parsing natural language descriptions into source code written in a general-purpose programming like Python. Existing data-driven methods treat this as generation task without considering underlying syntax target language. Informed by previous work semantic parsing, paper we propose novel neural architecture powered grammar model to explicitly capture prior knowledge. Experiments find an effective way scale up complex programs from descriptions, achieving state-of-the-art results that well outperform and approaches."
https://openalex.org/W2346452181,https://doi.org/10.1093/database/baw068,BioCreative V CDR task corpus: a resource for chemical disease relation extraction,2016,"Community-run, formal evaluations and manually annotated text corpora are critically important for advancing biomedical text-mining research. Recently in BioCreative V, a new challenge was organized the tasks of disease named entity recognition (DNER) chemical-induced (CID) relation extraction. Given nature both tasks, test collection is required to contain disease/chemical annotations same set articles. Despite previous efforts corpus construction, none found be sufficient task. Thus, we developed our own called BC5CDR during by inviting team Medical Subject Headings (MeSH) indexers annotation Comparative Toxicogenomics Database (CTD) curators CID annotation. To ensure high quality productivity, detailed guidelines automatic tools were provided. The resulting consists 1500 PubMed articles with 4409 chemicals, 5818 diseases 3116 chemical-disease interactions. Each includes mention spans normalized concept identifiers, using MeSH as controlled vocabulary. accuracy, entities first captured independently two annotators followed consensus annotation: average inter-annotator agreement (IAA) scores 87.49% 96.05% respectively, according Jaccard similarity coefficient. Our successfully used V should serve valuable resource research community.Database URL: http://www.biocreative.org/tasks/biocreative-v/track-3-cdr/."
https://openalex.org/W2467186984,https://doi.org/10.18653/v1/s15-2078,SemEval-2015 Task 10: Sentiment Analysis in Twitter,2015,"In this paper, we describe the 2015 iteration of SemEval shared task on Sentiment Analysis in Twitter. This was most popular sentiment analysis to date with more than 40 teams participating each last three years. year’s competition consisted five prediction subtasks. Two were reruns from previous years: (A) expressed by a phrase context tweet, and (B) overall tweet. We further included new subtasks asking predict (C) towards topic single (D) set tweets, (E) degree prior polarity phrase."
https://openalex.org/W2791170418,https://doi.org/10.1145/3278721.3278729,Measuring and Mitigating Unintended Bias in Text Classification,2018,"We introduce and illustrate a new approach to measuring mitigating unintended bias in machine learning models. Our definition of is parameterized by test set subset input features. how this can be used evaluate text classifiers using synthetic public corpus comments annotated for toxicity from Wikipedia Talk pages. also demonstrate imbalances training data lead the resulting models, therefore potentially unfair applications. use common demographic identity terms as features on which we measure bias. This technique permits analysis scenario where information authors readers unavailable, so that mitigation must focus content itself. The method an unsupervised based balancing dataset. reduces without compromising overall model quality."
https://openalex.org/W2807873315,https://doi.org/10.24963/ijcai.2018/643,Commonsense Knowledge Aware Conversation Generation with Graph Attention,2018,"Commonsense knowledge is vital to many natural language processing tasks. In this paper, we present a novel open-domain conversation generation model demonstrate how large-scale commonsense can facilitate understanding and generation. Given user post, the retrieves relevant graphs from base then encodes with static graph attention mechanism, which augments semantic information of post thus supports better post. Then, during word generation, attentively reads retrieved triples within each through dynamic mechanism. This first attempt that uses in Furthermore, unlike existing models use (entities) separately independently, our treats as whole, more structured, connected graphs. Experiments show proposed generate appropriate informative responses than state-of-the-art baselines."
https://openalex.org/W3035507081,https://doi.org/10.18653/v1/2020.acl-main.442,Beyond Accuracy: Behavioral Testing of NLP Models with CheckList,2020,"Although measuring held-out accuracy has been the primary approach to evaluate generalization, it often overestimates performance of NLP models, while alternative approaches for evaluating models either focus on individual tasks or specific behaviors. Inspired by principles behavioral testing in software engineering, we introduce CheckList, a task-agnostic methodology models. CheckList includes matrix general linguistic capabilities and test types that facilitate comprehensive ideation, as well tool generate large diverse number cases quickly. We illustrate utility with tests three tasks, identifying critical failures both commercial state-of-art In user study, team responsible sentiment analysis model found new actionable bugs an extensively tested model. another practitioners created twice many tests, almost times users without it."
https://openalex.org/W2952638691,https://doi.org/10.18653/v1/p19-1493,How Multilingual is Multilingual BERT?,2019,"In this paper, we show that Multilingual BERT (M-BERT), released by Devlin et al. (2018) as a single language model pre-trained from monolingual corpora in 104 languages, is surprisingly good at zero-shot cross-lingual transfer, which task-specific annotations one are used to fine-tune the for evaluation another language. To understand why, present large number of probing experiments, showing transfer possible even languages different scripts, works best between typologically similar can train models code-switching, and find translation pairs. From these results, conclude M-BERT does create multilingual representations, but representations exhibit systematic deficiencies affecting certain"
https://openalex.org/W2752201871,https://doi.org/10.18653/v1/s16-1001,SemEval-2016 Task 4: Sentiment Analysis in Twitter,2016,"This paper discusses the fourth year of ”Sentiment Analysis in Twitter Task”. SemEval-2016 Task 4 comprises five subtasks, three which represent a significant departure from previous editions. The first two subtasks are reruns prior years and ask to predict overall sentiment, sentiment towards topic tweet. new focus on variants basic “sentiment classification Twitter” task. variant adopts five-point scale, confers an ordinal character second focuses correct estimation prevalence each class interest, task has been called quantification supervised learning literature. continues be very popular, attracting total 43 teams."
https://openalex.org/W2155454737,https://doi.org/10.3115/v1/p15-1061,Classifying Relations by Ranking with Convolutional Neural Networks,2015,"Relation classification is an important semantic processing task for which state-ofthe-art systems still rely on costly handcrafted features. In this work we tackle the relation using a convolutional neural network that performs by ranking (CR-CNN). We propose new pairwise loss function makes it easy to reduce impact of artificial classes. perform experiments SemEval-2010 Task 8 dataset, designed classifying relationship between two nominals marked in sentence. Using CRCNN, outperform state-of-the-art dataset and achieve F1 84.1 without any Additionally, our experimental results show that: (1) approach more effective than CNN followed softmax classifier; (2) omitting representation class Other improves both precision recall; (3) only word embeddings as input features enough if consider text target nominals."
https://openalex.org/W2251599843,https://doi.org/10.3115/v1/p15-1109,End-to-end learning of semantic role labeling using recurrent neural networks,2015,"Semantic role labeling (SRL) is one of the basic natural language processing (NLP) problems. To this date, most successful SRL systems were built on top some form parsing results (Koomen et al., 2005; Palmer 2010; Pradhan 2013), where pre-defined feature templates over syntactic structure are used. The attempts building an end-to-end learning system without using less (Collobert 2011). In work, we propose to use deep bi-directional recurrent network as for SRL. We take only original text information input feature, any knowledge. proposed algorithm semantic was mainly evaluated CoNLL-2005 shared task and achieved F1 score 81.07. This result outperforms previous state-of-the-art from combination different trees or models. also obtained same conclusion with = 81.27 CoNLL2012 task. As a simplicity, our model computationally efficient that speed 6.7k tokens per second. Our analysis shows better at handling longer sentences than traditional And latent variables implicitly capture sentence."
https://openalex.org/W2806872289,https://doi.org/10.1007/978-3-319-93417-4_48,Detecting Hate Speech on Twitter Using a Convolution-GRU Based Deep Neural Network,2018,"In recent years, the increasing propagation of hate speech on social media and urgent need for effective counter-measures have drawn significant investment from governments, companies, empirical research. Despite a large number emerging scientific studies to address problem, major limitation existing work is lack comparative evaluations, which makes it difficult assess contribution individual works. This paper introduces new method based deep neural network combining convolutional gated recurrent networks. We conduct an extensive evaluation against several baselines state art largest collection publicly available Twitter datasets date, show that compared previously reported results these datasets, our proposed able capture both word sequence order information in short texts, sets benchmark by outperforming 6 out 7 between 1 13% F1. also extend dataset this task creating covering different topics."
https://openalex.org/W2896348597,https://doi.org/10.1145/3295748,A Comprehensive Survey of Deep Learning for Image Captioning,2019,"Generating a description of an image is called captioning. Image captioning requires recognizing the important objects, their attributes, and relationships in image. It also needs to generate syntactically semantically correct sentences. Deep-learning-based techniques are capable handling complexities challenges In this survey article, we aim present comprehensive review existing deep-learning-based techniques. We discuss foundation analyze performances, strengths, limitations. datasets evaluation metrics popularly used automatic"
https://openalex.org/W2952746495,https://doi.org/10.1109/cvpr.2017.367,Lip Reading Sentences in the Wild,2016,"The goal of this work is to recognise phrases and sentences being spoken by a talking face, with or without the audio. Unlike previous works that have focussed on recognising limited number words phrases, we tackle lip reading as an open-world problem - unconstrained natural language sentences, in wild videos. Our key contributions are: (1) 'Watch, Listen, Attend Spell' (WLAS) network learns transcribe videos mouth motion characters; (2) curriculum learning strategy accelerate training reduce overfitting; (3) 'Lip Reading Sentences' (LRS) dataset for visual speech recognition, consisting over 100,000 from British television. WLAS model trained LRS surpasses performance all standard benchmark datasets, often significant margin. This beats professional reader BBC television, also demonstrate information helps improve recognition even when audio available."
https://openalex.org/W2962808042,https://doi.org/10.18653/v1/p18-1234,Aspect Based Sentiment Analysis with Gated Convolutional Networks,2018,"Aspect based sentiment analysis (ABSA) can provide more detailed information than general analysis, because it aims to predict the polarities of given aspects or entities in text. We summarize previous approaches into two subtasks: aspect-category (ACSA) and aspect-term (ATSA). Most employ long short-term memory attention mechanisms polarity concerned targets, which are often complicated need training time. propose a model on convolutional neural networks gating mechanisms, is accurate efficient. First, novel Gated Tanh-ReLU Units selectively output features according aspect entity. The architecture much simpler layer used existing models. Second, computations our could be easily parallelized during training, layers do not have time dependency as LSTM layers, units also work independently. experiments SemEval datasets demonstrate efficiency effectiveness"
https://openalex.org/W2252024663,https://doi.org/10.18653/v1/d15-1168,Fine-grained Opinion Mining with Recurrent Neural Networks and Word Embeddings,2015,"The tasks in fine-grained opinion mining can be regarded as either a token-level sequence labeling problem or semantic compositional task. We propose general class of discriminative models based on recurrent neural networks (RNNs) and word embeddings that successfully applied to such without any taskspecific feature engineering effort. Our experimental results the task target identification show RNNs, using hand-crafted features, outperform feature-rich CRF-based models. framework is flexible, allows us incorporate other linguistic achieves rival top performing systems SemEval-2014."
https://openalex.org/W2966683369,https://doi.org/10.1109/cvpr.2019.00644,Deep Modular Co-Attention Networks for Visual Question Answering,2019,"Visual Question Answering (VQA) requires a fine-grained and simultaneous understanding of both the visual content images textual questions. Therefore, designing an effective `co-attention' model to associate key words in questions with objects is central VQA performance. So far, most successful attempts at co-attention learning have been achieved by using shallow models, deep models show little improvement over their counterparts. In this paper, we propose Modular Co-Attention Network (MCAN) that consists (MCA) layers cascaded depth. Each MCA layer self-attention images, as well guided-attention jointly modular composition two basic attention units. We quantitatively qualitatively evaluate MCAN on benchmark VQA-v2 dataset conduct extensive ablation studies explore reasons behind MCAN's effectiveness. Experimental results demonstrate significantly outperforms previous state-of-the-art. Our best single delivers 70.63$\%$ overall accuracy test-dev set. Code available https://github.com/MILVLG/mcan-vqa."
https://openalex.org/W2010608861,https://doi.org/10.1145/2786805.2786849,Suggesting accurate method and class names,2015,"Descriptive names are a vital part of readable, and hence maintainable, code. Recent progress on automatically suggesting for local variables tantalizes with the prospect replicating that success method class names. However, methods classes is much more difficult. This because good need to be functionally descriptive, but such requires model goes beyond context. We introduce neural probabilistic language source code specifically designed naming problem. Our learns which semantically similar by assigning them locations, called embeddings, in high-dimensional continuous space, way embeddings tend used contexts. These seem contain semantic information about tokens, even though they learned only from statistical co-occurrences tokens. Furthermore, we variant our is, knowledge, first can propose neologisms, have not appeared training corpus. obtain state art results method, class, simpler variable tasks. More broadly, potential wide application within software engineering."
https://openalex.org/W2962788148,https://doi.org/10.18653/v1/d18-1548,Linguistically-Informed Self-Attention for Semantic Role Labeling,2018,"Current state-of-the-art semantic role labeling (SRL) uses a deep neural network with no explicit linguistic features. However, prior work has shown that gold syntax trees can dramatically improve SRL decoding, suggesting the possibility of increased accuracy from modeling syntax. In this work, we present linguistically-informed self-attention (LISA): model combines multi-head multi-task learning across dependency parsing, part-of-speech tagging, predicate detection and SRL. Unlike previous models which require significant pre-processing to prepare features, LISA incorporate using merely raw tokens as input, encoding sequence only once simultaneously perform for all predicates. Syntax is incorporated by training one attention head attend syntactic parents each token. Moreover, if high-quality parse already available, it be beneficially injected at test time without re-training our model. experiments on CoNLL-2005 SRL, achieves new performance predicted predicates standard word embeddings, attaining 2.5 F1 absolute higher than newswire more 3.5 out-of-domain data, nearly 10% reduction in error. On ConLL-2012 English also show an improvement F1. out-performs contextually-encoded (ELMo) representations, 1.0 news 2.0 text."
https://openalex.org/W2963899988,https://doi.org/10.3115/v1/p15-1142,Compositional Semantic Parsing on Semi-Structured Tables,2015,"Two important aspects of semantic parsing for question answering are the breadth knowledge source and depth logical compositionality. While existing work trades off one aspect another, this paper simultaneously makes progress on both fronts through a new task: complex questions semi-structured tables using question-answer pairs as supervision. The central challenge arises from two compounding factors: broader domain results in an open-ended set relations, deeper compositionality combinatorial explosion space forms. We propose logical-form driven algorithm guided by strong typing constraints show that it obtains significant improvements over natural baselines. For evaluation, we created dataset 22,033 Wikipedia tables, which is made publicly available."
https://openalex.org/W2988217457,https://doi.org/10.18653/v1/d19-1006,"How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings",2019,"Replacing static word embeddings with contextualized representations has yielded significant improvements on many NLP tasks. However, just how contextual are the produced by models such as ELMo and BERT? Are there infinitely context-specific for each word, or words essentially assigned one of a finite number word-sense representations? For one, we find that all not isotropic in any layer contextualizing model. While same different contexts still have greater cosine similarity than those two words, this self-similarity is much lower upper layers. This suggests layers produce more representations, like LSTMs task-specific representations. In ELMo, BERT, GPT-2, average, less 5% variance word’s can be explained embedding providing some justification success"
https://openalex.org/W3011574394,https://doi.org/10.1007/s11431-020-1647-3,Pre-trained models for natural language processing: A survey,2020,"Recently, the emergence of pre-trained models (PTMs) has brought natural language processing (NLP) to a new era. In this survey, we provide comprehensive review PTMs for NLP. We first briefly introduce representation learning and its research progress. Then systematically categorize existing based on taxonomy with four perspectives. Next, describe how adapt knowledge downstream tasks. Finally, outline some potential directions future research. This survey is purposed be hands-on guide understanding, using, developing various NLP"
https://openalex.org/W2767784948,https://doi.org/10.1016/j.procs.2017.10.117,AraVec: A set of Arabic Word Embedding Models for use in Arabic NLP,2017,"Abstract Advancements in neural networks have led to developments fields like computer vision, speech recognition and natural language processing (NLP). One of the most influential recent NLP is use word embeddings, where words are represented as vectors a continuous space, capturing many syntactic semantic relations among them. AraVec pre-trained distributed representation (word embedding) open source project which aims provide Arabic research community with free powerful embedding models. The first version provides six different models built on top three content domains; Tweets, World Wide Web pages Wikipedia articles. total number tokens used build amounts more than 3,300,000,000. This paper describes resources for building models, employed data cleaning techniques, carried out preprocessing step, well details creation techniques."
https://openalex.org/W2962904552,https://doi.org/10.18653/v1/p18-1144,Chinese NER Using Lattice LSTM,2018,"We investigate a lattice-structured LSTM model for Chinese NER, which encodes sequence of input characters as well all potential words that match lexicon. Compared with character-based methods, our explicitly leverages word and information. word-based lattice does not suffer from segmentation errors. Gated recurrent cells allow to choose the most relevant sentence better NER results. Experiments on various datasets show outperforms both baselines, achieving best"
https://openalex.org/W2964089981,https://doi.org/10.1109/iccv.2017.563,TALL: Temporal Activity Localization via Language Query,2017,"This paper focuses on temporal localization of actions in untrimmed videos. Existing methods typically train classifiers for a pre-defined list and apply them sliding window fashion. However, activities the wild consist wide combination actors, objects; it is difficult to design proper activity that meets users’ needs. We propose localize by natural language queries. Temporal Activity Localization via Language (TALL) challenging as requires: (1) suitable text video representations allow cross-modal matching queries; (2) ability locate accurately given features from windows limited granularity. novel Cross-modal Regression Localizer (CTRL) jointly model query clips, output alignment scores action boundary regression results candidate clips. Lor evaluation, we adopt TaCoS dataset, build new dataset this task top Charades adding sentence annotations, called Charades-STA. also complex queries Charades-STA test. Experimental show CTRL outperforms previous significantly both datasets."
https://openalex.org/W2760656271,https://doi.org/10.18653/v1/w17-4717,Findings of the 2017 Conference on Machine Translation (WMT17),2017,"This paper presents the results of WMT17 shared tasks, which included
three machine translation (MT) tasks (news, biomedical, and multimodal), two evaluation (metrics run-time estimation MT quality), an automatic post-editing task, a neural training bandit learning task."
https://openalex.org/W2251765408,https://doi.org/10.3115/v1/w15-1521,Bilingual Word Representations with Monolingual Quality in Mind,2015,"Recent work in learning bilingual representations tend to tailor towards achieving good performance on tasks, most often the crosslingual document classification (CLDC) evaluation, but detriment of preserving clustering structures word monolingually. In this work, we propose a joint model learn from scratch that utilizes both context coocurrence information through monolingual component and meaning equivalent signals constraint. Specifically, extend recently popular skipgram high quality efficiently. Our learned embeddings achieve new state-of-the-art accuracy 80.3 for German English CLDC task highly competitive 90.7 other direction. At same time, our models outperform best past representation by large margin similarity evaluation. 1"
https://openalex.org/W2963242190,https://doi.org/10.1109/icsda.2017.8384449,AISHELL-1: An open-source Mandarin speech corpus and a speech recognition baseline,2017,"An open-source Mandarin speech corpus called AISHELL-1 is released. It by far the largest which suitable for conducting recognition research and building systems Mandarin. The recording procedure, including audio capturing devices environments are presented in details. preparation of related resources, transcriptions lexicon described. released with a Kaldi recipe. Experimental results implies that quality recordings promising."
https://openalex.org/W2062913298,https://doi.org/10.1016/j.ipm.2015.01.005,Contextual semantics for sentiment analysis of Twitter,2016,"We propose a semantic sentiment representation of words called SentiCircle.SentiCircle captures the contextual from their co-occurrences.SentiCircle updates based on semantics.SentiCircle can be used to perform entity- and tweet-level level analysis. Sentiment analysis Twitter has attracted much attention recently due its wide applications in both, commercial public sectors. In this paper we present SentiCircles, lexicon-based approach for Twitter. Different typical approaches, which offer fixed static prior polarities regardless context, SentiCircles takes into account co-occurrence patterns different contexts tweets capture semantics update pre-assigned strength polarity lexicons accordingly. Our allows detection at both entity-level tweet-level. evaluate our proposed three datasets using derive word sentiments. Results show that significantly outperforms baselines accuracy F-measure subjectivity (neutral vs. polar) (positive negative) detections. For detection, performs better than state-of-the-art SentiStrength by 4-5% two datasets, but falls marginally behind 1% third dataset."
https://openalex.org/W2251957808,https://doi.org/10.3115/v1/p15-1129,Building a Semantic Parser Overnight,2015,"How do we build a semantic parser in new domain starting with zero training examples? We introduce methodology for this setting: First, use simple grammar to generate logical forms paired canonical utterances. The are meant cover the desired set of compositional operators, and utterances capture meaning (although clumsily). then crowdsourcing paraphrase these into natural resulting data is used train parser. further study role compositionality paraphrases. Finally, test our on seven domains show that can an adequate just few hours."
https://openalex.org/W2520861906,https://doi.org/10.1109/tpami.2016.2608901,Semantic Pooling for Complex Event Analysis in Untrimmed Videos,2017,"Pooling plays an important role in generating a discriminative video representation. In this paper, we propose new semantic pooling approach for challenging event analysis tasks (e.g., detection, recognition, and recounting) long untrimmed Internet videos, especially when only few shots/segments are relevant to the of interest while many other shots irrelevant or even misleading. The commonly adopted strategies aggregate indifferently one way another, resulting great loss information. Instead, work first define novel notion saliency that assesses relevance each shot with interest. We then prioritize according their scores since semantically more salient expected contribute final analysis. Next, isotonic regularizer is able exploit constructed ordering nearly-isotonic support vector machine classifier exhibits higher power tasks. Computationally, develop efficient implementation using proximal gradient algorithm, prove closed-form steps. conduct extensive experiments on three real-world datasets achieve promising improvements."
https://openalex.org/W2997591391,https://doi.org/10.1609/aaai.v34i07.7005,Unified Vision-Language Pre-Training for Image Captioning and VQA,2020,"This paper presents a unified Vision-Language Pre-training (VLP) model. The model is in that (1) it can be fine-tuned for either vision-language generation (e.g., image captioning) or understanding visual question answering) tasks, and (2) uses shared multi-layer transformer network both encoding decoding, which differs from many existing methods where the encoder decoder are implemented using separate models. VLP pre-trained on large amount of image-text pairs unsupervised learning objectives two tasks: bidirectional sequence-to-sequence (seq2seq) masked prediction. tasks differ solely what context prediction conditions on. controlled by utilizing specific self-attention masks network. To best our knowledge, first reported achieves state-of-the-art results as disparate captioning answering, across three challenging benchmark datasets: COCO Captions, Flickr30k VQA 2.0. code models available at https://github.com/LuoweiZhou/VLP."
https://openalex.org/W2963672599,https://doi.org/10.18653/v1/d17-1238,Why We Need New Evaluation Metrics for NLG,2017,"The majority of NLG evaluation relies on automatic metrics, such as BLEU . In this paper, we motivate the need for novel, system- and data-independent methods: We investigate a wide range including state-of-the-art word-based novel grammar-based ones, demonstrate that they only weakly reflect human judgements system outputs generated by data-driven, end-to-end NLG. also show metric performance is data- system-specific. Nevertheless, our results suggest metrics perform reliably at system-level can support development finding cases where performs poorly."
https://openalex.org/W2963091658,https://doi.org/10.18653/v1/d16-1128,Neural Text Generation from Structured Data with Application to the Biography Domain,2016,"This paper introduces a neural model for concept-to-text generation that scales to large, rich domains. We experiment with new dataset of biographies from Wikipedia is an order magnitude larger than existing resources over 700k samples. The also vastly more diverse 400k vocabulary, compared few hundred words Weathergov or Robocup. Our builds upon recent work on conditional language text generation. To deal the large we extend these models mix fixed vocabulary copy actions transfer sample-specific input database generated output sentence. significantly out-performs classical Kneser-Ney adapted this task by nearly 15 BLEU."
https://openalex.org/W2770233088,https://doi.org/10.1007/s13735-017-0141-z,A review of semantic segmentation using deep neural networks,2018,"During the long history of computer vision, one grand challenges has been semantic segmentation which is ability to segment an unknown image into different parts and objects (e.g., beach, ocean, sun, dog, swimmer). Furthermore, even deeper than object recognition because not necessary for segmentation. Specifically, humans can perform without knowing what are (for example, in satellite imagery or medical X-ray scans, there may be several unknown, but they still segmented within typically further investigation). Performing exact identity all scene important part our visual understanding process give us a powerful model understand world also used improve augment existing vision techniques. Herein this work, we review field as pertaining deep convolutional neural networks. We provide comprehensive coverage top approaches summarize strengths, weaknesses major challenges."
https://openalex.org/W2798665661,https://doi.org/10.18653/v1/p18-2103,Breaking NLI Systems with Sentences that Require Simple Lexical Inferences,2018,"We create a new NLI test set that shows the deficiency of state-of-the-art models in inferences require lexical and world knowledge. The examples are simpler than SNLI set, containing sentences differ by at most one word from training set. Yet, performance on is substantially worse across systems trained SNLI, demonstrating these limited their generalization ability, failing to capture many simple inferences."
https://openalex.org/W2903193068,https://doi.org/10.18653/v1/w18-6401,Findings of the 2018 Conference on Machine Translation (WMT18),2018,"This paper presents the results of premier
shared task organized alongside Conference on Machine Translation (WMT) 2018.
Participants were asked to build machine
translation systems for any 7 language pairs
in both directions, be evaluated a test set
of news stories. The main metric this task
is human judgment translation quality. This
year, we also opened up additional
test suites probe specific aspects translation."
https://openalex.org/W2963545917,https://doi.org/10.18653/v1/n18-2072,Contextual Augmentation: Data Augmentation by Words with Paradigmatic Relations,2018,"We propose a novel data augmentation for labeled sentences called contextual augmentation. assume an invariance that are natural even if the words in replaced with other paradigmatic relations. stochastically replace predicted by bi-directional language model at word positions. Words according to context numerous but appropriate of original words. Furthermore, we retrofit label-conditional architecture, which allows augment without breaking label-compatibility. Through experiments six various different text classification tasks, demonstrate proposed method improves classifiers based on convolutional or recurrent neural networks."
https://openalex.org/W3034469191,https://doi.org/10.18653/v1/2020.acl-main.421,On the Cross-lingual Transferability of Monolingual Representations,2020,"State-of-the-art unsupervised multilingual models (e.g., BERT) have been shown to generalize in a zero-shot cross-lingual setting. This generalization ability has attributed the use of shared subword vocabulary and joint training across multiple languages giving rise deep abstractions. We evaluate this hypothesis by designing an alternative approach that transfers monolingual model new at lexical level. More concretely, we first train transformer-based masked language on one language, transfer it learning embedding matrix with same modeling objective, freezing parameters all other layers. does not rely or training. However, show is competitive BERT standard classification benchmarks Cross-lingual Question Answering Dataset (XQuAD). Our results contradict common beliefs basis suggest learn some abstractions languages. also release XQuAD as more comprehensive benchmark, which comprises 240 paragraphs 1190 question-answer pairs from SQuAD v1.1 translated into ten professional translators."
https://openalex.org/W2884822772,https://doi.org/10.1007/978-3-030-01228-1_26,Unified Perceptual Parsing for Scene Understanding,2018,"Humans recognize the visual world at multiple levels: we effortlessly categorize scenes and detect objects inside, while also identifying textures surfaces of along with their different compositional parts. In this paper, study a new task called Unified Perceptual Parsing, which requires machine vision systems to as many concepts possible from given image. A multi-task framework UPerNet training strategy are developed learn heterogeneous image annotations. We benchmark our on Parsing show that it is able effectively segment wide range images. The trained networks further applied discover knowledge in natural scenes. Models available \url{https://github.com/CSAILVision/unifiedparsing}."
https://openalex.org/W2984008963,https://doi.org/10.1109/iccv.2019.00272,HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips,2019,"Learning text-video embeddings usually requires a dataset of video clips with manually provided captions. However, such datasets are expensive and time consuming to create therefore difficult obtain on large scale. In this work, we propose instead learn from data readily available natural language annotations in the form automatically transcribed narrations. The contributions work three-fold. First, introduce HowTo100M: large-scale 136 million sourced 1.22M narrated instructional web videos depicting humans performing describing over 23k different visual tasks. Our collection procedure is fast, scalable does not require any additional manual annotation. Second, demonstrate that embedding trained leads state-of-the-art results for text-to-video retrieval action localization as YouCook2 or CrossTask. Finally, show transfers well other domains: fine-tuning generic Youtube (MSR-VTT dataset) movies (LSMDC outperforms models these alone. dataset, code publicly available."
https://openalex.org/W2970862333,https://doi.org/10.18653/v1/d19-1275,Designing and Interpreting Probes with Control Tasks,2019,"Probes, supervised models trained to predict properties (like parts-of-speech) from representations ELMo), have achieved high accuracy on a range of linguistic tasks. But does this mean that the encode structure or just probe has learned task? In paper, we propose control tasks, which associate word types with random outputs, complement By construction, these tasks can only be by itself. So good probe, (one reflects representation), should selective, achieving task and low accuracy. The selectivity puts in context probe’s capacity memorize types. We construct for English part-of-speech tagging dependency edge prediction, show popular probes ELMo are not selective. also find dropout, commonly used complexity, is ineffective improving MLPs, but other forms regularization effective. Finally, while first layer yield slightly better than second, second substantially more raises question represents parts-of-speech."
https://openalex.org/W2971258845,https://doi.org/10.18653/v1/w19-5006,Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT and ELMo on Ten Benchmarking Datasets,2019,"Inspired by the success of General Language Understanding Evaluation benchmark, we introduce Biomedical (BLUE) benchmark to facilitate research in development pre-training language representations biomedicine domain. The consists five tasks with ten datasets that cover both biomedical and clinical texts different dataset sizes difficulties. We also evaluate several baselines based on BERT ELMo find model pre-trained PubMed abstracts MIMIC-III notes achieves best results. make datasets, models, codes publicly available at https://github.com/ncbi-nlp/BLUE_Benchmark."
https://openalex.org/W2604735854,https://doi.org/10.1109/iccv.2017.87,Deep Direct Regression for Multi-oriented Scene Text Detection,2017,"In this paper, we first provide a new perspective to divide existing high performance object detection methods into direct and indirect regressions. Direct regression performs boundary by predicting the offsets from given point, while predicts some bounding box proposals. context of multioriented scene text detection, analyze drawbacks regression, which covers state-of-the-art structures Faster-RCNN SSD as instances, point out potential superiority regression. To verify view, propose deep based method for multi-oriented detection. Our framework is simple effective with fully convolutional network one-step post processing. The optimized in an end-to-end way has bi-task outputs where one pixel-wise classification between non-text, other determine vertex coordinates quadrilateral boundaries. proposed particularly beneficial localize incidental texts. On ICDAR2015 Incidental Scene Text benchmark, our achieves F-measure 81%, state-ofthe-art significantly outperforms previous approaches. standard datasets focused texts, also reaches performance."
https://openalex.org/W2114925438,https://doi.org/10.1109/icassp.2016.7472652,End-to-end text-dependent speaker verification,2016,"In this paper we present a data-driven, integrated approach to speaker verification, which maps test utterance and few reference utterances directly single score for verification jointly optimizes the system's components using same evaluation protocol metric as at time. Such an will result in simple efficient systems, requiring little domain-specific knowledge making model assumptions. We implement idea by formulating problem neural network architecture, including estimation of on only utterances, evaluate it our internal Ok Google benchmark text-dependent verification. The proposed appears be very effective big data applications Like ours that require highly accurate, easy-to-maintain systems with small footprint."
https://openalex.org/W2788496822,https://doi.org/10.1609/aaai.v32i1.12022,SciTaiL: A Textual Entailment Dataset from Science Question Answering,2018,"We present a new dataset and model for textual entailment, derived from treating multiple-choice question-answering as an entailment problem. SciTail is the first set that created solely natural sentences already exist independently ``in wild'' rather than authored specifically task. Different existing datasets, we create hypotheses science questions corresponding answer candidates, premises relevant web retrieved large corpus. These are often linguistically challenging. This, combined with high lexical similarity of premise hypothesis both entailed non-entailed pairs, makes this task particularly difficult. The resulting challenge evidenced by state-of-the-art systems achieving mediocre performance on SciTail, especially in comparison to simple majority class baseline. As step forward, demonstrate one can improve accuracy 5% using neural exploits linguistic structure."
https://openalex.org/W2508741746,https://doi.org/10.1007/978-3-319-46487-9_32,Laplacian Pyramid Reconstruction and Refinement for Semantic Segmentation,2016,"CNN architectures have terrific recognition performance but rely on spatial pooling which makes it difficult to adapt them tasks that require dense, pixel-accurate labeling. This paper two contributions: (1) We demonstrate while the apparent resolution of convolutional feature maps is low, high-dimensional representation contains significant sub-pixel localization information. (2) describe a multi-resolution reconstruction architecture based Laplacian pyramid uses skip connections from higher and multiplicative gating successively refine segment boundaries reconstructed lower-resolution maps. approach yields state-of-the-art semantic segmentation results PASCAL VOC Cityscapes benchmarks without resorting more complex random-field inference or instance detection driven architectures."
https://openalex.org/W2810840719,https://doi.org/10.1145/3209978.3210183,Neural Approaches to Conversational AI,2018,"The present paper surveys neural approaches to conversational AI that have been developed in the last few years. We group systems into three categories: (1) question answering agents, (2) task-oriented dialogue and (3) chatbots. For each category, we a review of state-of-the-art approaches, draw connection between them traditional discuss progress has made challenges still being faced, using specific models as case studies."
https://openalex.org/W2741252866,https://doi.org/10.18653/v1/p17-1036,An Unsupervised Neural Attention Model for Aspect Extraction,2017,"Methods, systems, and computer-readable storage media for receiving a vocabulary, the vocabulary including text data that is provided as at least portion of raw data, being in file, associating each word with feature vector, providing sentence embedding based on plurality vectors to provide embeddings, reconstructed weighted parameter matrix training unsupervised neural attention model embeddings trained model, used automatically determine aspects from vocabulary."
https://openalex.org/W3004346089,https://doi.org/10.1162/tacl_a_00298,What BERT Is Not: Lessons from a New Suite of Psycholinguistic Diagnostics for Language Models,2020,"Pre-training by language modeling has become a popular and successful approach to NLP tasks, but we have yet understand exactly what linguistic capacities these pre-training processes confer upon models. In this paper introduce suite of diagnostics drawn from human experiments, which allow us ask targeted questions about information used models for generating predictions in context. As case study, apply the BERT model, finding that it can generally distinguish good bad completions involving shared category or role reversal, albeit with less sensitivity than humans, robustly retrieves noun hypernyms, struggles challenging inference role-based event prediction— and, particular, shows clear insensitivity contextual impacts negation."
https://openalex.org/W3097777922,https://doi.org/10.21437/interspeech.2020-3015,Conformer: Convolution-augmented Transformer for Speech Recognition,2020,"Recently Transformer and Convolution neural network (CNN) based models have shown promising results in Automatic Speech Recognition (ASR), outperforming Recurrent networks (RNNs). are good at capturing content-based global interactions, while CNNs exploit local features effectively. In this work, we achieve the best of both worlds by studying how to combine convolution transformers model dependencies an audio sequence a parameter-efficient way. To regard, propose convolution-augmented transformer for speech recognition, named Conformer. Conformer significantly outperforms previous CNN achieving state-of-the-art accuracies. On widely used LibriSpeech benchmark, our achieves WER 2.1%/4.3% without using language 1.9%/3.9% with external on test/testother. We also observe competitive performance 2.7%/6.3% small only 10M parameters."
https://openalex.org/W2028742638,https://doi.org/10.1145/2806416.2806475,Short Text Similarity with Word Embeddings,2015,"Determining semantic similarity between texts is important in many tasks information retrieval such as search, query suggestion, automatic summarization and image finding. Many approaches have been suggested, based on lexical matching, handcrafted patterns, syntactic parse trees, external sources of structured knowledge distributional semantics. However, features, like string do not capture beyond a trivial level. Furthermore, patterns cannot be assumed to available all circumstances for domains. Lastly, depending trees are restricted syntactically well-formed texts, typically one sentence length. We investigate whether determining short text possible using only features---where by we mean, pertaining representation meaning---rather than relying or representations. use word embeddings, vector representations terms, computed from unlabelled data, that represent terms space which proximity vectors can interpreted similarity. propose go word-level text-level semantics combining insights methods with embeddings. A novel feature our approach an arbitrary number embedding sets incorporated. derive multiple types meta-features the comparison pairs, means their respective The features representing labelled pairs used train supervised learning algorithm. trained model at testing time predict new, show publicly evaluation set commonly task method outperforms baseline work under same conditions."
https://openalex.org/W2536769020,https://doi.org/10.7717/peerj-cs.93,Predicting judicial decisions of the European Court of Human Rights: a Natural Language Processing perspective,2016,"Recent advances in Natural Language Processing and Machine Learning provide us with the tools to build predictive models that can be used unveil patterns driving judicial decisions. This useful, for both lawyers judges, as an assisting tool rapidly identify cases extract which lead certain paper presents first systematic study on predicting outcome of tried by European Court Human Rights based solely textual content. We formulate a binary classification task where input our classifiers is content extracted from case target output actual judgment whether there has been violation article convention human rights. Textual information represented using contiguous word sequences, i.e., N-grams, topics. Our predict court’s decisions strong accuracy (79% average). empirical analysis indicates formal facts are most important factor. consistent theory legal realism suggesting decision-making significantly affected stimulus facts. also observe topical another feature this explore relationship further conducting qualitative analysis."
https://openalex.org/W2888329843,https://doi.org/10.18653/v1/d18-1179,Dissecting Contextual Word Embeddings: Architecture and Representation,2018,"Contextual word representations derived from pre-trained bidirectional language models (biLMs) have recently been shown to provide significant improvements the state of art for a wide range NLP tasks. However, many questions remain as how and why these are so effective. In this paper, we present detailed empirical study choice neural architecture (e.g. LSTM, CNN, or self attention) influences both end task accuracy qualitative properties that learned. We show there is tradeoff between speed accuracy, but all architectures learn high quality contextual outperform embeddings four challenging Additionally, vary with network depth, exclusively morphological based at embedding layer through local syntax in lower layers longer semantics such coreference upper layers. Together, results suggest unsupervised biLMs, independent architecture, learning much more about structure than previously appreciated."
https://openalex.org/W2962779575,https://doi.org/10.1109/cvpr.2017.344,Graph-Structured Representations for Visual Question Answering,2017,"This paper proposes to improve visual question answering (VQA) with structured representations of both scene contents and questions. A key challenge in VQA is require joint reasoning over the text domains. The predominant CNN/LSTM-based approach limited by monolithic vector that largely ignore structure question. CNN feature vectors cannot effectively capture situations as simple multiple object instances, LSTMs process questions series words, which do not reflect true complexity language structure. We instead propose build graphs objects we describe a deep neural network exploits these representations. show this achieves significant improvements state-of-the-art, increasing accuracy from 71.2% 74.4% on abstract scenes multiple-choice benchmark, 34.7% 39.1% pairs balanced scenes, i.e. images fine-grained differences opposite yes/no answers same"
https://openalex.org/W3171007011,https://doi.org/10.1109/cvpr46437.2021.01549,Exploring Simple Siamese Representation Learning,2021,"Siamese networks have become a common structure in various recent models for unsupervised visual representation learning. These maximize the similarity between two augmentations of one image, subject to certain conditions avoiding collapsing solutions. In this paper, we report surprising empirical results that simple can learn meaningful representations even using none following: (i) negative sample pairs, (ii) large batches, (iii) momentum encoders. Our experiments show solutions do exist loss and structure, but stop-gradient operation plays an essential role preventing collapsing. We provide hypothesis on implication stop-gradient, further proof-of-concept verifying it. ""SimSiam"" method achieves competitive ImageNet downstream tasks. hope baseline will motivate people rethink roles architectures Code is made available. <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>"
https://openalex.org/W2071478164,https://doi.org/10.1016/j.jbi.2014.11.002,Portable automatic text classification for adverse drug reaction detection via multi-corpus training,2015,"Automatic detection of adverse drug reaction (ADR) mentions from text has recently received significant interest in pharmacovigilance research. Current research focuses on various sources text-based information, including social media-where enormous amounts user posted data is available, which have the potential for use if collected and filtered accurately. The aims this study are: (i) to explore natural language processing (NLP) approaches generating useful features text, utilizing them optimized machine learning algorithms automatic classification ADR assertive segments; (ii) present two sets that we prepared task internet data; (iii) investigate combining training distinct corpora can improve accuracies.One our three contains annotated sentences clinical reports, other sets, built in-house, consist posts media. Our approach relies a large set features, representing semantic properties (e.g., sentiment, polarity, topic), short nuggets. Importantly, using expanded feature combine different attempts boost accuracies.Our feature-rich performs significantly better than previously published with class F-scores 0.812 (previously reported best: 0.770), 0.538 0.678 sets. Combining multiple compatible further improves in-house 0.597 (improvement 5.9 units) 0.704 2.6 respectively.Our results indicate advanced NLP techniques information rich accuracies over existing benchmarks. experiments illustrate benefits incorporating such as topics, concepts, sentiments, polarities. Finally, show integration performance. This form multi-corpus may be particularly cases where are heavily imbalanced media data), reduce time costs associated annotation future."
https://openalex.org/W2284851926,https://doi.org/10.1145/2939672.2939823,Multi-layer Representation Learning for Medical Concepts,2016,"Learning efficient representations for concepts has been proven to be an important basis many applications such as machine translation or document classification. Proper of medical diagnosis, medication, procedure codes and visits will have broad in healthcare analytics. However, Electronic Health Records (EHR) the visit sequences patients include multiple (diagnosis, procedure, medication codes) per visit. This structure provides two types relational information, namely sequential order co-occurrence within each In this work, we propose Med2Vec, which not only learns distributed both from a large EHR dataset with over 3 million visits, but also allows us interpret learned confirmed positively by clinical experts. experiments, Med2Vec displays significant improvement key compared popular baselines Skip-gram, GloVe stacked autoencoder, while providing clinically meaningful interpretation."
https://openalex.org/W2888922637,https://doi.org/10.18653/v1/d18-1151,Targeted Syntactic Evaluation of Language Models,2018,"We present a data set for evaluating the grammaticality of predictions language model. automatically construct large number minimally different pairs English sentences, each consisting grammatical and an ungrammatical sentence. The sentence represent variations structure-sensitive phenomena: subject-verb agreement, reflexive anaphora negative polarity items. expect model to assign higher probability than one. In experiment using this set, LSTM performed poorly on many constructions. Multi-task training with syntactic objective (CCG supertagging) improved LSTM’s accuracy, but gap remained between its performance accuracy human participants recruited online. This suggests that there is considerable room improvement over LSTMs in capturing syntax"
https://openalex.org/W2159544539,https://doi.org/10.1093/pan/mpu019,Computer-Assisted Text Analysis for Comparative Politics,2015,"Recent advances in research tools for the systematic analysis of textual data are enabling exciting new throughout social sciences. For comparative politics, scholars who often interested non-English and possibly multilingual datasets, these may be difficult to access. This article discusses practical issues that arise processing, management, translation, with a particular focus on how procedures differ across languages. These combined two applied examples automated text using recently introduced Structural Topic Model. We also show model can used analyze have been translated into single language via machine translation tools. All methods we describe here implemented open-source software packages available from authors."
https://openalex.org/W2774005037,https://doi.org/10.1109/cvprw.2018.00279,Embodied Question Answering,2018,"We present a new AI task - Embodied Question Answering (EmbodiedQA) where an agent is spawned at random location in 3D environment and asked question ('What color the car?'). In order to answer, must first intelligently navigate explore environment, gather necessary visual information through first-person (egocentric) vision, then answer ('orange'). EmbodiedQA requires range of skills language understanding, recognition, active perception, goal-driven navigation, commonsense reasoning, long-term memory, grounding into actions. this work, we develop dataset questions answers House3D environments [1], evaluation metrics, hierarchical model trained with imitation reinforcement learning."
https://openalex.org/W277886906,https://doi.org/10.1145/3191513,Never-ending learning,2018,"Whereas people learn many different types of knowledge from diverse experiences over years, and become better learners time, most current machine learning systems are much more narrow, just a single function or data model based on statistical analysis set. We suggest that than computers precisely because this difference, we key direction for research is to develop software architectures enable intelligent agents also knowledge, continuously time. In paper define never-ending paradigm learning, present one case study: the Never-Ending Language Learner (NELL), which achieves number desired properties learner. NELL has been read Web 24hrs/day since January 2010, so far acquired base with 120mn diverse, confidence-weighted beliefs (e.g., servedWith(tea,biscuits) ), while thousands interrelated functions continually improve its reading competence learned reason infer new it not yet those has, inventing relational predicates extend ontology uses represent beliefs. describe design NELL, experimental results illustrating behavior, discuss both successes shortcomings as study in learning. can be tracked online at http://rtw.ml.cmu.edu, followed Twitter @CMUNELL."
https://openalex.org/W2916979304,https://doi.org/10.1109/icassp.2018.8461870,The Microsoft 2017 Conversational Speech Recognition System,2018,"We describe the 2017 version of Microsoft's conversational speech recognition system, in which we update our 2016 system with recent developments neural-network-based acoustic and language modeling to further advance state art on Switchboard task. The adds a CNN-BLSTM model set architectures combined previously, includes character-based dialog session aware LSTM models rescoring. For combination adopt two-stage approach, whereby subsets are first at senone/frame level, followed by word-level voting via confusion networks. also added network rescoring step after combination. resulting yields 5.1\% word error rate 2000 evaluation set."
https://openalex.org/W2963241825,https://doi.org/10.18653/v1/p16-2022,Natural Language Inference by Tree-Based Convolution and Heuristic Matching,2016,"In this paper, we propose the TBCNN-pair model to recognize entailment and contradiction between two sentences. our model, a tree-based convolutional neural network (TBCNN) captures sentence-level semantics; then heuristic matching layers like concatenation, element-wise product/difference combine information in individual Experimental results show that outperforms existing sentence encoding-based approaches by large margin."
https://openalex.org/W2963718112,https://doi.org/10.18653/v1/d18-1360,"Multi-Task Identification of Entities, Relations, and Coreference for Scientific Knowledge Graph Construction",2018,"We introduce a multi-task setup of identifying entities, relations, and coreference clusters in scientific articles. create SciERC, dataset that includes annotations for all three tasks develop unified framework called SciIE with shared span representations. The reduces cascading errors between leverages cross-sentence relations through links. Experiments show our model outperforms previous models information extraction without using any domain-specific features. further the supports construction knowledge graph, which we use to analyze literature."
https://openalex.org/W3034655362,https://doi.org/10.1109/cvpr42600.2020.01059,Meshed-Memory Transformer for Image Captioning,2020,"Transformer-based architectures represent the state of art in sequence modeling tasks like machine translation and language understanding. Their applicability to multi-modal contexts image captioning, however, is still largely under-explored. With aim filling this gap, we present M² - a Meshed Transformer with Memory for Image Captioning. The architecture improves both encoding generation steps: it learns multi-level representation relationships between regions integrating learned priori knowledge, uses mesh-like connectivity at decoding stage exploit low- high-level features. Experimentally, investigate performance different fully-attentive models comparison recurrent ones. When tested on COCO, our proposal achieves new single-model ensemble configurations Karpathy test split online server. We also assess its performances when describing objects unseen training set. Trained code reproducing experiments are publicly available at: https://github.com/aimagelab/meshed-memory-transformer."
https://openalex.org/W3098341425,https://doi.org/10.18653/v1/p17-2090,Data Augmentation for Low-Resource Neural Machine Translation,2017,"The quality of a Neural Machine Translation system depends substantially on the availability sizable parallel corpora. For low-resource language pairs this is not case, resulting in poor translation quality. Inspired by work computer vision, we propose novel data augmentation approach that targets low-frequency words generating new sentence containing rare new, synthetically created contexts. Experimental results simulated settings show our method improves up to 2.9 BLEU points over baseline and 3.2 back-translation."
https://openalex.org/W2346604114,https://doi.org/10.1016/j.inffus.2016.04.005,Personalized individual semantics in computing with words for supporting linguistic group decision making. An application on consensus reaching,2017,"To propose a personalized individual semantics model (PIS).To 2-tuple linguistic comparison and aggregation.To discuss the application of PIS to support consensus reaching. In group decision making (GDM) dealing with Computing Words (CW) has been highlighted importance statement, words mean different things for people, because its influence in final decision. Different proposals that either grouping such meanings (uncertainty) provide one representation all people or use multi-granular term sets each granularity, have developed applied specialized literature. Despite these models are quite useful they do not individually yet person when he/she elicits information. Hence, this paper (PIS) is proposed personalize by means an interval numerical scale model. Specifically, consistency-driven optimization-based obtain represent introduced. A new CW framework based on then defined, allows us deal facilitate keeping idea people. order justify feasibility validity model, it solve GDM problems reaching process."
https://openalex.org/W2534253848,https://doi.org/10.1162/tacl_a_00107,Optimizing Statistical Machine Translation for Text Simplification,2016,"Most recent sentence simplification systems use basic machine translation models to learn lexical and syntactic paraphrases from a manually simplified parallel corpus. These methods are limited by the quality quantity of corpora, which expensive build. In this paper, we conduct an in-depth adaptation statistical perform text simplification, taking advantage large-scale learned bilingual texts small amount manual simplifications with multiple references. Our work is first design automatic metrics that effective for tuning evaluating systems, will facilitate iterative development task."
https://openalex.org/W2804897457,https://doi.org/10.18653/v1/n18-1023,Looking Beyond the Surface: A Challenge Set for Reading Comprehension over Multiple Sentences,2018,"We present a reading comprehension challenge in which questions can only be answered by taking into account information from multiple sentences. solicit and verify answers for this through 4-step crowdsourcing experiment. Our dataset contains 6,500+ 1000+ paragraphs across 7 different domains (elementary school science, news, travel guides, fiction stories, etc) bringing linguistic diversity to the texts wordings. On subset of our dataset, we found human solvers achieve an F1-score 88.1%. analyze range baselines, including recent state-of-art system, demonstrate difficulty challenge, despite high performance. The is first study multi-sentence inference at scale, with open-ended set question types that requires reasoning skills."
https://openalex.org/W2970120757,https://doi.org/10.18653/v1/d19-1445,Revealing the Dark Secrets of BERT,2019,"BERT-based architectures currently give state-of-the-art performance on many NLP tasks, but little is known about the exact mechanisms that contribute to its success. In current work, we focus interpretation of self-attention, which one fundamental underlying components BERT. Using a subset GLUE tasks and set handcrafted features-of-interest, propose methodology carry out qualitative quantitative analysis information encoded by individual BERT’s heads. Our findings suggest there limited attention patterns are repeated across different heads, indicating overall model overparametrization. While heads consistently use same patterns, they have varying impact tasks. We show manually disabling in certain leads improvement over regular fine-tuned BERT models."
https://openalex.org/W2143933463,https://doi.org/10.1016/j.ipm.2014.10.006,Analysis of named entity recognition and linking for tweets,2015,"Applying natural language processing for mining and intelligent information access to tweets (a form of microblog) is a challenging, emerging research area. Unlike carefully authored news text other longer content, pose number new challenges, due their short, noisy, context-dependent, dynamic nature. Information extraction from typically performed in pipeline, comprising consecutive stages identification, tokenisation, part-of-speech tagging, named entity recognition disambiguation (e.g. with respect DBpedia). In this work, we describe Twitter dataset, conduct an empirical analysis disambiguation, investigating how robust state-of-the-art systems are on such noisy texts, what the main sources error are, which problems should be further investigated improve state art."
https://openalex.org/W3034850762,https://doi.org/10.18653/v1/2020.acl-main.441,Adversarial NLI: A New Benchmark for Natural Language Understanding,2020,"We introduce a new large-scale NLI benchmark dataset, collected via an iterative, adversarial human-and-model-in-the-loop procedure. show that training models on this dataset leads to state-of-the-art performance variety of popular benchmarks, while posing more difficult challenge with its test set. Our analysis sheds light the shortcomings current models, and shows non-expert annotators are successful at finding their weaknesses. The data collection method can be applied in never-ending learning scenario, becoming moving target for NLU, rather than static will quickly saturate."
https://openalex.org/W2347127863,https://doi.org/10.1145/3003433,Stance and Sentiment in Tweets,2017,"We can often detect from a person’s utterances whether he or she is in favor of against given target entity—one’s stance toward the target. However, person may express same by using negative positive language. Here for first time we present dataset tweet–target pairs annotated both and sentiment. The targets not be referred to tweets, they opinion tweets. Partitions this were used as training test sets SemEval-2016 shared task competition. propose simple detection system that outperforms submissions all 19 teams participated task. Additionally, access sentiment annotations allows us explore several research questions. show although knowing expressed tweet beneficial classification, it alone sufficient. Finally, use additional unlabeled data through distant supervision techniques word embeddings further improve classification."
https://openalex.org/W2735580341,https://doi.org/10.1016/j.jbi.2017.07.012,Natural language processing systems for capturing and standardizing unstructured clinical information: A systematic review,2017,"• A literature review for clinical natural language processing systems. Over 7000 publications were reviewed in a multi-stage process. final list of 71 systems was identified. Each system briefly summarized based on information. We followed systematic approach the Preferred Reporting Items Systematic Reviews and Meta-Analyses to identify existing (NLP) that generate structured information from unstructured free text. Seven databases searched with query combining concepts data capture. Two reviewers screened all records relevance during two screening phases, about NLP collected set papers. total 7149 (after removing duplicates) retrieved screened, 86 determined fit criteria. These papers contained different systems, which then analyzed. The address wide variety important research tasks. Certain tasks are well addressed by while others remain as open challenges only small number attempt, such extraction temporal or normalization standard terminologies. This has identified many capable text generating output, evaluated here will be prioritizing development new approaches NLP."
https://openalex.org/W1906221854,https://doi.org/10.1016/j.neuron.2015.09.019,The Neural Representation of Sequences: From Transition Probabilities to Algebraic Patterns and Linguistic Trees,2015,"A sequence of images, sounds, or words can be stored at several levels detail, from specific items and their timing to abstract structure. We propose a taxonomy five distinct cerebral mechanisms for coding: transitions knowledge, chunking, ordinal algebraic patterns, nested tree structures. In each case, we review the available experimental paradigms list behavioral neural signatures systems involved. Tree structures require recursive code, as yet unidentified by electrophysiology, possibly unique humans, which may explain singularity human language cognition."
https://openalex.org/W2013659308,https://doi.org/10.1016/j.asoc.2015.02.023,Trust based consensus model for social network in an incomplete linguistic information context,2015,"Graphical abstract(A) Trust propagating aggregation and visual consensus model for MCGDM under incomplete information. (B) Visual feedback simulation: levels before after recommendations implemented by experts. Display Omitted HighlightsA theoretical framework to build within a networked social group is presented.A novel trust propagation method proposed derive relationship from an connected network.A process including recommendation mechanism provide individualised advice implemented.The implementation of the guarantees convergence reaching process. A building put forward. This article investigates based estimation methods as part multiple criteria decision making with linguistic network score induced order weighted averaging operator presented aggregate orthopairs trust/distrust values obtained different paths. Then, concept relative defined, whose use twofold: (1) estimate unknown preference (2) reliable source determine experts' weights. developed experts graphical representations their status well identify alternatives that should be reconsidered changing in subsequent round. The also includes those are identified contributing less on how change values. It proved"
https://openalex.org/W2251202616,https://doi.org/10.3115/v1/p15-2116,A Long Short-Term Memory Model for Answer Sentence Selection in Question Answering,2015,"In this paper, we present an approach that address the answer sentence selection problem for question answering. The proposed method uses a stacked bidirectional Long-Short Term Memory (BLSTM) network to sequentially read words from and sentences, then outputs their relevance scores. Unlike prior work, does not require any syntactic parsing or external knowledge resources such as WordNet which may be available in some domains languages. full system is based on combination of BLSTM model keywords matching. results our experiments public benchmark dataset TREC show outperforms previous work requires features resources."
https://openalex.org/W2499696929,https://doi.org/10.1609/aaai.v30i1.10329,Representation Learning of Knowledge Graphs with Entity Descriptions,2016,"Representation learning (RL) of knowledge graphs aims to project both entities and relations into a continuous low-dimensional space. Most methods concentrate on representations with triples indicating between entities. In fact, in most there are usually concise descriptions for entities, which cannot be well utilized by existing methods. this paper, we propose novel RL method taking advantages entity descriptions. More specifically, explore two encoders, including bag-of-words deep convolutional neural models encode semantics We further learn evaluate our tasks, graph completion classification. Experimental results real-world datasets show that, outperforms other baselines the especially under zero-shot setting, indicates that is capable building according their The source code paper can obtained from https://github.com/xrb92/DKRL."
https://openalex.org/W2606473278,https://doi.org/10.1109/tpami.2018.2797921,Learning Two-Branch Neural Networks for Image-Text Matching Tasks,2019,"Image-language matching tasks have recently attracted a lot of attention in the computer vision field. These include image-sentence matching, i.e., given an image query, retrieving relevant sentences and vice versa, region-phrase or visual grounding, phrase to regions. This paper investigates two-branch neural networks for learning similarity between these two data modalities. We propose network structures that produce different output representations. The first one, referred as embedding network, learns explicit shared latent space with maximum-margin ranking loss novel neighborhood constraints. Compared standard triplet sampling, we perform improved sampling takes information into consideration while constructing mini-batches. second structure, fuses branches via element-wise product is trained regression directly predict score. Extensive experiments show our achieve high accuracies localization on Flickr30K Entities dataset bi-directional retrieval MSCOCO datasets."
https://openalex.org/W2764166773,https://doi.org/10.1194/jlr.m079012,Harmonizing lipidomics: NIST interlaboratory comparison exercise for lipidomics using SRM 1950–Metabolites in Frozen Human Plasma,2017,"As the lipidomics field continues to advance, self-evaluation within community is critical. Here, we performed an interlaboratory comparison exercise for using Standard Reference Material (SRM) 1950-Metabolites in Frozen Human Plasma, a commercially available reference material. The study comprised 31 diverse laboratories, with each laboratory different workflow. A total of 1,527 unique lipids were measured across all laboratories and consensus location estimates associated uncertainties determined 339 these at sum composition level by five or more participating laboratories. These evaluated detected SRM 1950 serve as community-wide benchmarks intra- quality control method validation. analyses nonstandardized laboratory-independent workflows. locations also compared previous examination LIPID MAPS consortium. While central theme was provide values help harmonize lipids, lipid mediators, precursor measurements community, it initiated stimulate discussion regarding areas need improvement."
https://openalex.org/W2949433733,https://doi.org/10.48550/arxiv.1506.06724,"Aligning Books and Movies: Towards Story-like Visual Explanations by
  Watching Movies and Reading Books",2015,"Books are a rich source of both fine-grained information, how character, an object or scene looks like, as well high-level semantics, what someone is thinking, feeling and these states evolve through story. This paper aims to align books their movie releases in order provide descriptive explanations for visual content that go semantically far beyond the captions available current datasets. To movies we exploit neural sentence embedding trained unsupervised way from large corpus books, video-text computing similarities between clips sentences book. We propose context-aware CNN combine information multiple sources. demonstrate good quantitative performance movie/book alignment show several qualitative examples showcase diversity tasks our model can be used for."
https://openalex.org/W1570098300,https://doi.org/10.1145/2736277.2741627,Statistically Significant Detection of Linguistic Change,2015,"We propose a new computational approach for tracking and detecting statistically significant linguistic shifts in the meaning usage of words. Such are especially prevalent on Internet, where rapid exchange ideas can quickly change word's meaning. Our meta-analysis constructs property time series word usage, then uses sound point detection algorithms to identify shifts. consider analyze three approaches increasing complexity generate such series, culmination which distributional characteristics inferred from co-occurrences. Using recently proposed deep neural language models, we first train vector representations words each period. Second, warp spaces into one unified coordinate system. Finally, construct distance-based track its displacement over time. demonstrate that our is scalable by across years micro-blogging using Twitter, decade product reviews corpus movie Amazon, century written books Google Book Ngrams. analysis reveals interesting patterns commensurate with medium."
https://openalex.org/W2078238240,https://doi.org/10.1109/cvpr.2015.7298940,A dataset for Movie Description,2015,"Audio Description (AD) provides linguistic descriptions of movies and allows visually impaired people to follow a movie along with their peers. Such are by design mainly visual thus naturally form an interesting data source for computer vision computational linguistics. In this work we propose novel dataset which contains transcribed ADs, temporally aligned full length HD movies. addition also collected the scripts have been used in prior compare two different sources descriptions. total MPII Movie (MPII-MD) parallel corpus over 68K sentences video snippets from 94 We characterize benchmarking approaches generating Comparing ADs scripts, find that far more describe precisely what is shown rather than should happen according created production."
https://openalex.org/W2546950329,https://doi.org/10.18653/v1/p17-1003,Neural Symbolic Machines: Learning Semantic Parsers on Freebase with Weak Supervision,2017,"Harnessing the statistical power of neural networks to perform language understanding and symbolic reasoning is difficult, when it requires executing efficient discrete operations against a large knowledge-base. In this work, we introduce Neural Symbolic Machine, which contains (a) ""programmer"", i.e., sequence-to-sequence model that maps utterances programs utilizes key-variable memory handle compositionality (b) ""computer"", Lisp interpreter performs program execution, helps find good by pruning search space. We apply REINFORCE directly optimize task reward structured prediction problem. To train with weak supervision improve stability REINFORCE, augment an iterative maximum-likelihood training process. NSM outperforms state-of-the-art on WebQuestionsSP dataset trained from question-answer pairs only, without requiring any feature engineering or domain-specific knowledge."
https://openalex.org/W2741375528,https://doi.org/10.18653/v1/p17-1108,Abstractive Document Summarization with a Graph-Based Attentional Neural Model,2017,"Abstractive summarization is the ultimate goal of document research, but previously it less investigated due to immaturity text generation techniques. Recently impressive progress has been made abstractive sentence using neural models. Unfortunately, attempts on are still in a primitive stage, and evaluation results worse than extractive methods benchmark datasets. In this paper, we review difficulties summarization, propose novel graph-based attention mechanism sequence-to-sequence framework. The intuition address saliency factor which overlooked by prior works. Experimental demonstrate our model able achieve considerable improvement over previous data-driven method also competitive with state-of-the-art methods."
https://openalex.org/W2973088264,https://doi.org/10.1162/tacl_a_00288,Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond,2019,"We introduce an architecture to learn joint multilingual sentence representations for 93 languages, belonging more than 30 different families and written in 28 scripts. Our system uses a single BiLSTM encoder with shared byte-pair encoding vocabulary all which is coupled auxiliary decoder trained on publicly available parallel corpora. This enables us classifier top of the resulting embeddings using English annotated data only, transfer it any languages without modification. experiments cross-lingual natural language inference (XNLI set), document classification (MLDoc corpus mining (BUCC set) show effectiveness our approach. also new test set aligned sentences 112 that obtain strong results similarity search even low- resource languages. implementation, pre-trained encoder, are at https://github.com/facebookresearch/LASER ."
https://openalex.org/W2964341035,https://doi.org/10.1109/mlsp.2016.7738886,ITEM2VEC: Neural item embedding for collaborative filtering,2016,"Many Collaborative Filtering (CF) algorithms are item-based in the sense that they analyze item-item relations order to produce item similarities. Recently, several works field of Natural Language Processing (NLP) suggested learn a latent representation words using neural embedding algorithms. Among them, Skip-gram with Negative Sampling (SGNS), also known as word2vec, was shown provide state-of-the-art results on various linguistics tasks. In this paper, we show CF can be cast same framework word embedding. Inspired by SGNS, describe method name item2vec for produces items space. The is capable inferring even when user information not available. We present experimental demonstrate effectiveness and it competitive SVD."
https://openalex.org/W2964022985,https://doi.org/10.18653/v1/p19-1279,Matching the Blanks: Distributional Similarity for Relation Learning,2019,"General purpose relation extractors, which can model arbitrary relations, are a core aspiration in information extraction. Efforts have been made to build general extractors that represent relations with their surface forms, or jointly embed forms from an existing knowledge graph. However, both of these approaches limited ability generalize. In this paper, we on extensions Harris’ distributional hypothesis as well recent advances learning text representations (specifically, BERT), task agnostic solely entity-linked text. We show significantly outperform previous work exemplar based extraction (FewRel) even without using any task’s training data. also models initialized our representations, and then tuned supervised datasets, the methods SemEval 2010 Task 8, KBP37, TACRED"
https://openalex.org/W1808906688,https://doi.org/10.1371/journal.pone.0137041,Characterizing the Google Books Corpus: Strong Limits to Inferences of Socio-Cultural and Linguistic Evolution,2015,"It is tempting to treat frequency trends from the Google Books data sets as indicators of ""true"" popularity various words and phrases. Doing so allows us draw quantitatively strong conclusions about evolution cultural perception a given topic, such time or gender. However, corpus suffers number limitations which make it an obscure mask popularity. A primary issue that in effect library, containing one each book. single, prolific author thereby able noticeably insert new phrases into lexicon, whether widely read not. With this understood, remains important set be considered more lexicon-like than text-like. Here, we show distinct problematic feature arises inclusion scientific texts, have become increasingly substantive portion throughout 1900 s. The result surge typical academic articles but less common general, references form citations. We use information theoretic methods highlight these dynamics by examining comparing major contributions via divergence measure English between decades period 1800-2000. find only Fiction second version not heavily affected professional texts. Overall, our findings call question vast majority existing claims drawn corpus, point need fully characterize before using broad linguistic evolution."
https://openalex.org/W2799437918,https://doi.org/10.1073/pnas.1716999115,An explainable deep machine vision framework for plant stress phenotyping,2018,"Significance Plant stress identification based on visual symptoms has predominately remained a manual exercise performed by trained pathologists, primarily due to the occurrence of confounding symptoms. However, rating process is tedious, time-consuming, and suffers from inter- intrarater variabilities. Our work resolves such issues via concept explainable deep machine learning automate plant identification, classification, quantification. We construct very accurate model that can not only deliver pathologist-level performance but also explain which are used make predictions. demonstrate our method applicable large variety biotic abiotic stresses transferable other imaging conditions plants."
https://openalex.org/W2963542836,https://doi.org/10.18653/v1/n16-1170,Learning Natural Language Inference with LSTM,2016,"Natural language inference (NLI) is a fundamentally important task in natural processing that has many applications. The recently released Stanford Language Inference (SNLI) corpus made it possible to develop and evaluate learning-centered methods such as deep neural networks for (NLI). In this paper, we propose special long short-term memory (LSTM) architecture NLI. Our model builds on top of proposed attention NLI but based significantly different idea. Instead deriving sentence embeddings the premise hypothesis be used classification, our solution uses match-LSTM perform word-by-word matching with premise. This LSTM able place more emphasis word-level results. particular, observe remembers mismatches are critical predicting contradiction or neutral relationship label. On SNLI corpus, achieves an accuracy 86.1%, outperforming state art."
https://openalex.org/W2964120615,https://doi.org/10.18653/v1/n18-1059,The Web as a Knowledge-Base for Answering Complex Questions,2018,"Answering complex questions is a time-consuming activity for humans that requires reasoning and integration of information. Recent work on reading comprehension made headway in answering simple questions, but tackling still an ongoing research challenge. Conversely, semantic parsers have been successful at handling compositionality, only when the information resides target knowledge-base. In this paper, we present novel framework broad assuming possible using search engine model. We propose to decompose into sequence compute final answer from answers. To illustrate viability our approach, create new dataset ComplexWebQuestions, model decomposes interacts with web answer. empirically demonstrate question decomposition improves performance 20.8 precision@1 27.5 dataset."
https://openalex.org/W2988937804,https://doi.org/10.18653/v1/2020.acl-demos.30,DIALOGPT : Large-Scale Generative Pre-training for Conversational Response Generation,2020,"We present a large, tunable neural conversational response generation model, DialoGPT (dialogue generative pre-trained transformer). Trained on 147M conversation-like exchanges extracted from Reddit comment chains over period spanning 2005 through 2017, extends the Hugging Face PyTorch transformer to attain performance close human both in terms of automatic and evaluation single-turn dialogue settings. show that systems leverage generate more relevant, contentful context-consistent responses than strong baseline systems. The model training pipeline are publicly released facilitate research into development intelligent open-domain"
https://openalex.org/W3035252911,https://doi.org/10.18653/v1/2020.acl-main.704,BLEURT: Learning Robust Metrics for Text Generation,2020,"Text generation has made significant advances in the last few years. Yet, evaluation metrics have lagged behind, as most popular choices (e.g., BLEU and ROUGE) may correlate poorly with human judgment. We propose BLEURT, a learned metric for English based on BERT. BLEURT can model judgment thousand possibly biased training examples. A key aspect of our approach is novel pre-training scheme that uses millions synthetic examples to help generalize. provides state-of-the-art results three years WMT Metrics shared task WebNLG data set. In contrast vanilla BERT-based approach, it yields superior even when scarce out-of-distribution."
https://openalex.org/W2558535589,https://doi.org/10.1109/cvpr.2017.470,Modeling Relationships in Referential Expressions with Compositional Modular Networks,2017,"People often refer to entities in an image terms of their relationships with other entities. For example, the black cat sitting under table refers both a entity and its relationship another entity. Understanding these is essential for interpreting grounding such natural language expressions. Most prior work focuses on either entire referential expressions holistically one region, or localizing based fixed set categories. In this paper we instead present modular deep architecture capable analyzing into component parts, identifying mentioned input expression them all scene. We call approach Compositional Modular Networks (CMNs): novel that learns linguistic analysis visual inference end-to-end. Our built around two types neural modules inspect local regions pairwise interactions between regions. evaluate CMNs multiple datasets, outperforming state-of-the-art approaches tasks."
https://openalex.org/W2963527228,https://doi.org/10.18653/v1/p17-1103,Towards an Automatic Turing Test: Learning to Evaluate Dialogue Responses,2017,"Automatically evaluating the quality of dialogue responses for unstructured domains is a challenging problem. Unfortunately, existing automatic evaluation metrics are biased and correlate very poorly with human judgements response (Liu et al., 2016). Yet having an accurate procedure crucial research, as it allows rapid prototyping testing new models fewer expensive evaluations. In to this challenge, we formulate learning problem.We present model (ADEM)that learns predict human-like scores input responses, using dataset scores. We show that ADEM model’s predictions significantly, at level much higher than word-overlap such BLEU, both utterance system-level. also can generalize mod-els unseen during training, important step evaluation."
https://openalex.org/W2963876447,https://doi.org/10.18653/v1/w16-2209,Linguistic Input Features Improve Neural Machine Translation,2016,"Neural machine translation has recently achieved impressive results, while using little in the way of external linguistic information. In this paper we show that strong learning capability neural MT models does not make features redundant; they can be easily incorporated to provide further improvements performance. We generalize embedding layer encoder attentional encoder–decoder architecture support inclusion arbitrary features, addition baseline word feature. add morphological part-ofspeech tags, and syntactic dependency labels as input English↔German English→Romanian systems. experiments on WMT16 training test sets, find improve model quality according three metrics: perplexity, BLEU CHRF3. An opensource implementation our system is available1, are sample files configurations2."
https://openalex.org/W2118232585,https://doi.org/10.1016/j.cognition.2015.03.016,Compression and communication in the cultural evolution of linguistic structure,2015,"Language exhibits striking systematic structure. Words are composed of combinations reusable sounds, and those words in turn combined to form complex sentences. These properties make language unique among natural communication systems enable our species convey an open-ended set messages. We provide a cultural evolutionary account the origins this show, using simulations rational learners laboratory experiments, that structure arises from trade-off between pressures for compressibility (imposed during learning) expressivity communication). further demonstrate relative strength these two can be varied different social contexts, leading novel predictions about emergence structured behaviour wild."
https://openalex.org/W2963398599,https://doi.org/10.1109/cvpr.2016.500,Ask Me Anything: Free-Form Visual Question Answering Based on Knowledge from External Sources,2016,"We propose a method for visual question answering which combines an internal representation of the content image with information extracted from general knowledge base to answer broad range image-based questions. This allows more complex questions be answered using predominant neural network-based approach than has previously been possible. It particularly asked about contents image, even when itself does not contain whole answer. The constructs textual semantic and merges it sourced base, develop deeper understanding scene viewed. Priming recurrent network this combined information, submitted question, leads very flexible approach. are specifically able posed in natural language, that refer contained image. demonstrate effectiveness our model on two publicly available datasets, Toronto COCO-QA MS COCO-VQA show produces best reported results both cases."
https://openalex.org/W2963631950,https://doi.org/10.18653/v1/p18-1080,Style Transfer Through Back-Translation,2018,"Style transfer is the task of rephrasing text to contain specific stylistic properties without changing intent or affect within context. This paper introduces a new method for automatic style transfer. We first learn latent representation input sentence which grounded in language translation model order better preserve meaning while reducing properties. Then adversarial generation techniques are used make output match desired style. evaluate this technique on three different transformations: sentiment, gender and political slant. Compared two state-of-the-art modeling we show improvements both evaluation manual preservation fluency."
https://openalex.org/W2970279348,https://doi.org/10.18653/v1/w19-5301,Findings of the 2019 Conference on Machine Translation (WMT19),2019,"This paper presents the results of premier shared task organized alongside Conference on Machine Translation (WMT) 2019. Participants were asked to build machine translation systems for any 18 language pairs, be evaluated a test set news stories. The main metric this is human judgment quality. was also opened up additional suites probe specific aspects translation."
https://openalex.org/W2963035145,https://doi.org/10.48550/arxiv.1606.08340,Topic Aware Neural Response Generation,2017,"We consider incorporating topic information into the sequence-to-sequence framework to generate informative and interesting responses for chatbots. To this end, we propose a aware (TA-Seq2Seq) model. The model utilizes topics simulate prior knowledge of human that guides them form in conversation, leverages generation by joint attention mechanism biased probability. summarizes hidden vectors an input message as context attention, synthesizes from words obtained pre-trained LDA model, let these jointly affect decoding. increase possibility appearing responses, modifies probability adding extra item bias overall distribution. Empirical study on both automatic evaluation metrics annotations shows TA-Seq2Seq can more significantly outperform the-state-of-the-art response models."
https://openalex.org/W2963101081,https://doi.org/10.1609/aaai.v33i01.33013027,ATOMIC: An Atlas of Machine Commonsense for If-Then Reasoning,2019,"We present ATOMIC, an atlas of everyday commonsense reasoning, organized through 877k textual descriptions inferential knowledge. Compared to existing resources that center around taxonomic knowledge, ATOMIC focuses on knowledge as typed if-then relations with variables (e.g., “if X pays Y a compliment, then will likely return the compliment”). propose nine relation types distinguish causes vs. effects, agents themes, voluntary involuntary events, and actions mental states. By generatively training rich described in we show neural models can acquire simple capabilities reason about previously unseen events. Experimental results demonstrate multitask incorporate hierarchical structure lead more accurate inference compared trained isolation, measured by both automatic human evaluation."
https://openalex.org/W2963840241,https://doi.org/10.1109/cvpr.2018.00788,Multi-oriented Scene Text Detection via Corner Localization and Region Segmentation,2018,"Previous deep learning based state-of-the-art scene text detection methods can be roughly classified into two categories. The first category treats as a type of general objects and follows object paradigm to localize by regressing the box locations, but troubled arbitrary-orientation large aspect ratios text. second one segments regions directly, mostly needs complex post processing. In this paper, we present method that combines ideas types while avoiding their shortcomings. We propose detect localizing corner points bounding boxes segmenting in relative positions. inference stage, candidate are generated sampling grouping points, which further scored segmentation maps suppressed NMS. Compared with previous methods, our handle long oriented naturally doesn't need experiments on ICDAR2013, ICDAR2015, MSRA-TD500, MLT COCO-Text demonstrate proposed algorithm achieves better or comparable results both accuracy efficiency. Based VGG16, it an F-measure 84.3% ICDAR2015 81.5% MSRA-TD500."
https://openalex.org/W2181854537,https://doi.org/10.14257/ijmue.2015.10.4.21,A Lexicon-based Approach for Hate Speech Detection,2015,"We explore the idea of creating a classifier that can be used to detect presence hate speech in web discourses such as forums and blogs. In this work, problem is abstracted into three main thematic areas race, nationality religion. The goal our research create model uses sentiment analysis techniques particular subjectivity detection not only given sentence subjective but also identify rate polarity expressions. begin by whittling down document size removing objective sentences. Then, using semantic features related speech, we lexicon employed build for detection. Experiments with corpus show significant practical application real-world discourse."
https://openalex.org/W2251292973,https://doi.org/10.3115/v1/p15-1098,Learning Semantic Representations of Users and Products for Document Level Sentiment Classification,2015,"Neural network methods have achieved promising results for sentiment classification of text. However, these models only use semantics texts, while ignoring users who express the and products which are evaluated, both great influences on interpreting In this paper, we address issue by incorporating userand productlevel information into a neural approach document level classification. Users modeled using vector space models, representations capture important global clues such as individual preferences or overall qualities products. Such evidence in turn facilitates embedding learning procedure at level, yielding better text representations. By combining user-, productand documentlevel unified framework, proposed model achieves state-of-the-art performances IMDB Yelp datasets1."
https://openalex.org/W2552839021,https://doi.org/10.18653/v1/p17-1012,A Convolutional Encoder Model for Neural Machine Translation,2017,The prevalent approach to neural machine translation relies on bi-directional LSTMs encode the source sentence. We present a faster and simpler architecture based succession of convolutional layers. This allows sentence simultaneously compared recurrent networks for which computation is constrained by temporal dependencies. On WMT’16 English-Romanian we achieve competitive accuracy state-of-the-art WMT’15 English-German outperform several recently published results. Our models obtain almost same as very deep LSTM setup WMT’14 English-French translation. speed up CPU decoding more than two times at or higher strong LSTM.
https://openalex.org/W2890187992,https://doi.org/10.18653/v1/d18-1032,Cross-lingual Knowledge Graph Alignment via Graph Convolutional Networks,2018,"Multilingual knowledge graphs (KGs) such as DBpedia and YAGO contain structured of entities in several distinct languages, they are useful resources for cross-lingual AI NLP applications. Cross-lingual KG alignment is the task matching with their counterparts different which an important way to enrich links multilingual KGs. In this paper, we propose a novel approach via graph convolutional networks (GCNs). Given set pre-aligned entities, our trains GCNs embed each language into unified vector space. Entity alignments discovered based on distances between embedding Embeddings can be learned from both structural attribute information results structure combined get accurate alignments. experiments aligning real KGs, gets best performance compared other embedding-based approaches."
https://openalex.org/W2963457723,https://doi.org/10.18653/v1/n18-2002,Gender Bias in Coreference Resolution,2018,"We present an empirical study of gender bias in coreference resolution systems. first introduce a novel, Winograd schema-style set minimal pair sentences that differ only by pronoun gender. With these “Winogender schemas,” we evaluate and confirm systematic three publicly-available systems, correlate this with real-world textual statistics."
https://openalex.org/W2963240575,https://doi.org/10.18653/v1/p18-1087,Transformation Networks for Target-Oriented Sentiment Classification,2018,"Target-oriented sentiment classification aims at classifying polarities over individual opinion targets in a sentence. RNN with attention seems good fit for the characteristics of this task, and indeed it achieves state-of-the-art performance. After re-examining drawbacks mechanism obstacles that block CNN to perform well we propose new model results on few benchmarks. Instead attention, our employs layer extract salient features from transformed word representations originated bi-directional layer. Between two layers, component which first generates target-specific words sentence, then incorporates preserving original contextual information"
https://openalex.org/W2741447225,https://doi.org/10.18653/v1/p17-1067,EmoNet: Fine-Grained Emotion Detection with Gated Recurrent Neural Networks,2017,"Accurate detection of emotion from natural language has applications ranging building emotional chatbots to better understanding individuals and their lives. However, progress on been hampered by the absence large labeled datasets. In this work, we build a very dataset for fine-grained emotions develop deep learning models it. We achieve new state-of-the-art 24 types (with an average accuracy 87.58%). also extend task beyond model Robert Plutick’s 8 primary dimensions, acquiring superior 95.68%."
https://openalex.org/W2891602716,https://doi.org/10.18653/v1/d18-1217,Semi-Supervised Sequence Modeling with Cross-View Training,2018,"Unsupervised representation learning algorithms such as word2vec and ELMo improve the accuracy of many supervised NLP models, mainly because they can take advantage large amounts unlabeled text. However, models only learn from task-specific labeled data during main training phase. We therefore propose Cross-View Training (CVT), a semi-supervised algorithm that improves representations Bi-LSTM sentence encoder using mix data. On examples, standard is used. CVT teaches auxiliary prediction modules see restricted views input (e.g., part sentence) to match predictions full model seeing whole input. Since share intermediate representations, this in turn model. Moreover, we show particularly effective when combined with multi-task learning. evaluate on five sequence tagging tasks, machine translation, dependency parsing, achieving state-of-the-art results."
https://openalex.org/W2949128310,https://doi.org/10.18653/v1/p19-1103,Generating Natural Language Adversarial Examples through Probability Weighted Word Saliency,2019,"We address the problem of adversarial attacks on text classification, which is rarely studied comparing to image classification. The challenge this task generate examples that maintain lexical correctness, grammatical correctness and semantic similarity. Based synonyms substitution strategy, we introduce a new word replacement order determined by both saliency classification probability, propose greedy algorithm called probability weighted (PWWS) for attack. Experiments three popular datasets using convolutional as well LSTM models show PWWS reduces accuracy most extent, keeps very low rate. A human evaluation study shows our generated similarity are hard humans perceive. Performing training perturbed improves robustness models. At last, method also exhibits good transferability examples."
https://openalex.org/W2963926728,https://doi.org/10.18653/v1/n18-2097,A Discourse-Aware Attention Model for Abstractive Summarization of Long Documents,2018,"Neural abstractive summarization models have led to promising results in summarizing relatively short documents. We propose the first model for of single, longer-form documents (e.g., research papers). Our approach consists a new hierarchical encoder that discourse structure document, and an attentive discourse-aware decoder generate summary. Empirical on two large-scale datasets scientific papers show our significantly outperforms state-of-the-art models."
https://openalex.org/W2964154091,https://doi.org/10.1109/asru.2015.7404872,Applying deep learning to answer selection: A study and an open task,2015,"We apply a general deep learning framework to address the non-factoid question answering task. Our approach does not rely on any linguistic tools and can be applied different languages or domains. Various architectures are presented compared. create release QA corpus setup new task in insurance domain. Experimental results demonstrate superior performance compared baseline methods various technologies give further improvements. For this highly challenging task, top-1 accuracy reach up 65.3% test set, which indicates great potential for practical use."
https://openalex.org/W2964331270,https://doi.org/10.18653/v1/n16-1062,Sequential Short-Text Classification with Recurrent and Convolutional Neural Networks,2016,"Recent approaches based on artificial neural networks (ANNs) have shown promising results for short-text classification. However, many short texts occur in sequences (e.g., sentences a document or utterances dialog), and most existing ANN-based systems do not leverage the preceding when classifying subsequent one. In this work, we present model recurrent convolutional that incorporates texts. Our achieves state-of-the-art three different datasets dialog act prediction."
https://openalex.org/W2178725228,https://doi.org/10.1162/tacl_a_00140,Improving Topic Models with Latent Feature Word Representations,2015,"Probabilistic topic models are widely used to discover latent topics in document collections, while feature vector representations of words have been obtain high performance many NLP tasks. In this paper, we extend two different Dirichlet multinomial by incorporating trained on very large corpora improve the word-topic mapping learnt a smaller corpus. Experimental results show that using information from external corpora, our new produce significant improvements coherence, clustering and classification tasks, especially datasets with few or short documents."
https://openalex.org/W2578363764,https://doi.org/10.1109/lra.2017.2651944,Counting Apples and Oranges With Deep Learning: A Data-Driven Approach,2017,"This paper describes a fruit counting pipeline based on deep learning that accurately counts in unstructured environments. Obtaining reliable is challenging because of variations appearance due to illumination changes and occlusions from foliage neighboring fruits. We propose novel approach uses map input images total counts. The utilizes custom crowdsourcing platform quickly label large data sets. A blob detector fully convolutional network extracts candidate regions the images. algorithm second then estimates number fruits each region. Finally, linear regression model maps count estimate final count. analyze performance two distinct sets oranges daylight, green apples at night, utilizing human generated labels as ground truth. also show has short training time performs well with limited set size. Our method generalizes across both able perform even highly occluded are for labelers annotate."
https://openalex.org/W2912327653,https://doi.org/10.1016/j.neucom.2019.02.003,Survey on semantic segmentation using deep learning techniques,2019,"Abstract Semantic segmentation is a challenging task in computer vision systems. A lot of methods have been developed to tackle this problem ranging from autonomous vehicles, human-computer interaction, robotics, medical research, agriculture and so on. Many these built using the deep learning paradigm that has shown salient performance. For reason, we propose survey by, first categorizing them into ten different classes according common concepts underlying their architectures. Second, by providing an overview publicly available datasets on which they assessed. In addition, present evaluation matrix used measure accuracy. Moreover, focus some look closely at architectures order find out how achieved reported performances. Finally, conclude discussing open problems possible solutions."
https://openalex.org/W2963595025,https://doi.org/10.18653/v1/p16-1086,Text Understanding with the Attention Sum Reader Network,2016,"Several large cloze-style context-question-answer datasets have been introduced recently: the CNN and Daily Mail news data Children's Book Test. Thanks to size of these datasets, associated text comprehension task is well suited for deep-learning techniques that currently seem outperform all alternative approaches. We present a new, simple model uses attention directly pick answer from context as opposed computing using blended representation words in document usual similar models. This makes particularly suitable question-answering problems where single word document. Ensemble our models sets new state art on evaluated datasets."
https://openalex.org/W2963777632,https://doi.org/10.18653/v1/d18-1514,FewRel: A Large-Scale Supervised Few-Shot Relation Classification Dataset with State-of-the-Art Evaluation,2018,"We present a Few-Shot Relation Classification Dataset (dataset), consisting of 70, 000 sentences on 100 relations derived from Wikipedia and annotated by crowdworkers. The relation each sentence is first recognized distant supervision methods, then filtered adapt the most recent state-of-the-art few-shot learning methods for classification conduct thorough evaluation these methods. Empirical results show that even competitive models struggle this task, especially as compared with humans. also range different reasoning skills are needed to solve our task. These indicate remains an open problem still requires further research. Our detailed analysis points multiple directions future"
https://openalex.org/W2963815618,https://doi.org/10.1007/978-3-030-01249-6_17,ExFuse: Enhancing Feature Fusion for Semantic Segmentation,2018,"Modern semantic segmentation frameworks usually combine low-level and high-level features from pre-trained backbone convolutional models to boost performance. In this paper, we first point out that a simple fusion of could be less effective because the gap in levels spatial resolution. We find introducing information into high-resolution details is more for later fusion. Based on observation, propose new framework, named ExFuse, bridge between thus significantly improve quality by 4.0\% total. Furthermore, evaluate our approach challenging PASCAL VOC 2012 benchmark achieve 87.9\% mean IoU, which outperforms previous state-of-the-art results."
https://openalex.org/W2963943967,https://doi.org/10.18653/v1/w17-3012,Understanding Abuse: A Typology of Abusive Language Detection Subtasks,2017,"As the body of research on abusive language detection and analysis grows, there is a need for critical consideration relationships between different subtasks that have been grouped under this label. Based work hate speech, cyberbullying, online abuse we propose typology captures central similarities differences discuss implications data annotation feature construction. We emphasize practical actions can be taken by researchers to best approach their subtask interest."
https://openalex.org/W2962881743,https://doi.org/10.18653/v1/k17-1034,Zero-Shot Relation Extraction via Reading Comprehension,2017,"We show that relation extraction can be reduced to answering simple reading comprehension questions, by associating one or more natural-language questions with each slot. This reduction has several advantages: we (1) learn relation-extraction models extending recent neural reading-comprehension techniques, (2) build very large training sets for those combining relation-specific crowd-sourced distant supervision, and even (3) do zero-shot learning extracting new types are only specified at test-time, which have no labeled examples. Experiments on a Wikipedia slot-filling task demonstrate the approach generalize known high accuracy, generalization unseen is possible, lower accuracy levels, setting bar future work this task."
https://openalex.org/W2952984539,https://doi.org/10.18653/v1/p19-1459,Probing Neural Network Comprehension of Natural Language Arguments,2019,"We are surprised to find that BERT's peak performance of 77% on the Argument Reasoning Comprehension Task reaches just three points below average untrained human baseline. However, we show this result is entirely accounted for by exploitation spurious statistical cues in dataset. analyze nature these and demonstrate a range models all exploit them. This analysis informs construction an adversarial dataset which achieve random accuracy. Our provides more robust assessment argument comprehension should be adopted as standard future work."
https://openalex.org/W2963357517,https://doi.org/10.18653/v1/p18-1068,Coarse-to-Fine Decoding for Neural Semantic Parsing,2018,"Semantic parsing aims at mapping natural language utterances into structured meaning representations. In this work, we propose a structure-aware neural architecture which decomposes the semantic process two stages. Given an input utterance, first generate rough sketch of its meaning, where low-level information (such as variable names and arguments) is glossed over. Then, fill in missing details by taking account itself. Experimental results on four datasets characteristic different domains representations show that our approach consistently improves performance, achieving competitive despite use relatively simple decoders."
https://openalex.org/W2964219651,https://doi.org/10.1109/tsmc.2016.2560521,Managing Multigranular Linguistic Distribution Assessments in Large-Scale Multiattribute Group Decision Making,2017,"Linguistic large-scale group decision making (LGDM) problems are more and common nowadays. In such a large of makers involved in the process elicit linguistic information that usually assessed different scales with diverse granularity because makers' distinct knowledge background. To keep maximum initial stages LGDM problems, use multi-granular distribution assessments seems suitable choice, however to manage multigranular assessments, it is necessary development new computational approach. this paper proposed novel model based on extended hierarchies, which not only can be used operate but also provide interpretable results makers. Based model, an approach multi-attribute applied talent selection universities."
https://openalex.org/W2603266952,https://doi.org/10.1109/iccv.2017.321,Learning Cooperative Visual Dialog Agents with Deep Reinforcement Learning,2017,"We introduce the first goal-driven training for visual question answering and dialog agents. Specifically, we pose a cooperative ‘image guessing’ game between two agents – Q-BOT A-BOT– who communicate in natural language so that can select an unseen image from lineup of images. use deep reinforcement learning (RL) to learn policies these end-to-end pixels multi-agent multi-round reward.,,We demonstrate experimental results.,,First, as ‘sanity check’ demonstration pure RL (from scratch), show results on synthetic world, where ungrounded vocabularies, i.e., symbols with no pre-specified meanings (X, Y, Z). find bots invent their own communication protocol start using certain ask/answer about attributes (shape/color/style). Thus, emergence grounded among ‘visual’ human supervision.,,Second, conduct large-scale real-image experiments VisDial dataset [5], pretrain data supervised (SL) finetuned significantly outperform pretraining. Interestingly, learns ask questions A-BOT is good at, ultimately resulting more informative better team."
https://openalex.org/W2963385935,https://doi.org/10.18653/v1/n18-1150,Deep Communicating Agents for Abstractive Summarization,2018,"We present deep communicating agents in an encoder-decoder architecture to address the challenges of representing a long document for abstractive summarization. With agents, task encoding text is divided across multiple collaborating each charge subsection input text. These encoders are connected single decoder, trained end-to-end using reinforcement learning generate focused and coherent summary. Empirical results demonstrate that lead higher quality summary compared several strong baselines, including those based on encoder or non-communicating encoders."
https://openalex.org/W2890007195,https://doi.org/10.18653/v1/d18-1399,Unsupervised Statistical Machine Translation,2018,"While modern machine translation has relied on large parallel corpora, a recent line of work managed to train Neural Machine Translation (NMT) systems from monolingual corpora only (Artetxe et al., 2018c; Lample 2018). Despite the potential this approach for low-resource settings, existing are far behind their supervised counterparts, limiting practical interest. In paper, we propose an alternative based phrase-based Statistical (SMT) that significantly closes gap with systems. Our method profits modular architecture SMT: first induce phrase table through cross-lingual embedding mappings, combine it n-gram language model, and fine-tune hyperparameters unsupervised MERT variant. addition, iterative backtranslation improves results further, yielding, instance, 14.08 26.22 BLEU points in WMT 2014 English-German English-French, respectively, improvement more than 7-10 over previous systems, closing SMT (Moses trained Europarl) down 2-5 points. implementation is available at https://github.com/artetxem/monoses"
https://openalex.org/W2964142373,https://doi.org/10.18653/v1/n18-1100,Explainable Prediction of Medical Codes from Clinical Text,2018,"Clinical notes are text documents that created by clinicians for each patient encounter. They typically accompanied medical codes, which describe the diagnosis and treatment. Annotating these codes is labor intensive error prone; furthermore, connection between not annotated, obscuring reasons details behind specific diagnoses treatments. We present an attentional convolutional network predicts from clinical text. Our method aggregates information across document using a neural network, uses attention mechanism to select most relevant segments of thousands possible codes. The accurate, achieving precision@8 0.71 Micro-F1 0.54, both better than prior state art. Furthermore, through interpretability evaluation physician, we show identifies meaningful explanations code assignment."
https://openalex.org/W2217433794,https://doi.org/10.1109/tip.2016.2547588,Text-Attentional Convolutional Neural Network for Scene Text Detection,2016,"Recent deep learning models have demonstrated strong capabilities for classifying text and non-text components in natural images. They extract a high-level feature computed globally from whole image component (patch), where the cluttered background information may dominate true features representation. This leads to less discriminative power poorer robustness. In this work, we present new system scene detection by proposing novel Text-Attentional Convolutional Neural Network (Text-CNN) that particularly focuses on extracting text-related regions components. We develop mechanism train Text-CNN with multi-level rich supervised information, including region mask, character label, binary text/nontext information. The supervision enables capability discriminating ambiguous texts, also increases its robustness against complicated training process is formulated as multi-task problem, low-level greatly facilitates main task of text/non-text classification. addition, powerful detector called Contrast- Enhancement Maximally Stable Extremal Regions (CE-MSERs) developed, which extends widely-used MSERs enhancing intensity contrast between patterns background. allows it detect highly challenging patterns, resulting higher recall. Our approach achieved promising results ICDAR 2013 dataset, F-measure 0.82, improving state-of-the-art substantially."
https://openalex.org/W2781487490,https://doi.org/10.1109/access.2017.2776930,Deep Convolution Neural Networks for Twitter Sentiment Analysis,2018,"Twitter sentiment analysis technology provides the methods to survey public emotion about events or products related them. Most of current researches are focusing on obtaining features by analyzing lexical and syntactic features. These expressed explicitly through words, emoticons, exclamation marks, so on. In this paper, we introduce a word embeddings method obtained unsupervised learning based large twitter corpora, using latent contextual semantic relationships co-occurrence statistical characteristics between words in tweets. combined with n-grams polarity score form feature set The is integrated into deep convolution neural network for training predicting classification labels. We experimentally compare performance our model baseline that five data sets, results indicate performs better accuracy F1-measure classification."
https://openalex.org/W3037831233,https://doi.org/10.18653/v1/2020.acl-main.485,Language (Technology) is Power: A Critical Survey of “Bias” in NLP,2020,"We survey 146 papers analyzing bias in NLP systems, finding that their motivations are often vague, inconsistent, and lacking normative reasoning, despite the fact is an inherently process. further find these papers' proposed quantitative techniques for measuring or mitigating poorly matched to do not engage with relevant literature outside of NLP. Based on findings, we describe beginnings a path forward by proposing three recommendations should guide work systems. These rest greater recognition relationships between language social hierarchies, encouraging researchers practitioners articulate conceptualizations bias---i.e., what kinds system behaviors harmful, ways, whom, why, as well reasoning underlying statements---and center around lived experiences members communities affected while interrogating reimagining power relations technologists such communities."
https://openalex.org/W3176923149,https://doi.org/10.1186/s40537-021-00492-0,Text Data Augmentation for Deep Learning,2021,"Abstract Natural Language Processing (NLP) is one of the most captivating applications Deep Learning. In this survey, we consider how Data Augmentation training strategy can aid in its development. We begin with major motifs summarized into strengthening local decision boundaries, brute force training, causality and counterfactual examples, distinction between meaning form. follow these a concrete list augmentation frameworks that have been developed for text data. Learning generally struggles measurement generalization characterization overfitting. highlight studies cover augmentations construct test sets generalization. NLP at an early stage applying compared to Computer Vision. key differences promising ideas yet be tested NLP. For sake practical implementation, describe tools facilitate such as use consistency regularization, controllers, offline online pipelines, preview few. Finally, discuss interesting topics around task-specific augmentations, prior knowledge self-supervised learning versus Augmentation, intersections transfer multi-task learning, AI-GAs (AI-Generating Algorithms). hope paper inspires further research interest Text Augmentation."
https://openalex.org/W2997200074,https://doi.org/10.1609/aaai.v34i05.6428,ERNIE 2.0: A Continual Pre-Training Framework for Language Understanding,2020,"Recently, pre-trained models have achieved state-of-the-art results in various language understanding tasks, which indicates that pre-training on large-scale corpora may play a crucial role natural processing. Current procedures usually focus training the model with several simple tasks to grasp co-occurrence of words or sentences. However, besides co-occurring, there exists other valuable lexical, syntactic and semantic information corpora, such as named entity, closeness discourse relations. In order extract fullest extent, from we propose continual framework ERNIE 2.0 builds learns incrementally through constant multi-task learning. Experimental demonstrate outperforms BERT XLNet 16 including English GLUE benchmarks common Chinese. The source codes been released at https://github.com/PaddlePaddle/ERNIE."
https://openalex.org/W3011594683,https://doi.org/10.1109/tkde.2020.2981314,A Survey on Deep Learning for Named Entity Recognition,2020,"Named entity recognition (NER) is the task to identify mentions of rigid designators from text belonging predefined semantic types such as person, location, organization etc. NER always serves foundation for many natural language applications question answering, summarization, and machine translation. Early systems got a huge success in achieving good performance with cost human engineering designing domain-specific features rules. In recent years, deep learning, empowered by continuous real-valued vector representations composition through nonlinear processing, has been employed systems, yielding stat-of-the-art performance. this paper, we provide comprehensive review on existing learning techniques NER. We first introduce resources, including tagged corpora off-the-shelf tools. Then, systematically categorize works based taxonomy along three axes: distributed input, context encoder, tag decoder. Next, survey most representative methods applied new problem settings applications. Finally, present readers challenges faced outline future directions area."
https://openalex.org/W2963167649,https://doi.org/10.18653/v1/d16-1245,Deep Reinforcement Learning for Mention-Ranking Coreference Models,2016,"Coreference resolution systems are typically trained with heuristic loss functions that require careful tuning. In this paper we instead apply reinforcement learning to directly optimize a neural mention-ranking model for coreference evaluation metrics. We experiment two approaches: the REINFORCE policy gradient algorithm and reward-rescaled max-margin objective. find latter be more effective, resulting in significant improvements over current state-of-the-art on English Chinese portions of CoNLL 2012 Shared Task."
https://openalex.org/W2964051877,https://doi.org/10.18653/v1/p19-1656,Multimodal Transformer for Unaligned Multimodal Language Sequences,2019,"Human language is often multimodal, which comprehends a mixture of natural language, facial gestures, and acoustic behaviors. However, two major challenges in modeling such multimodal human time-series data exist: 1) inherent non-alignment due to variable sampling rates for the sequences from each modality; 2) long-range dependencies between elements across modalities. In this paper, we introduce Multimodal Transformer (MulT) generically address above issues an end-to-end manner without explicitly aligning data. At heart our model directional pairwise cross-modal attention, attends interactions distinct time steps latently adapt streams one modality another. Comprehensive experiments on both aligned non-aligned show that outperforms state-of-the-art methods by large margin. addition, empirical analysis suggests correlated crossmodal signals are able be captured proposed attention mechanism MulT."
https://openalex.org/W2970352191,https://doi.org/10.18653/v1/w19-4302,To Tune or Not to Tune? Adapting Pretrained Representations to Diverse Tasks,2019,"While most previous work has focused on different pretraining objectives and architectures for transfer learning, we ask how to best adapt the pretrained model a given target task. We focus two common forms of adaptation, feature extraction (where weights are frozen), directly fine-tuning model. Our empirical results across diverse NLP tasks with state-of-the-art models show that relative performance vs. depends similarity tasks. explore possible explanations this finding provide set adaptation guidelines practitioner."
https://openalex.org/W2998356391,https://doi.org/10.1609/aaai.v34i07.6795,Unicoder-VL: A Universal Encoder for Vision and Language by Cross-Modal Pre-Training,2020,"We propose Unicoder-VL, a universal encoder that aims to learn joint representations of vision and language in pre-training manner. Borrow ideas from cross-lingual pre-trained models, such as XLM (Lample Conneau 2019) Unicoder (Huang et al. 2019), both visual linguistic contents are fed into multi-layer Transformer (Vaswani 2017) for the cross-modal pre-training, where three tasks employed, including Masked Language Modeling(MLM), Object Classification(MOC) Visual-linguistic Matching(VLM). The first two context-aware input tokens based on jointly. last task tries predict whether an image text describe each other. After pretraining large-scale image-caption pairs, we transfer Unicoder-VL caption-based image-text retrieval commonsense reasoning, with just one additional output layer. achieve state-of-the-art or comparable results show powerful ability pre-training."
https://openalex.org/W1814992895,https://doi.org/10.1162/tacl_a_00143,From Paraphrase Database to Compositional Paraphrase Model and Back,2015,"The Paraphrase Database (PPDB; Ganitkevitch et al., 2013) is an extensive semantic resource, consisting of a list phrase pairs with (heuristic) confidence estimates. However, it still unclear how can best be used, due to the heuristic nature confidences and its necessarily incomplete coverage. We propose models leverage from PPDB build parametric paraphrase that score more accurately than PPDB’s internal scores while simultaneously improving They allow for learning embeddings as well improved word embeddings. Moreover, we introduce two new, manually annotated datasets evaluate short-phrase paraphrasing models. Using our model trained using PPDB, achieve state-of-the-art results on standard bigram similarity tasks beat strong baselines new short tasks."
https://openalex.org/W2283041611,https://doi.org/10.1093/jamia/ocv180,Extracting information from the text of electronic medical records to improve case detection: a systematic review,2016,"Abstract Background Electronic medical records (EMRs) are revolutionizing health-related research. One key issue for study quality is the accurate identification of patients with condition interest. Information in EMRs can be entered as structured codes or unstructured free text. The majority research studies have used only coded parts case-detection, which may bias findings, miss cases, and reduce quality. This review examines whether incorporating information from text into case-detection algorithms improve Methods A systematic search returned 9659 papers, 67 reported on extraction stated purpose detecting cases a named clinical condition. extracting technical accuracy were reviewed. Results Studies mainly US hospital-based EMRs, extracted 41 conditions using keyword searches, rule-based algorithms, machine learning methods. There was no clear difference algorithm between methods extraction. Inclusion resulted significant improvement sensitivity area under receiver operating characteristic comparison to alone (median 78% (codes + text) vs 62% (codes), P = .03; median 95% 88% .025). Conclusions Text accessible, especially open source significantly improves case detection when combined codes. More harmonization reporting within EMR needed, particularly standardized metrics like positive predictive value (precision) (recall)."
https://openalex.org/W2343649478,https://doi.org/10.1162/coli_a_00295,Parsing Argumentation Structures in Persuasive Essays,2017,"In this article, we present a novel approach for parsing argumentation structures. We identify argument components using sequence labeling at the token level and apply new joint model detecting The proposed globally optimizes component types argumentative relations Integer Linear Programming. show that our significantly outperforms challenging heuristic baselines on two different of discourse. Moreover, introduce corpus persuasive essays annotated with annotation scheme guidelines successfully guide human annotators to substantial agreement."
https://openalex.org/W2803098682,https://doi.org/10.1145/3129340,Speech emotion recognition,2018,Tracing 20 years of progress in making machines hear our emotions based on speech signal properties.
https://openalex.org/W2963855739,https://doi.org/10.18653/v1/k16-1025,Joint Learning of the Embedding of Words and Entities for Named Entity Disambiguation,2016,"Named Entity Disambiguation (NED) refers to the task of resolving multiple named entity mentions in a document their correct references knowledge base (KB) (e.g., Wikipedia). In this paper, we propose novel embedding method specifically designed for NED. The proposed jointly maps words and entities into same continuous vector space. We extend skip-gram model by using two models. KB graph learns relatedness link structure KB, whereas anchor context aims align vectors such that similar occur close one another space leveraging anchors words. By combining contexts based on with standard NED features, achieved state-of-the-art accuracy 93.1% CoNLL dataset 85.2% TAC 2010 dataset."
https://openalex.org/W2964325543,https://doi.org/10.18653/v1/d16-1057,Inducing Domain-Specific Sentiment Lexicons from Unlabeled Corpora,2016,"A word's sentiment depends on the domain in which it is used. Computational social science research thus requires lexicons that are specific to domains being studied. We combine domain-specific word embeddings with a label propagation framework induce accurate using small sets of seed words. show our approach achieves state-of-the-art performance inducing from corpora and purely corpus-based outperforms methods rely hand-curated resources (e.g., WordNet). Using framework, we release historical for 150 years English community-specific 250 online communities media forum Reddit. The more than 5% sentiment-bearing (non-neutral) words completely switched polarity during last years, highlight how varies drastically between different communities."
https://openalex.org/W2919290281,https://doi.org/10.18653/v1/n19-1388,Massively Multilingual Neural Machine Translation,2019,"Multilingual Neural Machine Translation enables training a single model that supports translation from multiple source languages into target languages. We perform extensive experiments in massively multilingual NMT models, involving up to 103 distinct and 204 directions simultaneously. explore different setups for such models analyze the trade-offs between quality various modeling decisions. report results on publicly available TED talks corpus where we show many-to-many are effective low resource settings, outperforming previous state-of-the-art while supporting 59 116 model. Our large-scale dataset with languages, trained one million examples per direction also promising results, surpassing strong bilingual baselines encouraging future work NMT."
https://openalex.org/W2963351113,https://doi.org/10.1109/cvpr.2018.00911,End-to-End Dense Video Captioning with Masked Transformer,2018,"Dense video captioning aims to generate text descriptions for all events in an untrimmed video. This involves both detecting and describing events. Therefore, previous methods on dense tackle this problem by building two models, i.e. event proposal a model, these sub-problems. The models are either trained separately or alternation. prevents direct influence of the language description proposal, which is important generating accurate descriptions. To address problem, we propose end-to-end transformer model captioning. encoder encodes into appropriate representations. decoder decodes from encoding with different anchors form proposals. employs masking network restrict its attention over feature. converts differentiable mask, ensures consistency between during training. In addition, our self-attention mechanism, enables use efficient non-recurrent structure leads performance improvements. We demonstrate effectiveness ActivityNet Captions YouCookII datasets, where achieved 10.12 6.58 METEOR score, respectively."
https://openalex.org/W2963324947,https://doi.org/10.18653/v1/p16-1100,Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models,2016,"Nearly all previous work on neural machine translation (NMT) has used quite restricted vocabularies, perhaps with a subsequent method to patch in unknown words. This paper presents novel word-character solution achieving open vocabulary NMT. We build hybrid systems that translate mostly at the word level and consult character components for rare Our character-level recurrent networks compute source representations recover target words when needed. The twofold advantage of such approach is it much faster easier train than character-based ones; same time, never produces as case word-based models. On WMT'15 English Czech task, this offers an addition boost +2.1-11.4 BLEU points over models already handle best system achieves new state-of-the-art result 20.7 score. demonstrate our can successfully learn not only generate well-formed Czech, highly-inflected language very complex vocabulary, but also correct"
https://openalex.org/W2986154550,https://doi.org/10.18653/v1/2020.acl-main.645,CamemBERT: a Tasty French Language Model,2019,"Pretrained language models are now ubiquitous in Natural Language Processing. Despite their success, most available have either been trained on English data or the concatenation of multiple languages. This makes practical use such --in all languages except English-- very limited. In this paper, we investigate feasibility training monolingual Transformer-based for other languages, taking French as an example and evaluating our part-of-speech tagging, dependency parsing, named entity recognition natural inference tasks. We show that web crawled is preferable to Wikipedia data. More surprisingly, a relatively small dataset (4GB) leads results good those obtained using larger datasets (130+GB). Our best performing model CamemBERT reaches improves state art four downstream"
https://openalex.org/W2963467339,https://doi.org/10.1109/cvpr.2018.00750,"Look, Imagine and Match: Improving Textual-Visual Cross-Modal Retrieval with Generative Models",2018,"Textual-visual cross-modal retrieval has been a hot research topic in both computer vision and natural language processing communities. Learning appropriate representations for multi-modal data is crucial the performance. Unlike existing image-text approaches that embed pairs as single feature vectors common representational space, we propose to incorporate generative processes into embedding, through which are able learn not only global abstract features but also local grounded features. Extensive experiments show our framework can well match images sentences with complex content, achieve state-of-the-art results on MSCOCO dataset."
https://openalex.org/W3106224367,https://doi.org/10.18653/v1/w19-5034,ScispaCy: Fast and Robust Models for Biomedical Natural Language Processing,2019,"Despite recent advances in natural language processing, many statistical models for processing text perform extremely poorly under domain shift. Processing biomedical and clinical is a critically important application area of which there are few robust, practical, publicly available models. This paper describes scispaCy, new tool practical biomedical/scientific heavily leverages the spaCy library. We detail performance two packages released scispaCy demonstrate their robustness on several tasks datasets. Models code at https://allenai.github.io/scispacy/"
https://openalex.org/W1916445035,https://doi.org/10.1109/iccv.2015.301,Multimodal Convolutional Neural Networks for Matching Image and Sentence,2015,"In this paper, we propose multimodal convolutional neural networks (m-CNNs) for matching image and sentence. Our m-CNN provides an end-to-end framework with architectures to exploit representation, word composition, the relations between two modalities. More specifically, it consists of one CNN encoding content, learning joint representation The composes words different semantic fragments learns inter-modal composed at levels, thus fully Experimental results on benchmark databases bidirectional sentence retrieval demonstrate that proposed m-CNNs can effectively capture information necessary matching. Specifically, our Flickr30K Microsoft COCO achieve state-of-the-art performances."
https://openalex.org/W2606089314,https://doi.org/10.1007/978-3-031-02165-7,Neural Network Methods for Natural Language Processing,2017,"Neural networks are a family of powerful machine learning models. This book focuses on the application neural network models to natural language data. The first half (Parts I and II) covers basics supervised feed-forward networks, working with over data, use vector-based rather than symbolic representations for words. It also computation-graph abstraction, which allows easily define train arbitrary is basis behind design contemporary software libraries. second part III IV) introduces more specialized architectures, including 1D convolutional recurrent conditioned-generation models, attention-based These architectures techniques driving force state-of-the-art algorithms translation, syntactic parsing, many other applications. Finally, we discuss tree-shaped structured prediction, prospects multi-task learning."
https://openalex.org/W2251654079,https://doi.org/10.18653/v1/d15-1042,Sentence Compression by Deletion with LSTMs,2015,"We present an LSTM approach to deletion-based sentence compression where the task is translate a into sequence of zeros and ones, corresponding token deletion decisions. demonstrate that even most basic version system, which given no syntactic information (no PoS or NE tags, dependencies) desired length, performs surprisingly well: around 30% compressions from large test set could be regenerated. compare system with competitive baseline trained on same amount data but additionally provided all kinds linguistic features. In experiment human raters LSTMbased model outperforms achieving 4.5 in readability 3.8 informativeness."
https://openalex.org/W2556388456,https://doi.org/10.1109/cvpr.2017.111,Video Captioning with Transferred Semantic Attributes,2017,"Automatically generating natural language descriptions of videos plays a fundamental challenge for computer vision community. Most recent progress in this problem has been achieved through employing 2-D and/or 3-D Convolutional Neural Networks (CNNs) to encode video content and Recurrent (RNNs) decode sentence. In paper, we present Long Short-Term Memory with Transferred Semantic Attributes (LSTM-TSA)&#x2014;a novel deep architecture that incorporates the transferred semantic attributes learnt from images into CNN plus RNN framework, by training them an end-to-end manner. The design LSTM-TSA is highly inspired facts 1) play significant contribution captioning, 2) carry complementary semantics thus can reinforce each other captioning. To boost propose transfer unit model mutually correlated videos. Extensive experiments are conducted on three public datasets, i.e., MSVD, M-VAD MPII-MD. Our proposed achieves to-date best published performance sentence generation MSVD: 52.8% 74.0% terms BLEU@4 CIDEr-D. Superior results also reported MPII-MD when compared state-of-the-art methods."
https://openalex.org/W2964085268,https://doi.org/10.18653/v1/n18-2084,When and Why Are Pre-Trained Word Embeddings Useful for Neural Machine Translation?,2018,"The performance of Neural Machine Translation (NMT) systems often suffers in low-resource scenarios where sufficiently large-scale parallel corpora cannot be obtained. Pre-trained word embeddings have proven to invaluable for improving natural language analysis tasks, which suffer from paucity data. However, their utility NMT has not been extensively explored. In this work, we perform five sets experiments that analyze when can expect pre-trained help tasks. We show such surprisingly effective some cases – providing gains up 20 BLEU points the most favorable setting."
https://openalex.org/W2521709538,https://doi.org/10.1145/3097983.3098177,ReasoNet,2017,"Teaching a computer to read and answer general questions pertaining document is challenging yet unsolved problem. In this paper, we describe novel neural network architecture called the Reasoning Network (ReasoNet) for machine comprehension tasks. ReasoNets make use of multiple turns effectively exploit then reason over relation among queries, documents, answers. Different from previous approaches using fixed number during inference, introduce termination state relax constraint on reasoning depth. With reinforcement learning, can dynamically determine whether continue process after digesting intermediate results, or terminate reading when it concludes that existing information adequate produce an answer. have achieved exceptional performance in datasets, including unstructured CNN Daily Mail Stanford SQuAD dataset, structured Graph Reachability dataset."
https://openalex.org/W2798456655,https://doi.org/10.18653/v1/p18-1103,Multi-Turn Response Selection for Chatbots with Deep Attention Matching Network,2018,"Human generates responses relying on semantic and functional dependencies, including coreference relation, among dialogue elements their context. In this paper, we investigate matching a response with its multi-turn context using dependency information based entirely attention. Our solution is inspired by the recently proposed Transformer in machine translation (Vaswani et al., 2017) extend attention mechanism two ways. First, construct representations of text segments at different granularities solely stacked self-attention. Second, try to extract truly matched segment pairs across response. We jointly introduce those kinds one uniform neural network. Experiments large-scale selection tasks show that our model significantly outperforms state-of-the-art models."
https://openalex.org/W2798914047,https://doi.org/10.18653/v1/p18-1133,Sequicity: Simplifying Task-oriented Dialogue Systems with Single Sequence-to-Sequence Architectures,2018,"Existing solutions to task-oriented dialogue systems follow pipeline designs which introduces architectural complexity and fragility. We propose a novel, holistic, extendable framework based on single sequence-to-sequence (seq2seq) model can be optimized with supervised or reinforcement learning. A key contribution is that we design text spans named belief track believes, allowing modeled in seq2seq way. Based this, simplistic Two Stage CopyNet instantiation emonstrates good scalability: significantly reducing terms of number parameters training time by magnitude. It outperforms state-of-the-art pipeline-based methods large datasets retains satisfactory entity match rate out-of-vocabulary (OOV) cases where pipeline-designed competitors totally fail."
https://openalex.org/W2891896107,https://doi.org/10.18653/v1/d18-1330,Loss in Translation: Learning Bilingual Word Mapping with a Retrieval Criterion,2018,"Continuous word representations learned separately on distinct languages can be aligned so that their words become comparable in a common space. Existing works typically solve quadratic problem to learn orthogonal matrix aligning bilingual lexicon, and use retrieval criterion for inference. In this paper, we propose an unified formulation directly optimizes end-to-end fashion. Our experiments standard benchmarks show our approach outperforms the state of art translation, with biggest improvements observed distant language pairs such as English-Chinese."
https://openalex.org/W2896807716,https://doi.org/10.18653/v1/p18-1061,Neural Document Summarization by Jointly Learning to Score and Select Sentences,2018,"Sentence scoring and sentence selection are two main steps in extractive document summarization systems. However, previous works treat them as separated subtasks. In this paper, we present a novel end-to-end neural network framework for by jointly learning to score select sentences. It first reads the sentences with hierarchical encoder obtain representation of Then it builds output summary extracting one one. Different from methods, our approach integrates strategy into model, which directly predicts relative importance given previously selected Experiments on CNN/Daily Mail dataset show that proposed significantly outperforms state-of-the-art models."
https://openalex.org/W2962728167,https://doi.org/10.18653/v1/p17-1105,Abstract Syntax Networks for Code Generation and Semantic Parsing,2017,"Tasks like code generation and semantic parsing require mapping unstructured (or partially structured) inputs to well-formed, executable outputs. We introduce abstract syntax networks, a modeling framework for these problems. The outputs are represented as trees (ASTs) constructed by decoder with dynamically-determined modular structure paralleling the of output tree. On benchmark Hearthstone dataset generation, our model obtains 79.2 BLEU 22.7% exact match accuracy, compared previous state-of-the-art values 67.1 6.1%. Furthermore, we perform competitively on Atis, Jobs, Geo datasets no task-specific engineering."
https://openalex.org/W2963018920,https://doi.org/10.1162/tacl_a_00030,Generating Sentences by Editing Prototypes,2018,"We propose a new generative model of sentences that first samples prototype sentence from the training corpus and then edits it into sentence. Compared to traditional models generate scratch either left-to-right or by sampling latent vector, our prototype-then-edit improves perplexity on language modeling generates higher quality outputs according human evaluation. Furthermore, gives rise edit vector captures interpretable semantics such as similarity sentence-level analogies."
https://openalex.org/W2740132093,https://doi.org/10.18653/v1/p17-1179,Adversarial Training for Unsupervised Bilingual Lexicon Induction,2017,"Word embeddings are well known to capture linguistic regularities of the language on which they trained. Researchers also observe that these can transfer across languages. However, previous endeavors connect separate monolingual word typically require cross-lingual signals as supervision, either in form parallel corpus or seed lexicon. In this work, we show such connection actually be established without any supervision. We achieve end by formulating problem a natural adversarial game, and investigating techniques crucial successful training. carry out evaluation unsupervised bilingual lexicon induction task. Even though task appears intrinsically cross-lingual, able demonstrate encouraging performance clues."
https://openalex.org/W2963923670,https://doi.org/10.1016/j.jbi.2018.09.008,A comparison of word embeddings for the biomedical natural language processing,2018,"• Word embeddings trained from clinical notes, literature, Wikipedia, and news are compared. notes literature capture word semantics better. There isn’t a consistent global ranking of for biomedical NLP applications. domain corpora do not necessarily perform have been prevalently used in Natural Language Processing (NLP) applications due to the ability vector representations being able useful semantic properties linguistic relationships between words. Different textual resources (e.g., Wikipedia corpus) utilized train these commonly leveraged as feature input downstream machine learning models. However, there has little work on evaluating different resources. In this study, we empirically evaluated four corpora, namely publications, news. For former two resources, using unstructured electronic health record (EHR) data available at Mayo Clinic articles (MedLit) PubMed Central, respectively. latter publicly pre-trained embeddings, GloVe Google News. The evaluation was done qualitatively quantitatively. qualitative evaluation, randomly selected medical terms three categories (i.e., disorder, symptom, drug), manually inspected five most similar words computed by each term. We also analyzed through 2-dimensional visualization plot 377 terms. quantitative conducted both intrinsic extrinsic evaluation. embeddings’ measruing similarity published datasets: Pedersen’s dataset, Hliaoutakis’s MayoSRS, UMNSRS. applied multiple applications, including information extraction (IE), retrieval (IR), relation (RE), with shared tasks. shows that EHR MedLit can find more than those verifies captured is closer human experts’ judgments all tested datasets. achieved best F1 score 0.900 IE task; no improved performance IR News had overall 0.790 RE task. Based results, draw following conclusions. First, better, semantically relevant Second, does exist adding extra features will improve results Finally, better general any"
https://openalex.org/W2964007535,https://doi.org/10.18653/v1/d16-1026,Zero-Resource Translation with Multi-Lingual Neural Machine Translation,2016,"In this paper, we propose a novel finetuning algorithm for the recently introduced multi-way, mulitlingual neural machine translate that enables zero-resource translation. When used together with many-to-one translation strategies, empirically show allows multilingual model to language pair (1) as well single-pair trained up 1M direct parallel sentences of same and (2) better than pivot-based strategy, while keeping only one additional copy attention-related parameters."
https://openalex.org/W2998385486,https://doi.org/10.1609/aaai.v34i03.5681,K-BERT: Enabling Language Representation with Knowledge Graph,2020,"Pre-trained language representation models, such as BERT, capture a general from large-scale corpora, but lack domain-specific knowledge. When reading domain text, experts make inferences with relevant For machines to achieve this capability, we propose knowledge-enabled model (K-BERT) knowledge graphs (KGs), in which triples are injected into the sentences However, too much incorporation may divert sentence its correct meaning, is called noise (KN) issue. To overcome KN, K-BERT introduces soft-position and visible matrix limit impact of can easily inject models by equipped KG without pre-training by-self because it capable loading parameters pre-trained BERT. Our investigation reveals promising results twelve NLP tasks. Especially tasks (including finance, law, medicine), significantly outperforms demonstrates that an excellent choice for solving knowledge-driven problems require experts."
https://openalex.org/W3122309589,https://doi.org/10.1016/j.jacceco.2017.07.002,The evolution of 10-K textual disclosure: Evidence from Latent Dirichlet Allocation,2017,"Abstract We document marked trends in 10-K disclosure over the period 1996–2013, with increases length, boilerplate, stickiness, and redundancy decreases specificity, readability, relative amount of hard information. use Latent Dirichlet Allocation (LDA) to examine specific topics find that new FASB SEC requirements explain most increase length 3 150 topics—fair value, internal controls, risk factor disclosures—account for virtually all increase. These three disclosures also play a major role explaining remaining textual characteristics."
https://openalex.org/W2220981600,https://doi.org/10.1109/iccv.2015.277,Guiding the Long-Short Term Memory Model for Image Caption Generation,2015,"In this work we focus on the problem of image caption generation. We propose an extension long short term memory (LSTM) model, which coin gLSTM for short. particular, add semantic information extracted from as extra input to each unit LSTM block, with aim guiding model towards solutions that are more tightly coupled content. Additionally, explore different length normalization strategies beam search avoid bias sentences. On various benchmark datasets such Flickr8K, Flickr30K and MS COCO, obtain results par or better than current state-of-the-art."
https://openalex.org/W2563399268,https://doi.org/10.1007/s11263-016-0966-6,VQA: Visual Question Answering,2017,"We propose the task of free-form and open-ended Visual Question Answering (VQA). Given an image a natural language question about image, is to provide accurate answer. Mirroring real-world scenarios, such as helping visually impaired, both questions answers are open-ended. selectively target different areas including background details underlying context. As result, system that succeeds at VQA typically needs more detailed understanding complex reasoning than producing generic captions. Moreover, amenable automatic evaluation, since many contain only few words or closed set can be provided in multiple-choice format. dataset containing $$\sim $$~0.25 M images, $$~0.76 questions, $$~10 (www.visualqa.org), discuss information it provides. Numerous baselines methods for compared with human performance. Our demo available on CloudCV (http://cloudcv.org/vqa)."
https://openalex.org/W2114719613,https://doi.org/10.1073/pnas.1502134112,Large-scale evidence of dependency length minimization in 37 languages,2015,"Significance We provide the first large-scale, quantitative, cross-linguistic evidence for a universal syntactic property of languages: that dependency lengths are shorter than chance. Our work supports long-standing ideas speakers prefer word orders with short and languages do not enforce long lengths. Dependency length minimization is well motivated because it allows more efficient parsing generation natural language. Over last 20 y, hypothesis pressure to minimize has been invoked explain many most striking recurring properties languages. broad-coverage findings support those explanations."
https://openalex.org/W2309415944,https://doi.org/10.1007/978-3-319-46448-0_8,Semantic Object Parsing with Graph LSTM,2016,"By taking the semantic object parsing task as an exemplar application scenario, we propose Graph Long Short-Term Memory (Graph LSTM) network, which is generalization of LSTM from sequential data or multi-dimensional to general graph-structured data. Particularly, instead evenly and fixedly dividing image pixels patches in existing structures (e.g., Row, Grid Diagonal LSTMs), take each arbitrary-shaped superpixel a semantically consistent node, adaptively construct undirected graph for image, where spatial relations superpixels are naturally used edges. Constructed on such adaptive topology, more aligned with visual patterns boundaries appearance similarities) provides economical information propagation route. Furthermore, optimization step over LSTM, use confidence-driven scheme update hidden memory states nodes progressively till all updated. In addition, forgets gates learned capture different degrees correlation neighboring nodes. Comprehensive evaluations four diverse datasets well demonstrate significant superiority our other state-of-the-art solutions."
https://openalex.org/W2964034111,https://doi.org/10.18653/v1/n16-1004,Multi-Source Neural Translation,2016,"We build a multi-source machine translation model and train it to maximize the probability of target English string given French German sources. Using neural encoder-decoder framework, we explore several combination methods report up +4.8 Bleu increases on top very strong attention-based model."
https://openalex.org/W2964212550,https://doi.org/10.18653/v1/p18-1042,ParaNMT-50M: Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations,2018,"We describe ParaNMT-50M, a dataset of more than 50 million English-English sentential paraphrase pairs. generated the pairs automatically by using neural machine translation to translate non-English side large parallel corpus, following Wieting et al. (2017). Our hope is that ParaNMT-50M can be valuable resource for generation and provide rich source semantic knowledge improve downstream natural language understanding tasks. To show its utility, we use train paraphrastic sentence embeddings outperform all supervised systems on every SemEval textual similarity competition, in addition showing how it used generation."
https://openalex.org/W1860935423,https://doi.org/10.18653/v1/d15-1041,Improved Transition-based Parsing by Modeling Characters instead of Words with LSTMs,2015,"We present extensions to a continuousstate dependency parsing method that makes it applicable morphologically rich languages. Starting with highperformance transition-based parser uses long short-term memory (LSTM) recurrent neural networks learn representations of the state, we replace lookup-based word constructed from orthographic words, also using LSTMs. This allows statistical sharing across forms are similar on surface. Experiments for languages show model benefits incorporating character-based encodings words."
https://openalex.org/W2963890019,https://doi.org/10.1109/cvpr.2016.542,Yin and Yang: Balancing and Answering Binary Visual Questions,2016,"The complex compositional structure of language makes problems at the intersection vision and challenging. But also provides a strong prior that can result in good superficial performance, without underlying models truly understanding visual content. This hinder progress pushing state art computer aspects multi-modal AI. In this paper, we address binary Visual Question Answering (VQA) on abstract scenes. We formulate problem as verification concepts inquired questions. Specifically, convert question to tuple concisely summarizes concept be detected image. If found image, answer is ""yes"", otherwise ""no"". Abstract scenes play two roles (1) They allow us focus high-level semantics VQA task opposed low-level recognition problems, perhaps more importantly, (2) provide modality balance dataset such priors are controlled, role essential. particular, collect fine-grained pairs for every question, ""yes"" one scene, ""no"" other exact same question. Indeed, alone do not perform better than chance our balanced dataset. Moreover, proposed approach matches performance state-of-the-art unbalanced dataset, outperforms it"
https://openalex.org/W2551361256,https://doi.org/10.24963/ijcai.2017/209,Multilingual Knowledge Graph Embeddings for Cross-lingual Knowledge Alignment,2017,"Many recent works have demonstrated the benefits of knowledge graph embeddings in completing monolingual graphs. Inasmuch as related bases are built several different languages, achieving cross-lingual alignment will help people constructing a coherent base, and assist machines dealing with expressions entity relationships across diverse human languages. Unfortunately, this highly desirable by labor is very costly error-prone. Thus, we propose MTransE, translation-based model for multilingual embeddings, to provide simple automated solution. By encoding entities relations each language separated embedding space, MTransE provides transitions vector its counterparts other spaces, while preserving functionalities embeddings. We deploy three techniques represent transitions, namely axis calibration, translation vectors, linear transformations, derive five variants using loss functions. Our models can be trained on partially aligned graphs, where just small portion triples their counterparts. The experiments matching triple-wise verification show promising results, some consistently outperforming others tasks. also explore how preserves key properties counterpart."
https://openalex.org/W2594229957,https://doi.org/10.18653/v1/e17-3017,Nematus: a Toolkit for Neural Machine Translation,2017,"We present Nematus, a toolkit for Neural Machine Translation. The prioritizes high translation accuracy, usability, and extensibility. Nematus has been used to build top-performing submissions shared tasks at WMT IWSLT, train systems production environments."
https://openalex.org/W2741065173,https://doi.org/10.18653/v1/w17-3013,Using Convolutional Neural Networks to Classify Hate-Speech,2017,"The paper introduces a deep learning-based Twitter hate-speech text classification system. classifier assigns each tweet to one of four predefined categories: racism, sexism, both (racism and sexism) non-hate-speech. Four Convolutional Neural Network models were trained on resp. character 4-grams, word vectors based semantic information built using word2vec, randomly generated vectors, combined with n-grams. feature set was down-sized in the networks by max-pooling, softmax function used classify tweets. Tested 10-fold cross-validation, model word2vec embeddings performed best, higher precision than recall, 78.3% F-score."
https://openalex.org/W2952768212,https://doi.org/10.18653/v1/p19-1024,Attention Guided Graph Convolutional Networks for Relation Extraction,2019,"Dependency trees convey rich structural information that is proven useful for extracting relations among entities in text. However, how to effectively make use of relevant while ignoring irrelevant from the dependency remains a challenging research question. Existing approaches employing rule based hard-pruning strategies selecting partial structures may not always yield optimal results. In this work, we propose Attention Guided Graph Convolutional Networks (AGGCNs), novel model which directly takes full as inputs. Our can be understood soft-pruning approach automatically learns selectively attend sub-structures relation extraction task. Extensive results on various tasks including cross-sentence n-ary and large-scale sentence-level show our able better leverage trees, giving significantly than previous approaches."
https://openalex.org/W2964271186,https://doi.org/10.18653/v1/p17-1089,Learning a Neural Semantic Parser from User Feedback,2017,"We present an approach to rapidly and easily build natural language interfaces databases for new domains, whose performance improves over time based on user feedback, requires minimal intervention. To achieve this, we adapt neural sequence models map utterances directly SQL with its full expressivity, bypassing any intermediate meaning representations. These are immediately deployed online solicit feedback from real users flag incorrect queries. Finally, the popularity of facilitates gathering annotations predictions using crowd, which is used improve our models. This complete loop, without representations or database specific engineering, opens up ways building high quality semantic parsers. Experiments suggest that this can be quickly target domain, as show by learning a parser academic scratch."
https://openalex.org/W2560920409,https://doi.org/10.1109/tpami.2017.2708709,Image Captioning and Visual Question Answering Based on Attributes and External Knowledge,2018,"Much recent progress in Vision-to-Language problems has been achieved through a combination of Convolutional Neural Networks (CNNs) and Recurrent (RNNs). This approach does not explicitly represent high-level semantic concepts, but rather seeks to directly from image features text. In this paper we first propose method incorporating concepts into the successful CNN-RNN approach, show that it achieves significant improvement on state-of-the-art both captioning visual question answering. We further same mechanism can be used incorporate external knowledge, which is critically important for answering high level questions. Specifically, design model combines an internal representation content with information extracted general knowledge base answer broad range image-based It particularly allows questions asked about contents image, even when itself contain complete answer. Our final best reported results several benchmark datasets."
https://openalex.org/W3115903740,https://doi.org/10.18653/v1/2020.semeval-1.188,SemEval-2020 Task 12: Multilingual Offensive Language Identification in Social Media (OffensEval 2020),2020,"We present the results and main findings of SemEval-2020 Task 12 on Multilingual Offensive Language Identification in Social Media (OffensEval-2020). The task included three subtasks corresponding to hierarchical taxonomy OLID schema from OffensEval-2019, it was offered five languages: Arabic, Danish, English, Greek, Turkish. OffensEval-2020 one most popular tasks at SemEval-2020, attracting a large number participants across all total 528 teams signed up participate task, 145 submitted official runs test data, 70 system description papers."
https://openalex.org/W2889624842,https://doi.org/10.18653/v1/d18-1521,Learning Gender-Neutral Word Embeddings,2018,"Word embedding models have become a fundamental component in wide range of Natural Language Processing (NLP) applications. However, embeddings trained on human-generated corpora been demonstrated to inherit strong gender stereotypes that reflect social constructs. To address this concern, paper, we propose novel training procedure for learning gender-neutral word embeddings. Our approach aims preserve information certain dimensions vectors while compelling other be free influence. Based the proposed method, generate Gender-Neutral variant GloVe (GN-GloVe). Quantitative and qualitative experiments demonstrate GN-GloVe successfully isolates without sacrificing functionality model."
https://openalex.org/W2963542740,https://doi.org/10.18653/v1/p19-1176,Learning Deep Transformer Models for Machine Translation,2019,"Transformer is the state-of-the-art model in recent machine translation evaluations. Two strands of research are promising to improve models this kind: first uses wide networks (a.k.a. Transformer-Big) and has been de facto standard for development system, other deeper language representation but faces difficulty arising from learning deep networks. Here, we continue line on latter. We claim that a truly can surpass Transformer-Big counterpart by 1) proper use layer normalization 2) novel way passing combination previous layers next. On WMT'16 English- German, NIST OpenMT'12 Chinese-English larger WMT'18 tasks, our system (30/25-layer encoder) outperforms shallow Transformer-Big/Base baseline (6-layer 0.4-2.4 BLEU points. As another bonus, 1.6X smaller size 3X faster training than Transformer-Big."
https://openalex.org/W2996851481,https://doi.org/10.1609/aaai.v34i05.6311,Is BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment,2020,"Machine learning algorithms are often vulnerable to adversarial examples that have imperceptible alterations from the original counterparts but can fool state-of-the-art models. It is helpful evaluate or even improve robustness of these models by exposing maliciously crafted examples. In this paper, we present TextFooler, a simple strong baseline generate natural text. By applying it two fundamental language tasks, text classification and textual entailment, successfully attacked three target models, including powerful pre-trained BERT, widely used convolutional recurrent neural networks. We demonstrate advantages framework in ways: (1) effective---it outperforms attacks terms success rate perturbation rate, (2) utility-preserving---it preserves semantic content grammaticality, remains correctly classified humans, (3) efficient---it generates with computational complexity linear length. *The code, test available at https://github.com/jind11/TextFooler."
https://openalex.org/W2555745756,https://doi.org/10.48550/arxiv.1611.04798,"Toward Multilingual Neural Machine Translation with Universal Encoder
  and Decoder",2016,"In this paper, we present our first attempts in building a multilingual Neural Machine Translation framework under unified approach. We are then able to employ attention-based NMT for many-to-many translation tasks. Our approach does not require any special treatment on the network architecture and it allows us learn minimal number of free parameters standard way training. has shown its effectiveness an under-resourced scenario with considerable improvements up 2.6 BLEU points. addition, achieved interesting promising results when applied task that there is no direct parallel corpus between source target languages."
https://openalex.org/W2945475330,https://doi.org/10.18653/v1/p19-1078,Transferable Multi-Domain State Generator for Task-Oriented Dialogue Systems,2019,"Over-dependence on domain ontology and lack of knowledge sharing across domains are two practical yet less studied problems dialogue state tracking. Existing approaches generally fall short in tracking unknown slot values during inference often have difficulties adapting to new domains. In this paper, we propose a Transferable Dialogue State Generator (TRADE) that generates states from utterances using copy mechanism, facilitating transfer when predicting (domain, slot, value) triplets not encountered training. Our model is composed an utterance encoder, gate, generator, which shared Empirical results demonstrate TRADE achieves state-of-the-art joint goal accuracy 48.62% for the five MultiWOZ, human-human dataset. addition, show its transferring ability by simulating zero-shot few-shot unseen 60.58% one domains, able adapt cases without forgetting already trained"
https://openalex.org/W2963729324,https://doi.org/10.1162/tacl_a_00039,Adversarial Deep Averaging Networks for Cross-Lingual Sentiment Classification,2018,"In recent years great success has been achieved in sentiment classification for English, thanks part to the availability of copious annotated resources. Unfortunately, most languages do not enjoy such an abundance labeled data. To tackle problem low-resource without adequate data, we propose Adversarial Deep Averaging Network (ADAN 1 ) transfer knowledge learned from data on a resource-rich source language where only unlabeled exist. ADAN two discriminative branches: classifier and adversarial discriminator. Both branches take input shared feature extractor learn hidden representations that are simultaneously indicative task invariant across languages. Experiments Chinese Arabic demonstrate significantly outperforms state-of-the-art systems."
https://openalex.org/W2067477726,https://doi.org/10.1016/j.tics.2014.12.008,Neurobiological roots of language in primate audition: common computational properties,2015,"Here, we present a new perspective on an old question: how does the neurobiology of human language relate to brain systems in nonhuman primates? We argue that higher-order combinatorics, including sentence and discourse processing, can be situated unified, cross-species dorsal-ventral streams architecture for higher auditory functions dorsal ventral processing grounded their respective computational properties primate audition. This view challenges assumption, common cognitive sciences, model forms inherently inadequate basis modeling higher-level functions."
https://openalex.org/W2963344337,https://doi.org/10.18653/v1/p17-1168,Gated-Attention Readers for Text Comprehension,2017,"In this paper we study the problem of answering cloze-style questions over documents. Our model, Gated-Attention (GA) Reader, integrates a multi-hop architecture with novel attention mechanism, which is based on multiplicative interactions between query embedding and intermediate states recurrent neural network document reader. This enables reader to build query-specific representations tokens in for accurate answer selection. The GA Reader obtains state-of-the-art results three benchmarks task–the CNN & Daily Mail news stories Who Did What dataset. effectiveness interaction demonstrated by an ablation study, comparing alternative compositional operators implementing gated-attention."
https://openalex.org/W2963349562,https://doi.org/10.1007/978-3-030-01219-9_47,Women Also Snowboard: Overcoming Bias in Captioning Models,2018,"Most machine learning methods are known to capture and exploit biases of the training data. While some beneficial for learning, others harmful. Specifically, image captioning models tend exaggerate present in data (e.g., if a word is 60% sentences, it might be predicted 70% sentences at test time). This can lead incorrect captions domains where unbiased desired, or required, due over-reliance on learned prior context. In this work we investigate generation gender-specific caption words (e.g. man, woman) based person’s appearance We introduce new Equalizer model that encourages equal gender probability when evidence occluded scene confident predictions present. The resulting forced look person rather than use contextual cues make prediction. losses comprise our model, Appearance Confusion Loss Confident Loss, general, added any description order mitigate impacts unwanted bias dataset. Our proposed has lower error describing images with people mentioning their more closely matches ground truth ratio including women men. Finally, show often looks predicting (https://people.eecs.berkeley.edu/~lisa anne/snowboard.html)."
https://openalex.org/W2964300796,https://doi.org/10.1609/aaai.v33i01.33016818,DialogueRNN: An Attentive RNN for Emotion Detection in Conversations,2019,"Emotion detection in conversations is a necessary step for number of applications, including opinion mining over chat history, social media threads, debates, argumentation mining, understanding consumer feedback live conversations, and so on. Currently systems do not treat the parties conversation individually by adapting to speaker each utterance. In this paper, we describe new method based on recurrent neural networks that keeps track individual party states throughout uses information emotion classification. Our model outperforms state-of-the-art significant margin two different datasets."
https://openalex.org/W2022792389,https://doi.org/10.1073/pnas.1411678112,Human language reveals a universal positivity bias,2015,"Significance The most commonly used words of 24 corpora across 10 diverse human languages exhibit a clear positive bias, big data confirmation the Pollyanna hypothesis. study’s findings are based on 5 million individual scores and pave way for development powerful language-based tools measuring emotion."
https://openalex.org/W2401379394,https://doi.org/10.1016/j.jksues.2016.04.002,A survey on sentiment analysis challenges,2016,"Abstract With accelerated evolution of the internet as websites, social networks, blogs, online portals, reviews, opinions, recommendations, ratings, and feedback are generated by writers. This writer sentiment content can be about books, people, hotels, products, research, events, etc. These sentiments become very beneficial for businesses, governments, individuals. While this is meant to useful, a bulk require using text mining techniques analysis. But there several challenges facing analysis evaluation process. obstacles in analyzing accurate meaning detecting suitable polarity. Sentiment practice applying natural language processing identify extract subjective information from text. paper presents survey on relevant their approaches techniques."
https://openalex.org/W2414378847,https://doi.org/10.1093/bioinformatics/btw343,TaggerOne: joint named entity recognition and normalization with semi-Markov Models,2016,"Text mining is increasingly used to manage the accelerating pace of biomedical literature. Many text applications depend on accurate named entity recognition (NER) and normalization (grounding). While high performing machine learning methods trainable for many types exist NER, are usually specialized a single type. NER systems also typically in serial pipeline, causing cascading errors limiting ability system directly exploit lexical information provided by normalization.We propose first model joint during both training prediction. The arbitrary consists semi-Markov structured linear classifier, with rich feature approach supervised semantic indexing normalization. We introduce TaggerOne, Java implementation our as general toolkit TaggerOne not specific any type, requiring only annotated data corresponding lexicon, has been optimized throughput.We validated multiple gold-standard corpora containing mention- concept-level annotations. Benchmarking results show that achieves performance diseases (NCBI Disease corpus, f-score: 0.829, 0.807) chemicals (BioCreative 5 CDR 0.914, f-score 0.895). These compare favorably previous state art, notwithstanding greater flexibility model. conclude jointly modeling greatly improves performance.The source code an online demonstration available at: http://www.ncbi.nlm.nih.gov/bionlp/taggeronezhiyong.lu@nih.govSupplementary at Bioinformatics online."
https://openalex.org/W2549599535,https://doi.org/10.1109/cvpr.2017.356,A Hierarchical Approach for Generating Descriptive Image Paragraphs,2017,"Recent progress on image captioning has made it possible to generate novel sentences describing images in natural language, but compressing an into a single sentence can describe visual content only coarse detail. While one new approach, dense captioning, potentially finer levels of detail by many regions within image, turn is unable produce coherent story for image. In this paper we overcome these limitations generating entire paragraphs images, which tell detailed, unified stories. We develop model that decomposes both and their constituent parts, detecting semantic using hierarchical recurrent neural network reason about language. Linguistic analysis confirms the complexity paragraph generation task, thorough experiments dataset pairs demonstrate effectiveness our approach."
https://openalex.org/W2962769558,https://doi.org/10.18653/v1/p16-1061,Improving Coreference Resolution by Learning Entity-Level Distributed Representations,2016,"A long-standing challenge in coreference resolution has been the incorporation of entity-level information - features defined over clusters mentions instead mention pairs. We present a neural network based system that produces high-dimensional vector representations for pairs clusters. Using these representations, our learns when combining is desirable. train with learning-to-search algorithm teaches it which local decisions (cluster merges) will lead to high-scoring final partition. The substantially outperforms current state-of-the-art on English and Chinese portions CoNLL 2012 Shared Task dataset despite using few hand-engineered features."
https://openalex.org/W2944400536,https://doi.org/10.1038/s41597-019-0055-0,"BioWordVec, improving biomedical word embeddings with subword information and MeSH",2019,"Distributed word representations have become an essential foundation for biomedical natural language processing (BioNLP), text mining and information retrieval. Word embeddings are traditionally computed at the level from a large corpus of unlabeled text, ignoring present in internal structure words or any available domain specific structured resources such as ontologies. However, holds potentials greatly improving quality representation, suggested some recent studies general domain. Here we BioWordVec: open set vectors/embeddings that combines subword with widely-used controlled vocabulary called Medical Subject Headings (MeSH). We assess both validity utility our generated over multiple NLP tasks Our benchmarking results demonstrate can result significantly improved performance previous state art those challenging tasks. Machine-accessible metadata file describing reported data (ISA-Tab format)"
https://openalex.org/W2962714319,https://doi.org/10.1109/iccv.2015.483,Predicting Deep Zero-Shot Convolutional Neural Networks Using Textual Descriptions,2015,"One of the main challenges in Zero-Shot Learning visual categories is gathering semantic attributes to accompany images. Recent work has shown that learning from textual descriptions, such as Wikipedia articles, avoids problem having explicitly define these attributes. We present a new model can classify unseen their description. Specifically, we use text features predict output weights both convolutional and fully connected layers deep neural network (CNN). take advantage architecture CNNs learn at different layers, rather than just an embedding space for modalities, common with existing approaches. The proposed also allows us automatically generate list pseudo- each category consisting words articles. train our models end-to-end us- ing Caltech-UCSD bird flower datasets evaluate ROC Precision-Recall curves. Our empirical results show significantly outperforms previous methods."
https://openalex.org/W2963047628,https://doi.org/10.18653/v1/p18-1072,On the Limitations of Unsupervised Bilingual Dictionary Induction,2018,"Unsupervised machine translation - i.e., not assuming any cross-lingual supervision signal, whether a dictionary, translations, or comparable corpora seems impossible, but nevertheless, Lample et al. (2017) recently proposed fully unsupervised (MT) model. The model relies heavily on an adversarial, word embedding technique for bilingual dictionary induction (Conneau al., 2017), which we examine here. Our results identify the limitations of current MT: performs much worse morphologically rich languages that are dependent marking, when monolingual from different domains algorithms used. We show simple trick, exploiting weak signal identical words, enables more robust and establish near-perfect correlation between performance previously unexplored graph similarity metric."
https://openalex.org/W3102659883,https://doi.org/10.18653/v1/2020.emnlp-main.437,How Much Knowledge Can You Pack Into the Parameters of a Language Model?,2020,"It has recently been observed that neural language models trained on unstructured text can implicitly store and retrieve knowledge using natural queries. In this short paper, we measure the practical utility of approach by fine-tuning pre-trained to answer questions without access any external context or knowledge. We show scales with model size performs competitively open-domain systems explicitly answers from an source when answering questions. To facilitate reproducibility future work, release our code models."
https://openalex.org/W2769387903,https://doi.org/10.1093/bioinformatics/btx761,An attention-based BiLSTM-CRF approach to document-level chemical named entity recognition,2018,"In biomedical research, chemical is an important class of entities, and named entity recognition (NER) task in the field information extraction. However, most popular NER methods are based on traditional machine learning their performances heavily dependent feature engineering. Moreover, these sentence-level ones which have tagging inconsistency problem.In this paper, we propose a neural network approach, i.e. attention-based bidirectional Long Short-Term Memory with conditional random layer (Att-BiLSTM-CRF), to document-level NER. The approach leverages global obtained by attention mechanism enforce consistency across multiple instances same token document. It achieves better little engineering than other state-of-the-art BioCreative IV compound drug name (CHEMDNER) corpus V chemical-disease relation (CDR) (the F-scores 91.14 92.57%, respectively).Data code available at https://github.com/lingluodlut/Att-ChemdNER.yangzh@dlut.edu.cn or wangleibihami@gmail.com.Supplementary data Bioinformatics online."
https://openalex.org/W2798357113,https://doi.org/10.18653/v1/p18-1017,"Obtaining Reliable Human Ratings of Valence, Arousal, and Dominance for 20,000 English Words",2018,"Words play a central role in language and thought. Factor analysis studies have shown that the primary dimensions of meaning are valence, arousal, dominance (VAD). We present NRC VAD Lexicon, which has human ratings for more than 20,000 English words. use Best–Worst Scaling to obtain fine-grained scores address issues annotation consistency plague traditional rating scale methods annotation. show obtained vastly reliable those existing lexicons. also there exist statistically significant differences shared understanding across demographic variables such as age, gender, personality."
https://openalex.org/W2250378130,https://doi.org/10.18653/v1/w15-2812,Generating Semantically Precise Scene Graphs from Textual Descriptions for Improved Image Retrieval,2015,"Semantically complex queries which include attributes of objects and relations between still pose a major challenge to image retrieval systems. Recent work in computer vision has shown that graph-based semantic representation called scene graph is an effective for very detailed descriptions retrieval. In this paper, we show graphs can be effectively created automatically from natural language description. We present rule-based classifierbased parser whose output used including the query outperforms model only considers using our parsers almost as human-constructed (Recall@10 27.1% vs. 33.4%). Additionally, demonstrate general usefulness parsing by showing also generate 3D scenes."
https://openalex.org/W2740782137,https://doi.org/10.18653/v1/e17-1010,Word Sense Disambiguation: A Unified Evaluation Framework and Empirical Comparison,2017,"Word Sense Disambiguation is a long-standing task in Natural Language Processing, lying at the core of human language understanding. However, evaluation automatic systems has been problematic, mainly due to lack reliable framework. In this paper we develop unified framework and analyze performance various fair setup. The results show that supervised clearly outperform knowledge-based models. Among systems, linear classifier trained on conventional local features still proves be hard baseline beat. Nonetheless, recent approaches exploiting neural networks unlabeled corpora achieve promising results, surpassing most test sets."
https://openalex.org/W2769851464,https://doi.org/10.1093/jamia/ocx132,CLAMP – a toolkit for efficiently building customized clinical natural language processing pipelines,2018,"Abstract Existing general clinical natural language processing (NLP) systems such as MetaMap and Clinical Text Analysis Knowledge Extraction System have been successfully applied to information extraction from text. However, end users often customize existing for their individual tasks, which can require substantial NLP skills. Here we present CLAMP (Clinical Language Annotation, Modeling, Processing), a newly developed toolkit that provides not only state-of-the-art components, but also user-friendly graphic user interface help quickly build customized pipelines applications. Our evaluation shows the default pipeline achieved good performance on named entity recognition concept encoding. We demonstrate efficiency of in building customized, high-performance with 2 use cases, extracting smoking status lab test values. is publicly available research use, believe it unique asset community."
https://openalex.org/W2951559648,https://doi.org/10.1609/aaai.v30i1.10362,Character-Aware Neural Language Models,2015,"We describe a simple neural language model that relies only on character-level inputs. Predictions are still made at the word-level. Our employs convolutional network (CNN) and highway over characters, whose output is given to long short-term memory (LSTM) recurrent (RNN-LM). On English Penn Treebank par with existing state-of-the-art despite having 60% fewer parameters. languages rich morphology (Arabic, Czech, French, German, Spanish, Russian), outperforms word-level/morpheme-level LSTM baselines, again The results suggest many languages, character inputs sufficient for modeling. Analysis of word representations obtained from composition part reveals able encode, characters only, both semantic orthographic information."
https://openalex.org/W2327894203,https://doi.org/10.1109/tcyb.2016.2539546,Bi-Level Semantic Representation Analysis for Multimedia Event Detection,2017,"Multimedia event detection has been one of the major endeavors in video analysis. A variety approaches have proposed recently to tackle this problem. Among others, using semantic representation accredited for its promising performance and desirable ability human-understandable reasoning. To generate representation, we usually utilize several external image/video archives apply concept detectors trained on them videos. Due intrinsic difference these archives, resulted is presumable different predicting capabilities a certain event. Notwithstanding, not much work available assessing efficacy from source-level. On other hand, it plausible perceive that some concepts are noisy detecting specific Motivated by two shortcomings, propose bi-level analyzing method. Regarding source-level, our method learns weights attained multimedia archives. Meanwhile, restrains negative influence or irrelevant overall concept-level. In addition, particularly focus efficient with few positive examples, which highly appreciated real-world scenario. We perform extensive experiments challenging TRECVID MED 2013 2014 datasets encouraging results validate approach."
https://openalex.org/W2337875011,https://doi.org/10.18653/v1/n16-1138,Emergent: a novel data-set for stance classification,2016,"We present Emergent, a novel data-set derived from digital journalism project for rumour debunking. The contains 300 rumoured claims and 2,595 associated news articles, collected labelled by journalists with an estimation of their veracity (true, false or unverified). Each article is summarized into headline to indicate whether its stance for, against, observing the claim, where indicates that merely repeats claim. Thus, Emergent provides real-world data source variety natural language processing tasks in context fact-checking. Further presenting dataset, we address task determining respect For this purpose use logistic regression classifier develop features examine agreement accuracy achieved was 73% which 26% higher than one Excitement Open Platform (Magnini et al., 2014)."
https://openalex.org/W2740560510,https://doi.org/10.18653/v1/p17-2102,Separating Facts from Fiction: Linguistic Models to Classify Suspicious and Trusted News Posts on Twitter,2017,"Pew research polls report 62 percent of U.S. adults get news on social media (Gottfried and Shearer, 2016). In a December poll, 64 said that “made-up news” has caused “great deal confusion” about the facts current events (Barthel et al., Fabricated stories in media, ranging from deliberate propaganda to hoaxes satire, contributes this confusion addition having serious effects global stability. work we build predictive models classify 130 thousand posts as suspicious or verified, predict four sub-types – hoaxes, clickbait propaganda. We show neural network trained tweet content interactions outperform lexical models. Unlike previous deception detection, find adding syntax grammar features our does not improve performance. Incorporating linguistic improves classification results, however, interaction are most informative for finer-grained separation between types posts."
https://openalex.org/W2799051177,https://doi.org/10.18653/v1/p18-1117,Context-Aware Neural Machine Translation Learns Anaphora Resolution,2018,"Standard machine translation systems process sentences in isolation and hence ignore extra-sentential information, even though extended context can both prevent mistakes ambiguous cases improve coherence. We introduce a context-aware neural model designed such way that the flow of information from to be controlled analyzed. experiment with an English-Russian subtitles dataset, observe much what is captured by our deals improving pronoun translation. measure correspondences between induced attention distributions coreference relations implicitly captures anaphora. It consistent gains for where pronouns need gendered Beside improvements anaphoric cases, also improves overall BLEU, over its context-agnostic version (+0.7) simple concatenation source (+0.6)."
https://openalex.org/W2963245493,,Modulating early visual processing by language,2017,"It is commonly assumed that language refers to high-level visual concepts while leaving low-level processing unaffected. This view dominates the current literature in computational models for language-vision tasks, where and linguistic inputs are mostly processed independently before being fused into a single representation. In this paper, we deviate from classic pipeline propose modulate \emph{entire processing} by input. Specifically, introduce Conditional Batch Normalization (CBN) as an efficient mechanism convolutional feature maps embedding. We apply CBN pre-trained Residual Network (ResNet), leading MODulatEd ResNet (\MRN) architecture, show significantly improves strong baselines on two question answering tasks. Our ablation study confirms modulating early stages of beneficial."
https://openalex.org/W2964084097,https://doi.org/10.1162/tacl_a_00109,"Many Languages, One Parser",2016,"We train one multilingual model for dependency parsing and use it to parse sentences in several languages. The uses (i) word clusters embeddings; (ii) token-level language information; (iii) language-specific features (fine-grained POS tags). This input representation enables the parser not only effectively multiple languages, but also generalize across languages based on linguistic universals typological similarities, making more effective learn from limited annotations. Our parser’s performance compares favorably strong baselines a range of data scenarios, including when target has large treebank, small or no treebank training."
https://openalex.org/W2506660467,https://doi.org/10.3389/fpsyg.2016.01116,"How Many Words Do We Know? Practical Estimates of Vocabulary Size Dependent on Word Definition, the Degree of Language Input and the Participant’s Age",2016,"Based on an analysis of the literature and a large scale crowdsourcing experiment, we estimate that average 20-year-old native speaker American English knows 42,000 lemmas 4,200 non-transparent multiword expressions, derived from 11,100 word families. The numbers range 27,000 for lowest 5% to 52,000 highest 5%. Between ages 20 60, person learns 6,000 extra or about one new lemma every 2 days. knowledge words can be as shallow knowing exists. In addition, people learn tens thousands inflected forms proper nouns (names), which account substantially high 'words known' mentioned in other publications."
https://openalex.org/W2883409523,https://doi.org/10.18653/v1/p18-1208,Multimodal Language Analysis in the Wild: CMU-MOSEI Dataset and Interpretable Dynamic Fusion Graph,2018,"Analyzing human multimodal language is an emerging area of research in NLP. Intrinsically this (heterogeneous), sequential and asynchronous; it consists the (words), visual (expressions) acoustic (paralinguistic) modalities all form asynchronous coordinated sequences. From a resource perspective, there genuine need for large scale datasets that allow in-depth studies language. In paper we introduce CMU Multimodal Opinion Sentiment Emotion Intensity (CMU-MOSEI), largest dataset sentiment analysis emotion recognition to date. Using data from CMU-MOSEI novel fusion technique called Dynamic Fusion Graph (DFG), conduct experimentation exploit how interact with each other Unlike previously proposed techniques, DFG highly interpretable achieves competative performance when compared previous state art."
https://openalex.org/W2962741379,https://doi.org/10.18653/v1/p18-2094,Double Embeddings and CNN-based Sequence Labeling for Aspect Extraction,2018,"One key task of fine-grained sentiment analysis product reviews is to extract aspects or features that users have expressed opinions on. This paper focuses on supervised aspect extraction using deep learning. Unlike other highly sophisticated learning models, this proposes a novel and yet simple CNN model employing two types pre-trained embeddings for extraction: general-purpose domain-specific embeddings. Without any additional supervision, achieves surprisingly good results, outperforming state-of-the-art existing methods. To our knowledge, the first report such double based achieve very results."
https://openalex.org/W2988975212,https://doi.org/10.18653/v1/d19-1633,Mask-Predict: Parallel Decoding of Conditional Masked Language Models,2019,"Most machine translation systems generate text autoregressively from left to right. We, instead, use a masked language modeling objective train model predict any subset of the target words, conditioned on both input and partially translation. This approach allows for efficient iterative decoding, where we first all words non-autoregressively, then repeatedly mask out regenerate that is least confident about. By applying this strategy constant number iterations, our improves state-of-the-art performance levels non-autoregressive parallel decoding models by over 4 BLEU average. It also able reach within about 1 point typical left-to-right transformer model, while significantly faster."
https://openalex.org/W2250418535,https://doi.org/10.3115/v1/p15-1010,SensEmbed: Learning Sense Embeddings for Word and Relational Similarity,2015,"Word embeddings have recently gained considerable popularity for modeling words in different Natural Language Processing (NLP) tasks including semantic similarity measurement. However, notwithstanding their success, word are by very nature unable to capture polysemy, as meanings of a conflated into single representation. In addition, learning process usually relies on massive corpora only, preventing them from taking advantage structured knowledge. We address both issues proposing multifaceted approach that transforms the sense level and leverages knowledge large network effective evaluate our relational frameworks, reporting state-of-the-art performance multiple datasets."
https://openalex.org/W2555428947,https://doi.org/10.18653/v1/d17-1039,Unsupervised Pretraining for Sequence to Sequence Learning,2017,"This work presents a general unsupervised learning method to improve the accuracy of sequence (seq2seq) models. In our method, weights encoder and decoder seq2seq model are initialized with pretrained two language models then fine-tuned labeled data. We apply this challenging benchmarks in machine translation abstractive summarization find that it significantly improves subsequent supervised Our main result is pretraining generalization achieve state-of-the-art results on WMT English→German task, surpassing range methods using both phrase-based neural translation. achieves significant improvement 1.3 BLEU from th previous best WMT’14 WMT’15 English→German. also conduct human evaluations outperforms purely baseline statistically manner."
https://openalex.org/W2757361303,https://doi.org/10.18653/v1/d17-1160,Neural Semantic Parsing with Type Constraints for Semi-Structured Tables,2017,"We present a new semantic parsing model for answering compositional questions on semi-structured Wikipedia tables. Our parser is an encoder-decoder neural network with two key technical innovations: (1) grammar the decoder that only generates well-typed logical forms; and (2) entity embedding linking module identifies mentions while generalizing across also introduce novel method training our question-answer supervision. On WikiTableQuestions data set, achieves state-of-the-art accuracy of 43.3% single 45.9% 5-model ensemble, improving best prior score 38.7% set by 15-model ensemble. These results suggest type constraints are valuable components to incorporate in parsers."
https://openalex.org/W2913443447,https://doi.org/10.1007/978-3-030-29135-8_7,The Second Conversational Intelligence Challenge (ConvAI2),2019,"We describe the setting and results of ConvAI2 NeurIPS competition that aims to further state-of-the-art in open-domain chatbots. Some key takeaways from are: (1) pretrained Transformer variants are currently best performing models on this task, (2) but improve performance multi-turn conversations with humans, future systems must go beyond single word metrics like perplexity measure across sequences utterances (conversations)—in terms repetition, consistency balance dialogue acts (e.g. how many questions asked vs. answered)."
https://openalex.org/W3099462466,https://doi.org/10.1145/3123266.3123394,Learning Fashion Compatibility with Bidirectional LSTMs,2017,"The ubiquity of online fashion shopping demands effective recommendation services for customers. In this paper, we study two types recommendation: (i) suggesting an item that matches existing components in a set to form stylish outfit (a collection items), and (ii) generating with multimodal (images/text) specifications from user. To end, propose jointly learn visual-semantic embedding the compatibility relationships among items end-to-end fashion. More specifically, consider be sequence (usually top bottom then accessories) each as time step. Given outfit, train bidirectional LSTM (Bi-LSTM) model sequentially predict next conditioned on previous ones their relationships. Further, space by regressing image features semantic representations aiming inject attribute category information regularization training LSTM. trained network can not only perform aforementioned recommendations effectively but also given outfit. We conduct extensive experiments our newly collected Polyvore dataset, results provide strong qualitative quantitative evidence framework outperforms alternative methods."
https://openalex.org/W2962852262,https://doi.org/10.18653/v1/d17-1259,Deal or No Deal? End-to-End Learning of Negotiation Dialogues,2017,"Much of human dialogue occurs in semi-cooperative settings, where agents with different goals attempt to agree on common decisions. Negotiations require complex communication and reasoning skills, but success is easy measure, making this an interesting task for AI. We gather a large dataset human-human negotiations multi-issue bargaining task, who cannot observe each other’s reward functions must reach agreement (or deal) via natural language dialogue. For the first time, we show it possible train end-to-end models negotiation, which learn both linguistic skills no annotated states. also introduce rollouts, model plans ahead by simulating complete continuations conversation, find that technique dramatically improves performance. Our code are publicly available."
https://openalex.org/W2963609017,https://doi.org/10.1109/cvpr.2018.00915,Multimodal Explanations: Justifying Decisions and Pointing to the Evidence,2018,"Deep models that are both effective and explainable desirable in many settings; prior have been unimodal, offering either image-based visualization of attention weights or text-based generation post-hoc justifications. We propose a multimodal approach to explanation, argue the two modalities provide complementary explanatory strengths. collect new datasets define evaluate this task, novel model which can joint textual rationale visualization. Our visual justifications classification decision for activity recognition tasks (ACT-X) question answering (VQA-X). quantitatively show training with explanations not only yields better justification models, but also localizes evidence supports decision. qualitatively cases where explanation is more insightful than vice versa, supporting our thesis offer significant benefits over unimodal approaches."
https://openalex.org/W2964116568,https://doi.org/10.18653/v1/p17-1014,Neural AMR: Sequence-to-Sequence Models for Parsing and Generation,2017,"Sequence-to-sequence models have shown strong performance across a broad range of applications. However, their application to parsing and generating text using Abstract Meaning Representation (AMR) has been limited, due the relatively limited amount labeled data non-sequential nature AMR graphs. We present novel training procedure that can lift this limitation millions unlabeled sentences careful preprocessing For parsing, our model achieves competitive results 62.1 SMATCH, current best score reported without significant use external semantic resources. generation, establishes new state-of-the-art BLEU 33.8. extensive ablative qualitative analysis including evidence sequence-based are robust against ordering variations graph-to-sequence conversions."
https://openalex.org/W2964138343,https://doi.org/10.1016/j.cviu.2017.05.001,Visual question answering: A survey of methods and datasets,2017,"Abstract Visual Question Answering (VQA) is a challenging task that has received increasing attention from both the computer vision and natural language processing communities. Given an image question in language, it requires reasoning over visual elements of general knowledge to infer correct answer. In first part this survey, we examine state art by comparing modern approaches problem. We classify methods their mechanism connect textual modalities. particular, common approach combining convolutional recurrent neural networks map images questions feature space. also discuss memory-augmented modular architectures interface with structured bases. second review datasets available for training evaluating VQA systems. The various datatsets contain at different levels complexity, which require capabilities types reasoning. depth question/answer pairs Genome project, evaluate relevance annotations scene graphs VQA. Finally, promising future directions field, particular connection bases use models."
https://openalex.org/W2972359262,https://doi.org/10.21437/interspeech.2019-2441,LibriTTS: A Corpus Derived from LibriSpeech for Text-to-Speech,2019,"This paper introduces a new speech corpus called LibriTTS designed for text-to-speech use. It is derived from the original audio and text materials of LibriSpeech corpus, which has been used training evaluating automatic recognition systems. The inherits desired properties while addressing number issues make less than ideal work. released consists 585 hours data at 24kHz sampling rate 2,456 speakers corresponding texts. Experimental results show that neural end-to-end TTS models trained achieved above 4.0 in mean opinion scores naturalness five out six evaluation speakers. freely available download this http URL."
https://openalex.org/W2798935874,https://doi.org/10.18653/v1/p18-1128,The Hitchhiker’s Guide to Testing Statistical Significance in Natural Language Processing,2018,"Statistical significance testing is a standard statistical tool designed to ensure that experimental results are not coincidental. In this opinion/ theoretical paper we discuss the role of in Natural Language Processing (NLP) research. We establish fundamental concepts and specific aspects NLP tasks, setups evaluation measures affect choice tests Based on discussion propose simple practical protocol for test selection accompany with brief survey most relevant tests. then recent empirical papers published ACL TACL during 2017 show while our community assigns great value results, often ignored or misused. conclude open issues should be properly addressed so important can applied. research statistically sound manner."
https://openalex.org/W2901476322,https://doi.org/10.1039/c8sc04175j,Learning continuous and data-driven molecular descriptors by translating equivalent chemical representations,2019,Translation between semantically equivalent but syntactically different line notations of molecular structures compresses meaningful information into a continuous descriptor.
https://openalex.org/W1010415138,https://doi.org/10.18653/v1/d15-1200,Do Multi-Sense Embeddings Improve Natural Language Understanding?,2015,"Learning a distinct representation for each sense of an ambiguous word could lead to more powerful and fine-grained models vector-space representations. Yet while `multi-sense' methods have been proposed tested on artificial word-similarity tasks, we don't know if they improve real natural language understanding tasks. In this paper introduce multi-sense embedding model based Chinese Restaurant Processes that achieves state the art performance matching human similarity judgments, propose pipelined architecture incorporating embeddings into understanding. We then test our part-of-speech tagging, named entity recognition, sentiment analysis, semantic relation identification relatedness, controlling dimensionality. find do some tasks (part-of-speech identification, relatedness) but not others (named various forms analysis). discuss how these differences may be caused by different role information in The results highlight importance testing applications."
https://openalex.org/W2138162199,https://doi.org/10.1136/bmj.h1885,Development of phenotype algorithms using electronic medical records and incorporating natural language processing,2015,"Electronic medical records are emerging as a major source of data for clinical and translational research studies, although phenotypes interest need to be accurately defined first. This article provides an overview how develop phenotype algorithm from electronic records, incorporating modern informatics biostatistics methods."
https://openalex.org/W2799054028,https://doi.org/10.48550/arxiv.1804.07461,"GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language
  Understanding",2018,"For natural language understanding (NLU) technology to be maximally useful, both practically and as a scientific object of study, it must general: able process in way that is not exclusively tailored any one specific task or dataset. In pursuit this objective, we introduce the General Language Understanding Evaluation benchmark (GLUE), tool for evaluating analyzing performance models across diverse range existing NLU tasks. GLUE model-agnostic, but incentivizes sharing knowledge tasks because certain have very limited training data. We further provide hand-crafted diagnostic test suite enables detailed linguistic analysis models. evaluate baselines based on current methods multi-task transfer learning find they do immediately give substantial improvements over aggregate separate model per task, indicating room improvement developing general robust systems."
https://openalex.org/W2808284704,https://doi.org/10.24963/ijcai.2018/611,Bootstrapping Entity Alignment with Knowledge Graph Embedding,2018,"Embedding-based entity alignment represents different knowledge graphs (KGs) as low-dimensional embeddings and finds by measuring the similarities between embeddings. Existing approaches have achieved promising results, however, they are still challenged lack of enough prior labeled training data. In this paper, we propose a bootstrapping approach to embedding-based alignment. It iteratively labels likely data for learning alignment-oriented KG Furthermore, it employs an editing method reduce error accumulation during iterations. Our experiments on real-world datasets showed that proposed significantly outperformed state-of-the-art ones The embedding, process all contributed performance improvement."
https://openalex.org/W2964321064,https://doi.org/10.18653/v1/n18-1012,"Dear Sir or Madam, May I Introduce the GYAFC Dataset: Corpus, Benchmarks and Metrics for Formality Style Transfer",2018,"Style transfer is the task of automatically transforming a piece text in one particular style into another. A major barrier to progress this field has been lack training and evaluation datasets, as well benchmarks automatic metrics. In work, we create largest corpus for stylistic (formality) show that techniques from machine translation community can serve strong baselines future work. We also discuss challenges using"
https://openalex.org/W2996908057,https://doi.org/10.1609/aaai.v34i05.6399,WinoGrande: An Adversarial Winograd Schema Challenge at Scale,2020,"The Winograd Schema Challenge (WSC) (Levesque, Davis, and Morgenstern 2011), a benchmark for commonsense reasoning, is set of 273 expert-crafted pronoun resolution problems originally designed to be unsolvable statistical models that rely on selectional preferences or word associations. However, recent advances in neural language have already reached around 90% accuracy variants WSC. This raises an important question whether these truly acquired robust capabilities they spurious biases the datasets lead overestimation true machine commonsense.To investigate this question, we introduce WinoGrande, large-scale dataset 44k problems, inspired by original WSC design, but adjusted improve both scale hardness dataset. key steps construction consist (1) carefully crowdsourcing procedure, followed (2) systematic bias reduction using novel AfLite algorithm generalizes human-detectable associations machine-detectable embedding best state-of-the-art methods WinoGrande achieve 59.4 – 79.1%, which are ∼15-35% (absolute) below human performance 94.0%, depending amount training data allowed (2% 100% respectively).Furthermore, establish new results five related benchmarks — (→ 90.1%), DPR 93.1%), COPA(→ 90.6%), KnowRef 85.6%), Winogender 97.1%). These dual implications: one hand, demonstrate effectiveness when used as resource transfer learning. On other raise concern likely overestimating across all benchmarks. We emphasize importance algorithmic existing future mitigate such overestimation."
https://openalex.org/W3210120707,https://doi.org/10.1109/taslp.2021.3124365,Pre-Training With Whole Word Masking for Chinese BERT,2021,"Bidirectional Encoder Representations from Transformers (BERT) has shown marvelous improvements across various NLP tasks, and its consecutive variants have been proposed to further improve the performance of pre-trained language models. In this paper, we aim first introduce whole word masking (wwm) strategy for Chinese BERT, along with a series Then also propose simple but effective model called MacBERT, which improves upon RoBERTa in several ways. Especially, new MLM as correction (Mac). To demonstrate effectiveness these models, create models our baselines, including RoBERTa, ELECTRA, RBT, etc. We carried out extensive experiments on ten tasks evaluate created well MacBERT. Experimental results show that MacBERT could achieve state-of-the-art performances many ablate details findings may help future research. open-source facilitating research community. Resources are available: https://github.com/ymcui/Chinese-BERT-wwm"
https://openalex.org/W2250739653,https://doi.org/10.18653/v1/d15-1141,Long Short-Term Memory Neural Networks for Chinese Word Segmentation,2015,"Currently most of state-of-the-art methods for Chinese word segmentation are based on supervised learning, whose features aremostly extracted from a local context. Thesemethods cannot utilize the long distance information which is also crucial segmentation. In this paper, we propose novel neural network model segmentation, adopts short-term memory (LSTM) to keep previous important inmemory cell and avoids limit window size Experiments PKU, MSRA CTB6 benchmark datasets show that our outperforms models methods."
https://openalex.org/W2756442192,https://doi.org/10.1016/j.inffus.2017.09.012,A minimum adjustment cost feedback mechanism based consensus model for group decision making under social network with distributed linguistic trust,2018,"Abstract A theoretical feedback mechanism framework to model consensus in social network group decision making (SN-GDM) is proposed with following two main components: (1) the modelling of trust relationship linguistic information; and (2) minimum adjustment cost mechanism. To do so, a distributed space defined, which includes novel concepts functions, expectation degree, uncertainty degrees ranking method. Then, analysis (SNA) methodology developed represent between networked group, in-degree centrality indexes are calculated assign an importance degree associated user. identify inconsistent users, three levels functions calculated. activated generate recommendation advices for users increase degree. Its novelty that it produces boundary parameter based on optimisation model. Therefore, able reach threshold value incurring modification their opinions or cost, provides optimum balance individual independence. Finally, after has been achieved, order relation constructed select most appropriate alternative consensus."
https://openalex.org/W2756894032,https://doi.org/10.1016/j.cub.2018.01.080,"Electrophysiological Correlates of Semantic Dissimilarity Reflect the Comprehension of Natural, Narrative Speech",2018,"People routinely hear and understand speech at rates of 120-200 words per minute [1, 2]. Thus, comprehension must involve rapid, online neural mechanisms that process words' meanings in an approximately time-locked fashion. However, electrophysiological evidence for such processing has been lacking continuous speech. Although valuable insights into semantic have provided by the ""N400 component"" event-related potential [3-6], this literature dominated paradigms using incongruous within specially constructed sentences, with less emphasis on natural, narrative comprehension. Building discovery cortical activity ""tracks"" dynamics running [7-9] psycholinguistic work demonstrating [10-12] modeling [13-15] how context impacts word processing, we describe a new approach deriving correlate natural We used computational model [16] to quantify meaning carried based semantically dissimilar they were their preceding then regressed measure against electroencephalographic (EEG) data recorded from subjects as listened This produced prominent negativity time lag 200-600 ms centro-parietal EEG channels, characteristics common N400. Applying datasets involving time-reversed speech, cocktail party attention, audiovisual speech-in-noise demonstrated response was very sensitive whether or not understood heard. These findings demonstrate that, when successfully comprehending human brain responds contextual content each relatively"
https://openalex.org/W2786660442,https://doi.org/10.18653/v1/w17-3518,The WebNLG Challenge: Generating Text from RDF Data,2017,"The WebNLG challenge consists in mapping sets of RDF triples to text. It provides a common benchmark on which train, evaluate and compare “microplanners”, i.e. generation systems that verbalise given content by making range complex interacting choices including referring expression generation, aggregation, lexicalisation, surface realisation sentence segmentation. In this paper, we introduce the microplanning task, describe data preparation, our evaluation methodology, analyse participant results provide brief description participating systems."
https://openalex.org/W947140380,https://doi.org/10.1109/mis.2016.45,How to Generate a Good Word Embedding,2016,"We analyze three critical components of word embedding training: the model, corpus, and training parameters. systematize existing neural-network-based algorithms compare them using same corpus. evaluate each in ways: analyzing its semantic properties, it as a feature for supervised tasks to initialize neural networks. also provide several simple guidelines embeddings. First, we discover that corpus domain is more important than size. recommend choosing suitable desired task, after that, larger yields better results. Second, find faster models sufficient performance most cases, complex can be used if sufficiently large. Third, early stopping metric iterating should rely on development set task rather validation loss embedding."
https://openalex.org/W2963047498,https://doi.org/10.1109/tpami.2017.2695539,Drawing and Recognizing Chinese Characters with Recurrent Neural Network,2018,"Recent deep learning based approaches have achieved great success on handwriting recognition. Chinese characters are among the most widely adopted writing systems in world. Previous research has mainly focused recognizing handwritten characters. However, recognition is only one aspect for understanding a language, another challenging and interesting task to teach machine automatically write (pictographic) In this paper, we propose framework by using recurrent neural network (RNN) as both discriminative model generative drawing (generating) To recognize characters, previous methods usually adopt convolutional (CNN) models which require transforming online trajectory into image-like representations. Instead, our RNN approach an end-to-end system directly deals with sequential structure does not any domain-specific knowledge. With (combining LSTM GRU), state-of-the-art performance can be ICDAR-2013 competition database. Furthermore, under framework, conditional character embedding proposed recognizable The generated (in vector format) human-readable also recognized high accuracy. Experimental results verify effectiveness of RNNs tasks"
https://openalex.org/W2963850840,https://doi.org/10.1613/jair.1.11259,From Word To Sense Embeddings: A Survey on Vector Representations of Meaning,2018,"&#x0D; Over the past years, distributed semantic representations have proved to be effective and flexible keepers of prior knowledge integrated into downstream applications. This survey focuses on representation meaning. We start from theoretical background behind word vector space models highlight one their major limitations: meaning conflation deficiency, which arises representing a with all its possible meanings as single vector. Then, we explain how this deficiency can addressed through transition level more fine-grained senses (in broader acceptation) method for modelling unambiguous lexical present comprehensive overview wide range techniques in two main branches sense representation, i.e., unsupervised knowledge-based. Finally, covers evaluation procedures applications type provides an analysis four important aspects: interpretability, granularity, adaptability different domains compositionality.&#x0D;"
https://openalex.org/W1608166496,https://doi.org/10.3389/fpsyg.2015.00731,Timing in turn-taking and its implications for processing models of language,2015,"The core niche for language use is in verbal interaction, involving the rapid exchange of turns at talking. This paper reviews extensive literature about this system, adding new statistical analyses behavioral data where they have been missing, demonstrating that turn-taking has systematic properties originally noted by Sacks et al. (1974; hereafter SSJ). system poses some significant puzzles current theories processing: gaps between are short (of order 200 ms), but latencies involved production much longer (over 600 ms). seems to imply participants conversation must predict (or 'project' as SSJ it) end speaker's turn prepare their response advance. implies overlap and comprehension despite common processing resources. Collecting together what known behaviorally experimentally space explanations can be significantly narrowed, we sketch first model mental processes participant preparing speak next."
https://openalex.org/W2173180041,https://doi.org/10.1109/cvpr.2016.8,Deep Compositional Captioning: Describing Novel Object Categories without Paired Training Data,2016,"While recent deep neural network models have achieved promising results on the image captioning task, they rely largely availability of corpora with paired and sentence captions to describe objects in context. In this work, we propose Deep Compositional Captioner (DCC) address task generating descriptions novel which are not present imagesentence datasets. Our method achieves by leveraging large object recognition datasets external text transferring knowledge between semantically similar concepts. Current caption can only contained image-sentence corpora, despite fact that pre-trained datasets, namely ImageNet. contrast, our model compose sentences their interactions other objects. We demonstrate model's ability concepts empirically evaluating its performance MSCOCO show qualitative ImageNet images for no data exist. Further, extend approach generate video clips. DCC has distinct advantages over existing approaches new"
https://openalex.org/W2577366047,https://doi.org/10.21437/interspeech.2017-343,Towards Better Decoding and Language Model Integration in Sequence to Sequence Models,2017,"The recently proposed Sequence-to-Sequence (seq2seq) framework advocates replacing complex data processing pipelines, such as an entire automatic speech recognition system, with a single neural network trained in end-to-end fashion. In this contribution, we analyse attention-based seq2seq system that directly transcribes recordings into characters. We observe two shortcomings: overconfidence its predictions and tendency to produce incomplete transcriptions when language models are used. propose practical solutions both problems achieving competitive speaker independent word error rates on the Wall Street Journal dataset: without separate reach 10.6% WER, while together trigram model, 6.7% WER."
https://openalex.org/W2962727366,https://doi.org/10.18653/v1/d18-1546,How Much Reading Does Reading Comprehension Require? A Critical Investigation of Popular Benchmarks,2018,"Many recent papers address reading comprehension, where examples consist of (question, passage, answer) tuples. Presumably, a model must combine information from both questions and passages to predict corresponding answers. However, despite intense interest in the topic, with hundreds published vying for leaderboard dominance, basic about difficulty many popular benchmarks remain unanswered. In this paper, we establish sensible baselines bAbI, SQuAD, CBT, CNN, Who-did-What datasets, finding that question- passage-only models often perform surprisingly well. On 14 out 20 bAbI tasks, achieve greater than 50% accuracy, sometimes matching full model. Interestingly, while CBT provides 20-sentence passages, only last is needed accurate prediction. By comparison, SQuAD CNN appear better-constructed."
https://openalex.org/W2962779710,https://doi.org/10.1016/j.patcog.2016.08.005,Online and offline handwritten Chinese character recognition: A comprehensive study and new benchmark,2017,"Abstract Recent deep learning based methods have achieved the state-of-the-art performance for handwritten Chinese character recognition (HCCR) by discriminative representations directly from raw data. Nevertheless, we believe that long-and-well investigated domain-specific knowledge should still help to boost of HCCR. By integrating traditional normalization-cooperated direction-decomposed feature map (directMap) with convolutional neural network (convNet), are able obtain new highest accuracies both online and offline HCCR on ICDAR-2013 competition database. With this framework, can eliminate needs data augmentation model ensemble, which widely used in other systems achieve their best results. This makes our framework be efficient effective training testing. Furthermore, although directMap+convNet results surpass human-level performance, show writer adaptation case is effective. A layer proposed reduce mismatch between test a particular source layer. The process efficiently effectively implemented an unsupervised manner. adding into pre-trained convNet, it adapt handwriting styles writers, accuracy further improved consistently significantly. paper gives overview comparison recent approaches HCCR, also sets benchmarks"
https://openalex.org/W2963686907,https://doi.org/10.1109/cvpr.2018.00583,Convolutional Image Captioning,2018,"Image captioning is an important task, applicable to virtual assistants, editing tools, image indexing, and support of the disabled. In recent years significant progress has been made in captioning, using Recurrent Neural Networks powered by long-short-term-memory (LSTM) units. Despite mitigating vanishing gradient problem, despite their compelling ability memorize dependencies, LSTM units are complex inherently sequential across time. To address this issue, work shown benefits convolutional networks for machine translation conditional generation [9, 34, 35]. Inspired success, paper, we develop a technique. We demonstrate its efficacy on challenging MSCOCO dataset performance par with baseline [16], while having faster training time per number parameters. also perform detailed analysis, providing reasons favor language approaches."
https://openalex.org/W3034723486,https://doi.org/10.18653/v1/2020.acl-main.463,"Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data",2020,"The success of the large neural language models on many NLP tasks is exciting. However, we find that these successes sometimes lead to hype in which are being described as ``understanding'' or capturing ``meaning''. In this position paper, argue a system trained only form has priori no way learn meaning. keeping with ACL 2020 theme ``Taking Stock Where We've Been and We're Going'', clear understanding distinction between meaning will help guide field towards better science around natural understanding."
https://openalex.org/W2890431379,https://doi.org/10.18653/v1/d18-1425,Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task,2018,"We present Spider, a large-scale complex and cross-domain semantic parsing text-to-SQL dataset annotated by 11 college students. It consists of 10,181 questions 5,693 unique SQL queries on 200 databases with multiple tables covering 138 different domains. define new task so that complicated appear in train test sets. In this way, the requires model to generalize well both database schemas. Therefore, Spider is distinct from most previous tasks because they all use single have exact same program set set. experiment various state-of-the-art models best achieves only 9.7% matching accuracy split setting. This shows presents strong challenge for future research. Our recent updates are publicly available at https://yale-lily.github.io/seq2sql/spider."
https://openalex.org/W3102286003,https://doi.org/10.1145/3331184.3331317,CEDR,2019,"Although considerable attention has been given to neural ranking architectures recently, far less paid the term representations that are used as input these models. In this work, we investigate how two pretrained contextualized language models (ELMo and BERT) can be utilized for ad-hoc document ranking. Through experiments on TREC benchmarks, find several existing benefit from additional context provided by Furthermore, propose a joint approach incorporates BERT's classification vector into show it outperforms state-of-the-art baselines. We call CEDR (Contextualized Embeddings Document Ranking). also address practical challenges in using ranking, including maximum length imposed BERT runtime performance impacts of"
https://openalex.org/W1746111881,https://doi.org/10.1162/tacl_a_00139,Problems in Current Text Simplification Research: New Data Can Help,2015,"Simple Wikipedia has dominated simplification research in the past 5 years. In this opinion paper, we argue that focusing on limits research. We back up our arguments with corpus analysis and by highlighting statements other researchers have made literature. introduce a new dataset is significant improvement over Wikipedia, present novel quantitative-comparative approach to study quality of data resources."
https://openalex.org/W2169491861,https://doi.org/10.1186/1758-2946-7-s1-s3,tmChem: a high performance approach for chemical named entity recognition and normalization,2015,"Abstract Chemical compounds and drugs are an important class of entities in biomedical research with great potential a wide range applications, including clinical medicine. Locating chemical named the literature is useful step text mining pipelines for identifying mentions, their properties, relationships as discussed literature. We introduce tmChem system, entity recognizer created by combining two independent machine learning models ensemble. use corpus released part recent CHEMDNER task to develop evaluate tmChem, achieving micro-averaged f-measure 0.8739 on CEM subtask (mention-level evaluation) 0.8745 CDI (abstract-level evaluation). also report high-recall combination (0.9212 0.9224 CDI). achieved highest reported subtask, high recall variant both tasks. that state-of-the-art tool recognition performance has now tied (or exceeded) previously genes diseases. Future should focus tighter integration between normalization steps improved performance. The source code trained model available at: http://www.ncbi.nlm.nih.gov/CBBresearch/Lu/Demo/tmChem . results running (Model 2) PubMed PubTator: http://www.ncbi.nlm.nih.gov/CBBresearch/Lu/Demo/PubTator"
https://openalex.org/W2561412020,https://doi.org/10.1371/journal.pone.0181142,"""What is relevant in a text document?"": An interpretable machine learning approach",2017,"Text documents can be described by a number of abstract concepts such as semantic category, writing style, or sentiment. Machine learning (ML) models have been trained to automatically map these concepts, allowing annotate very large text collections, more than could processed human in lifetime. Besides predicting the text's category accurately, it is also highly desirable understand how and why categorization process takes place. In this paper, we demonstrate that understanding achieved tracing classification decision back individual words using layer-wise relevance propagation (LRP), recently developed technique for explaining predictions complex non-linear classifiers. We train two word-based ML models, convolutional neural network (CNN) bag-of-words SVM classifier, on topic task adapt LRP method decompose onto words. Resulting scores indicate much contribute overall decision. This enables one distill relevant information from without an explicit extraction step. further use word-wise generating novel vector-based document representations which capture information. Based vectors, introduce measure model explanatory power show that, although CNN perform similarly terms accuracy, latter exhibits higher level explainability makes comprehensible humans potentially useful other applications."
https://openalex.org/W2962845008,https://doi.org/10.1109/cvpr.2019.00160,MirrorGAN: Learning Text-To-Image Generation by Redescription,2019,"Generating an image from a given text description has two goals: visual realism and semantic consistency. Although significant progress been made in generating high-quality visually realistic images using generative adversarial networks, guaranteeing consistency between the content remains very challenging. In this paper, we address problem by proposing novel global-local attentive semantic-preserving text-to-image-to-text framework called MirrorGAN. MirrorGAN exploits idea of learning text-to-image generation redescription consists three modules: embedding module (STEM), collaborative for cascaded (GLAM), regeneration alignment (STREAM). STEM generates word- sentence-level embeddings. GLAM architecture target coarse to fine scales, leveraging both local word attention global sentence progressively enhance diversity generated images. STREAM seeks regenerate image, which semantically aligns with description. Thorough experiments on public benchmark datasets demonstrate superiority over other representative state-of-the-art methods."
https://openalex.org/W2963265326,https://doi.org/10.18653/v1/p17-1054,Deep Keyphrase Generation,2017,"© 2017 Association for Computational Linguistics. Keyphrase provides highly-summative information that can be effectively used understanding, organizing and retrieving text content. Though previous studies have provided many workable solutions automated keyphrase extraction, they commonly divided the to-be-summarized content into multiple chunks, then ranked selected most meaningful ones. These approaches could neither identify keyphrases do not appear in text, nor capture real semantic meaning behind text. We propose a generative model prediction with an encoder-decoder framework, which overcome above drawbacks. name it as deep generation since attempts to of learning method. Empirical analysis on six datasets demonstrates our proposed only achieves significant performance boost extracting source but also generate absent based Code dataset are available at https://github.com/memray/seq2seqkeyphrase."
https://openalex.org/W2052452825,https://doi.org/10.1016/j.eswa.2014.09.024,Business intelligence in banking: A literature analysis from 2002 to 2013 using text mining and latent Dirichlet allocation,2015,"A recent review on the application of business intelligence to banking domain.Coverage last twelve years scientific literature those subjects.Usage text mining and latent Dirichlet allocation analyze articles.Provide new insights future research trends which may benefit business. This paper analyzes in search for applications industry. Searches were performed relevant journals resulting 219 articles published between 2002 2013. To such a large number manuscripts, techniques used pursuit terms both domains. Moreover, modeling was order group several topics. The analysis conducted using dictionary belonging Such procedure allowed identification relationships topics grouping articles, enabling emerge hypotheses regarding directions. confirm hypotheses, collected scrutinized, allowing validate procedure. results show that credit is clearly main trend, particularly predicting risk thus supporting approval or denial. There also interest bankruptcy fraud prediction. Customer retention seems be associated, although weakly, with targeting, justifying bank offers reduce churn. In addition, focused more its applications, industry just evaluation, thus, not acclaiming benefits By identifying these current topics, this study highlights opportunities research."
https://openalex.org/W2119728020,https://doi.org/10.1016/j.bandl.2014.10.006,The ERP response to the amount of information conveyed by words in sentences,2015,"Reading times on words in a sentence depend the amount of information convey, which can be estimated by probabilistic language models. We investigate whether event-related potentials (ERPs), too, are predicted measures. Three types models four different measures each word sample English sentences. Six ERP deflections were extracted from EEG signal participants reading same A comparison between and ERPs revealed reliable correlation N400 amplitude surprisal. Language that make no use syntactic structure fitted data better than did phrase-structure grammar, not account for unique variance amplitude. These findings suggest quantify cognitively processes readers do sentence's hierarchical generating expectations about upcoming word."
https://openalex.org/W2252381721,https://doi.org/10.1145/3124420,Automatic Sarcasm Detection,2017,"Automatic sarcasm detection is the task of predicting in text. This a crucial step to sentiment analysis, considering prevalence and challenges sentiment-bearing Beginning with an approach that used speech-based features, automatic has witnessed great interest from analysis community. article compilation past work detection. We observe three milestones research so far: semi-supervised pattern extraction identify implicit sentiment, use hashtag-based supervision, incorporation context beyond target In this article, we describe datasets, approaches, trends, issues also discuss representative performance values, shared tasks, provide pointers future work, as given prior works. terms resources understand state-of-the-art, survey presents several useful illustrations—most prominently, table summarizes papers along different dimensions such types annotation techniques, datasets used."
https://openalex.org/W2518202280,https://doi.org/10.18653/v1/p16-1085,Embeddings for Word Sense Disambiguation: An Evaluation Study,2016,"Recent years have seen a dramatic growth in the popularity of word embeddings mainly owing to their ability capture semantic information from massive amounts textual content. As result, many tasks Natural Language Processing tried take advantage potential these distributional models. In this work, we study how can be used Word Sense Disambiguation, one oldest and Artificial Intelligence. We propose different methods through which leveraged state-of-the-art supervised WSD system architecture, perform deep analysis parameters affect performance. show that makes use alone, if designed properly, provide significant performance improvement over state-ofthe-art incorporates several standard features."
https://openalex.org/W2523785361,https://doi.org/10.1021/acs.jcim.6b00207,ChemDataExtractor: A Toolkit for Automated Extraction of Chemical Information from the Scientific Literature,2016,"The emergence of ""big data"" initiatives has led to the need for tools that can automatically extract valuable chemical information from large volumes unstructured data, such as scientific literature. Since be present in figures, tables, and textual paragraphs, successful extraction often depends on ability interpret all these domains simultaneously. We a complete toolkit automated entities their associated properties, measurements, relationships documents used populate structured databases. Our system provides an extensible, chemistry-aware, natural language processing pipeline tokenization, part-of-speech tagging, named entity recognition, phrase parsing. Within this scope, we report improved performance recognition through use unsupervised word clustering based massive corpus chemistry articles. For parsing extraction, novel multiple rule-based grammars are tailored interpreting specific document captions, tables. also describe document-level resolve data interdependencies show is particularly necessary autogeneration databases since captions tables commonly contain identifiers references defined elsewhere text. correctly various types was evaluated, affording F-score 93.4%, 86.8%, 91.5% extracting identifiers, spectroscopic attributes, property respectively; set against CHEMDNER name challenge, ChemDataExtractor yields competitive 87.8%. All have been released under MIT license available download http://www.chemdataextractor.org ."
https://openalex.org/W2251882135,https://doi.org/10.3115/v1/p15-2070,"PPDB 2.0: Better paraphrase ranking, fine-grained entailment relations, word embeddings, and style classification",2015,"Ellie Pavlick, Pushpendre Rastogi, Juri Ganitkevitch, Benjamin Van Durme, Chris Callison-Burch. Proceedings of the 53rd Annual Meeting Association for Computational Linguistics and 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers). 2015."
https://openalex.org/W2550799801,https://doi.org/10.1016/j.inffus.2016.11.012,Ranking products through online reviews: A method based on sentiment analysis technique and intuitionistic fuzzy set theory,2017,"A new problem on ranking products through online reviews is formulated.A resolution process for the presented.A algorithm given to identify sentiment orientations concerning features.An approach converting identified into intuitionistic fuzzy numbers proposed.An based set theory given. Online product have significant impacts consumers purchase decisions. To support decisions, how rank a valuable research topic, while this issue still relatively scarce. This paper proposes method analysis technique and reviews. An dictionaries developed positive, neutral or negative orientation alternative feature in each review. According orientations, an number constructed representing performance of feature. The determined by weighted averaging (IFWA) operator preference organization methods enrichment evaluations II (PROMETHEE II). case study illustrate use proposed method. comparisons experiments are further conducted characteristics advantages Converting idea processing fusing large Based method, decision system can be decisions more conveniently."
https://openalex.org/W2796422723,https://doi.org/10.1007/978-3-030-01261-8_4,Learning SO(3) Equivariant Representations with Spherical CNNs,2020,We address the problem of 3D rotation equivariance in convolutional neural networks. rotations have been a challenging nuisance classification tasks requiring higher capacity and extended data augmentation order to tackle it. model with multi-valued spherical functions we propose novel network that implements exact convolutions on sphere by realizing them harmonic domain. Resulting filters local symmetry are localized enforcing smooth spectra. apply pooling spectral domain our operations independent underlying resolution throughout network. show networks much lower without can exhibit performance comparable state art standard retrieval benchmarks.
https://openalex.org/W2962795934,https://doi.org/10.1109/cvpr.2016.495,Unsupervised Learning from Narrated Instruction Videos,2016,"We address the problem of automatically learning main steps to complete a certain task, such as changing car tire, from set narrated instruction videos. The contributions this paper are three-fold. First, we develop new unsupervised approach that takes advantage complementary nature input video and associated narration. method solves two clustering problems, one in text video, applied after each other linked by joint constraints obtain single coherent sequence both modalities. Second, collect annotate challenging dataset real-world videos Internet. contains about 800,000 frames for five different tasks1 include complex interactions between people objects, captured variety indoor outdoor settings. Third, experimentally demonstrate proposed can discover, an manner, achieve task locate"
https://openalex.org/W2981087920,https://doi.org/10.1016/j.csl.2019.101027,Voxceleb: Large-scale speaker verification in the wild,2020,"Abstract The objective of this work is speaker recognition under noisy and unconstrained conditions. We make two key contributions. First, we introduce a very large-scale audio-visual dataset collected from open source media using a fully automated pipeline. Most existing datasets for identification contain samples obtained quite constrained conditions, usually require manual annotations, hence are limited in size. propose pipeline based on computer vision techniques to create the open-source media. Our involves obtaining videos YouTube; performing active verification two-stream synchronization Convolutional Neural Network (CNN), confirming identity CNN facial recognition. use curate VoxCeleb which contains over million ‘real-world’ utterances 6000 speakers. This several times larger than any publicly available dataset. Second, develop compare different architectures with various aggregation methods training loss functions that can effectively recognise identities voice models trained our surpass performance previous works by significant margin."
https://openalex.org/W2962737704,https://doi.org/10.1109/iccv.2017.121,Visual Relationship Detection with Internal and External Linguistic Knowledge Distillation,2017,"Understanding visual relationships involves identifying the subject, object, and a predicate relating them. We leverage strong correlations between (subj,obj) pair (both semantically spatially) to predict predicates conditioned on subjects objects. Modeling three entities jointly more accurately reflects their relationships, but complicates learning since semantic space of is huge training data limited, especially for long-tail that have few instances. To overcome this, we use knowledge linguistic statistics regularize model learning. obtain by mining from both annotations (internal knowledge) publicly available text, e.g., Wikipedia (external knowledge), computing conditional probability distribution given pair. Then, distill into deep achieve better generalization. Our experimental results Visual Relationship Detection (VRD) Genome datasets suggest with this distillation, our outperforms state-of-the-art methods significantly, when predicting unseen (e.g., recall improved 8.45% 19.17% VRD zero-shot testing set)."
https://openalex.org/W2963690854,https://doi.org/10.1109/tg.2018.2846639,Procedural Content Generation via Machine Learning (PCGML),2018,"This survey explores procedural content generation via machine learning (PCGML), defined as the of game using models trained on existing content. As importance PCG for development increases, researchers explore new avenues generating high-quality with or without human involvement; this paper addresses relatively paradigm (in contrast search-based, solver-based, and constructive methods). We focus what is most often considered functional content, such platformer levels, maps, interactive fiction stories, cards in collectible card games, opposed to cosmetic sprites sound effects. In addition autonomous generation, cocreativity, mixed-initiative design, compression, PCGML suited repair, critique, analysis because its modeling discuss various data sources representations that affect generated Multiple methods are covered, including neural networks: long short-term memory networks, autoencoders, deep convolutional networks; Markov models: $n$ -grams multi-dimensional chains; clustering; matrix factorization. Finally, we open problems PCGML, from small sets, lack training data, multilayered learning, style-transfer, parameter tuning, a mechanic."
https://openalex.org/W2068297964,https://doi.org/10.1145/2766462.2767752,Monolingual and Cross-Lingual Information Retrieval Models Based on (Bilingual) Word Embeddings,2015,"We propose a new unified framework for monolingual (MoIR) and cross-lingual information retrieval (CLIR) which relies on the induction of dense real-valued word vectors known as embeddings (WE) from comparable data. To this end, we make several important contributions: (1) present novel representation learning model called Bilingual Word Embeddings Skip-Gram (BWESG) is first able to learn bilingual solely basis document-aligned data; (2) demonstrate simple yet effective approach building document single by utilizing models compositional distributional semantics. BWESG induces shared embedding vector space in both words, queries, documents may be presented vectors; (3) build ad-hoc MoIR CLIR rely induced space; (4) Experiments English Dutch MoIR, well English-to-Dutch Dutch-to-English using benchmarking CLEF 2001-2003 collections queries utility our WE-based models. The best results are obtained combination unigram language model. also report significant improvements IR tasks over state-of-the-art text representations data based latent Dirichlet allocation (LDA)."
https://openalex.org/W2963266252,https://doi.org/10.21437/interspeech.2017-405,English Conversational Telephone Speech Recognition by Humans and Machines,2017,"One of the most difficult speech recognition tasks is accurate human to communication. Advances in deep learning over last few years have produced major improvements on representative Switchboard conversational corpus. Word error rates that just a ago were 14% dropped 8.0%, then 6.6% and recently 5.8%, are now believed be within striking range performance. This raises two issues - what IS performance, how far down can we still drive rates? A recent paper by Microsoft suggests already achieved In trying verify this statement, performed an independent set performance measurements found may considerably better than was earlier reported, giving community significantly harder goal achieve. We also report our own efforts area, presenting acoustic language modeling techniques lowered word rate English telephone LVCSR system level 5.5%/10.3% Switchboard/CallHome subsets Hub5 2000 evaluation, which at least writing new milestone (albeit not measure performance!). On side, use score fusion three models: one LSTM with multiple feature inputs, second trained speaker-adversarial multi-task third residual net (ResNet) 25 convolutional layers time-dilated convolutions. character LSTMs WaveNet-style models."
https://openalex.org/W2963686995,https://doi.org/10.18653/v1/p19-1050,MELD: A Multimodal Multi-Party Dataset for Emotion Recognition in Conversations,2019,"Emotion recognition in conversations is a challenging task that has recently gained popularity due to its potential applications. Until now, however, large-scale multimodal multi-party emotional conversational database containing more than two speakers per dialogue was missing. Thus, we propose the Multimodal EmotionLines Dataset (MELD), an extension and enhancement of EmotionLines. MELD contains about 13,000 utterances from 1,433 dialogues TV-series Friends. Each utterance annotated with emotion sentiment labels, encompasses audio, visual textual modalities. We several strong baselines show importance contextual information for conversations. The full dataset available use at http://affective-meld.github.io."
https://openalex.org/W1775434803,https://doi.org/10.48550/arxiv.1502.01710,Text Understanding from Scratch,2015,"This article demontrates that we can apply deep learning to text understanding from character-level inputs all the way up abstract concepts, using temporal convolutional networks (ConvNets). We ConvNets various large-scale datasets, including ontology classification, sentiment analysis, and categorization. show achieve astonishing performance without knowledge of words, phrases, sentences any other syntactic or semantic structures with regards a human language. Evidence shows our models work for both English Chinese."
https://openalex.org/W2133458109,https://doi.org/10.18653/v1/s15-2045,"SemEval-2015 Task 2: Semantic Textual Similarity, English, Spanish and Pilot on Interpretability",2015,"In semantic textual similarity (STS), systems rate the degree of equivalence between two text snippets. This year, participants were challenged with new datasets in English and Spanish. The annotations for both subtasks leveraged crowdsourcing. subtask attracted 29 teams 74 system runs, Spanish engaged 7 participating 16 runs. addition, this year we ran a pilot task on interpretable STS, where needed to add an explanatory layer, that is, they had align chunks sentence pair, explicitly annotating kind relation score chunk pair. train test data manually annotated by expert, included headline image pairs from previous years. participated"
https://openalex.org/W2165975954,https://doi.org/10.1016/j.brainres.2015.02.014,Four central questions about prediction in language processing,2015,"The notion that prediction is a fundamental principle of human information processing has been en vogue over recent years. investigation language may be particularly illuminating for testing this claim. Linguists traditionally have argued plays only minor role during understanding because the vast possibilities available to user as each word encountered. In present review I consider four central questions anticipatory processing: Why (i.e. what function in processing)? What are cues used predict up-coming linguistic and type representations predicted)? How (what mechanisms involved predictive possible mediating factors such working memory)? When do individuals always input propose occurs via set diverse PACS (production-, association-, combinatorial-, simulation-based prediction) which minimally required comprehensive account processing. Models must revised take multiple mechanisms, factors, situational context into account. Finally, conjecture evidence considered here consistent with an important aspect but not This article part Special Issue entitled SI: Prediction Attention."
https://openalex.org/W2741075451,https://doi.org/10.18653/v1/p17-1132,Leveraging Knowledge Bases in LSTMs for Improving Machine Reading,2017,"This paper focuses on how to take advantage of external knowledge bases (KBs) improve recurrent neural networks for machine reading. Traditional methods that exploit from KBs encode as discrete indicator features. Not only do these features generalize poorly, but they require task-specific feature engineering achieve good performance. We propose KBLSTM, a novel model leverages continuous representations enhance the learning To effectively integrate background with information currently processed text, our employs an attention mechanism sentinel adaptively decide whether attend and which is useful. Experimental results show achieves accuracies surpass previous state-of-the-art both entity extraction event widely used ACE2005 dataset."
https://openalex.org/W2962785754,https://doi.org/10.18653/v1/p19-1499,HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization,2019,"Neural extractive summarization models usually employ a hierarchical encoder for document encoding and they are trained using sentence-level labels, which created heuristically rule-based methods. Training the with these \emph{inaccurate} labels is challenging. Inspired by recent work on pre-training transformer sentence encoders \cite{devlin:2018:arxiv}, we propose {\sc Hibert} (as shorthand {\bf HI}erachical B}idirectional E}ncoder R}epresentations from T}ransformers) method to pre-train it unlabeled data. We apply pre-trained our model outperforms its randomly initialized counterpart 1.25 ROUGE CNN/Dailymail dataset 2.0 version of New York Times dataset. also achieve state-of-the-art performance two datasets."
https://openalex.org/W2962903510,https://doi.org/10.18653/v1/s17-2091,SemEval 2017 Task 10: ScienceIE - Extracting Keyphrases and Relations from Scientific Publications,2017,"We describe the SemEval task of extracting keyphrases and relations between them from scientific documents, which is crucial for understanding publications processes, tasks materials. Although this was a new task, we had total 26 submissions across 3 evaluation scenarios. expect findings reported in paper to be relevant researchers working on content, as well broader knowledge base population information extraction communities."
https://openalex.org/W2963351776,https://doi.org/10.18653/v1/p16-1170,Generating Natural Questions About an Image,2016,"There has been an explosion of work in the vision & language community during past few years from image captioning to video transcription, and answering questions about images. These tasks have focused on literal descriptions image. To move beyond literal, we choose explore how are often directed at commonsense inference abstract events evoked by objects In this paper, introduce novel task Visual Question Generation (VQG), where system is tasked with asking a natural engaging question when shown We provide three datasets which cover variety images object-centric event-centric, considerably more training data than provided state-of-the-art systems thus far. train test several generative retrieval models tackle VQG. Evaluation results show that while such ask reasonable for images, there still wide gap human performance motivates further connecting knowledge pragmatics. Our proposed offers new challenge hope furthers interest exploring deeper connections between language."
https://openalex.org/W2963871344,https://doi.org/10.3115/v1/n15-1016,Combining Language and Vision with a Multimodal Skip-gram Model,2015,"We extend the SKIP-GRAM model of Mikolov et al. (2013a) by taking visual information into account. Like SKIP-GRAM, our multimodal models (MMSKIP-GRAM) build vector-based word representations learning to predict linguistic contexts in text corpora. However, for a restricted set words, are also exposed objects they denote (extracted from natural images), and must features jointly. The MMSKIP-GRAM achieve good performance on variety semantic benchmarks. Moreover, since propagate all we use them improve image labeling retrieval zero-shot setup, where test concepts never seen during training. Finally, discover intriguing properties abstract paving way realistic implementations embodied theories meaning."
https://openalex.org/W2963912736,https://doi.org/10.18653/v1/p18-1216,Joint Embedding of Words and Labels for Text Classification,2018,"Word embeddings are effective intermediate representations for capturing semantic regularities between words, when learning the of text sequences. We propose to view classification as a label-word joint embedding problem: each label is embedded in same space with word vectors. introduce an attention framework that measures compatibility sequences and labels. The learned on training set labeled samples ensure that, given sequence, relevant words weighted higher than irrelevant ones. Our method maintains interpretability embeddings, enjoys built-in ability leverage alternative sources information, addition input Extensive results several large datasets show proposed outperforms state-of-the-art methods by margin, terms both accuracy speed."
https://openalex.org/W2964212344,https://doi.org/10.18653/v1/p16-1220,Question Answering on Freebase via Relation Extraction and Textual Evidence,2016,"Existing knowledge-based question answering systems often rely on small annotated training data. While shallow methods like relation extraction are robust to data scarcity, they less expressive than the deep meaning representation semantic parsing, thereby failing at questions involving multiple constraints. Here we alleviate this problem by empowering a method with additional evidence from Wikipedia. We first present neural network based extractor retrieve candidate answers Freebase, and then infer over Wikipedia validate these answers. Experiments WebQuestions dataset show that our achieves an F_1 of 53.3%, substantial improvement state-of-the-art."
https://openalex.org/W2970295111,https://doi.org/10.18653/v1/w19-5333,Facebook FAIR’s WMT19 News Translation Task Submission,2019,"This paper describes Facebook AI’s submission to WMT20 shared news translation task. We focus on the low resource setting and participate in two language pairs, Tamil English Inuktitut English, where there are limited out-of-domain bitext monolingual data. approach problem using main strategies, leveraging all available data adapting system target domain. explore techniques that leverage from languages, such as self-supervised model pretraining, multilingual models, augmentation, reranking. To better adapt test domain, we dataset tagging fine-tuning in-domain observe different provide varied improvements based of pair. Based finding, integrate these into one training pipeline. For En->Ta, an unconstrained setup with additional show further improvement can be obtained. On set, our best submitted systems achieve 21.5 13.7 BLEU for Ta->En En->Ta respectively, 27.9 13.0 Iu->En En->Iu respectively."
https://openalex.org/W2970550868,https://doi.org/10.18653/v1/d19-1588,BERT for Coreference Resolution: Baselines and Analysis,2019,"We apply BERT to coreference resolution, achieving a new state of the art on GAP (+11.5 F1) and OntoNotes (+3.9 benchmarks. A qualitative analysis model predictions indicates that, compared ELMo BERT-base, BERT-large is particularly better at distinguishing between related but distinct entities (e.g., President CEO), that there still room for improvement in modeling document-level context, conversations, mention paraphrasing. will release all code trained models upon publication."
https://openalex.org/W3035665735,https://doi.org/10.1007/s10462-020-09854-1,Deep semantic segmentation of natural and medical images: a review,2021,"The semantic image segmentation task consists of classifying each pixel an into instance, where instance corresponds to a class. This is part the concept scene understanding or better explaining global context image. In medical analysis domain, can be used for image-guided interventions, radiotherapy, improved radiological diagnostics. this review, we categorize leading deep learning-based and non-medical solutions six main groups architectural, data synthesis-based, loss function-based, sequenced models, weakly supervised, multi-task methods and provide comprehensive review contributions in these groups. Further, group, analyze variant discuss limitations current approaches present potential future research directions segmentation."
https://openalex.org/W2250594687,https://doi.org/10.18653/v1/d15-1299,ASTD: Arabic Sentiment Tweets Dataset,2015,"This paper introduces ASTD, an Arabic social sentiment analysis dataset gathered from Twitter. It consists of about 10,000 tweets which are classified as objective, subjective positive, negative, and mixed. We present the properties statistics dataset, run experiments using standard partitioning dataset. Our provide benchmark results for 4 way classification on"
https://openalex.org/W2250709962,https://doi.org/10.18653/v1/d15-1064,Named Entity Recognition for Chinese Social Media with Jointly Trained Embeddings,2015,"We consider the task of named entity recognition for Chinese social media. The long line work in NER has focused on formal domains, and media been largely restricted to English. present a new corpus Weibo messages annotated both name nominal mentions. Additionally, we evaluate three types neural embeddings representing text. Finally, propose joint training objective that makes use (NER) labeled unlabeled raw Our methods yield 9% improvement over stateof-the-art baseline."
https://openalex.org/W2571175805,https://doi.org/10.1109/cvpr.2017.375,A Joint Speaker-Listener-Reinforcer Model for Referring Expressions,2017,"Referring expressions are natural language constructions used to identify particular objects within a scene. In this paper, we propose unified framework for the tasks of referring expression comprehension and generation. Our model is composed three modules: speaker, listener, reinforcer. The speaker generates expressions, listener comprehends reinforcer introduces reward function guide sampling more discriminative expressions. listener-speaker modules trained jointly in an end-to-end learning framework, allowing be aware one another during while also benefiting from reinforcer's feedback. We demonstrate that training achieves state-of-the-art results both generation on datasets. Project demo page: https://vision.cs.unc.edu/refer"
https://openalex.org/W2593560537,https://doi.org/10.1109/tkde.2018.2812203,Automated Phrase Mining from Massive Text Corpora,2018,"As one of the fundamental tasks in text analysis, phrase mining aims at extracting quality phrases from a corpus. Phrase is important various such as information extraction/retrieval, taxonomy construction, and topic modeling. Most existing methods rely on complex, trained linguistic analyzers, thus likely have unsatisfactory performance corpora new domains genres without extra but expensive adaption. Recently, few data-driven been developed successfully for extraction massive domain-specific text. However, none state-of-the-art models fully automated because they require human experts designing rules or labeling phrases. Since can easily obtain many public knowledge bases to scale that much larger than produced by experts, this paper, we propose novel framework mining, AutoPhrase, which leverages large amount high-quality an effective way achieves better compared limited labeled In addition, develop POS-guided phrasal segmentation model, incorporates shallow syntactic part-of-speech (POS) tags further enhance performance, when POS tagger available. Note that, AutoPhrase support any language long general base (e.g., Wikipedia) available, while benefiting from, not requiring, tagger. Compared methods, method has shown significant improvements effectiveness five real-world datasets across different languages."
https://openalex.org/W2605243085,https://doi.org/10.18653/v1/d17-1062,Sentence Simplification with Deep Reinforcement Learning,2017,"Sentence simplification aims to make sentences easier read and understand. Most recent approaches draw on insights from machine translation learn rewrites monolingual corpora of complex simple sentences. We address the problem with an encoder-decoder model coupled a deep reinforcement learning framework. Our model, which we call DRESS (as shorthand for Deep REinforcement Simplification), explores space possible simplifications while optimize reward function that encourages outputs are simple, fluent, preserve meaning input. Experiments three datasets demonstrate our outperforms competitive systems."
https://openalex.org/W2891534142,https://doi.org/10.18653/v1/d18-1325,Document-Level Neural Machine Translation with Hierarchical Attention Networks,2018,"Neural Machine Translation (NMT) can be improved by including document-level contextual information. For this purpose, we propose a hierarchical attention model to capture the context in structured and dynamic manner. The is integrated original NMT architecture as another level of abstraction, conditioning on model’s own previous hidden states. Experiments show that significantly improves BLEU score over strong baseline with state-of-the-art context-aware methods, both encoder decoder benefit from complementary ways."
https://openalex.org/W2963859254,https://doi.org/10.14722/ndss.2019.23138,TextBugger: Generating Adversarial Text Against Real-world Applications,2019,"Deep Learning-based Text Understanding (DLTU) is the backbone technique behind various applications, including question answering, machine translation, and text classification. Despite its tremendous popularity, security vulnerabilities of DLTU are still largely unknown, which highly concerning given increasing use in security-sensitive applications such as sentiment analysis toxic content detection. In this paper, we show that inherently vulnerable to adversarial attacks, maliciously crafted texts trigger target systems services misbehave. Specifically, present TextBugger, a general attack framework for generating texts. contrast prior works, TextBugger differs significant ways: (i) effective -- it outperforms state-of-the-art attacks terms success rate; (ii) evasive preserves utility benign text, with 94.9\% correctly recognized by human readers; (iii) efficient generates computational complexity sub-linear length. We empirically evaluate on set real-world used detection, demonstrating effectiveness, evasiveness, efficiency. For instance, achieves 100\% rate IMDB dataset based Amazon AWS Comprehend within 4.61 seconds 97\% semantic similarity. further discuss possible defense mechanisms mitigate adversary's potential countermeasures, leads promising directions research."
https://openalex.org/W2964236999,https://doi.org/10.18653/v1/p16-1056,Generating Factoid Questions With Recurrent Neural Networks: The 30M Factoid Question-Answer Corpus,2016,"Over the past decade, large-scale supervised learning corpora have enabled machine researchers to make substantial advances. However, this date, there are no questionanswer available. In paper we present 30M Factoid QuestionAnswer Corpus, an enormous pair corpus produced by applying a novel neural network architecture on knowledge base Freebase transduce facts into natural language questions. The question-answer pairs evaluated both human evaluators and using automatic evaluation metrics, including well-established translation sentence similarity metrics. Across all criteria questiongeneration model outperforms competing template-based baseline. Furthermore, when presented evaluators, generated questions appear be comparable in quality real human-generated * First authors. ◦ Email: {iulian.vlad.serban,caglar.gulcehre, sungjin.ahn,sarath.chandar.anbil.parthipan, aaron.courville,yoshua.bengio}@umontreal.ca alberto.garcia-duran@utc.fr † CIFAR Senior Fellow"
https://openalex.org/W2984582583,https://doi.org/10.18653/v1/d19-1585,"Entity, Relation, and Event Extraction with Contextualized Span Representations",2019,"We examine the capabilities of a unified, multi-task framework for three information extraction tasks: named entity recognition, relation extraction, and event extraction. Our (called DyGIE++) accomplishes all tasks by enumerating, refining, scoring text spans designed to capture local (within-sentence) global (cross-sentence) context. achieves state-of-the-art results across tasks, on four datasets from variety domains. perform experiments comparing different techniques construct span representations. Contextualized embeddings like BERT well at capturing relationships among entities in same or adjacent sentences, while dynamic graph updates model long-range cross-sentence relationships. For instance, propagating representations via predicted coreference links can enable disambiguate challenging mentions. code is publicly available https://github.com/dwadden/dygiepp be easily adapted new datasets."
https://openalex.org/W2251805006,https://doi.org/10.3115/v1/p15-1130,Predicting Polarities of Tweets by Composing Word Embeddings with Long Short-Term Memory,2015,"In this paper, we introduce Long ShortTerm Memory (LSTM) recurrent network for twitter sentiment prediction. With the help of gates and constant error carousels in memory block structure, model could handle interactions between words through a flexible compositional function. Experiments on public noisy labelled data show that our outperforms several feature-engineering approaches, with result comparable to current best data-driven technique. According evaluation generated negation phrase test set, proposed architecture doubles performance non-neural based bag-of-word features. Furthermore, special functions (such as transition) are distinguished dissimilarities opposite magnified. An interesting case study expression processing shows promising potential dealing complex phrases."
https://openalex.org/W2509282593,https://doi.org/10.18653/v1/w16-2346,A Shared Task on Multimodal Machine Translation and Crosslingual Image Description,2016,"This paper introduces and summarises the findings of a new shared task at intersection Natural Language Processing Computer Vision: generation image descriptions in target language, given an and/or one or more different (source) language. challenge was organised along with Conference on Machine Translation (WMT16), called for system submissions two variants: (i) translation task, which source language description needs to be translated (optionally) additional cues from corresponding image, (ii) generated same image. In this first edition 16 systems were submitted seven total 10 teams."
https://openalex.org/W2604610161,https://doi.org/10.1609/aaai.v31i1.10953,Distant Supervision for Relation Extraction with Sentence-Level Attention and Entity Descriptions,2017,"Distant supervision for relation extraction is an efficient method to scale very large corpora which contains thousands of relations. However, the existing approaches have flaws on selecting valid instances and lack background knowledge about entities. In this paper, we propose a sentence-level attention model select instances, makes full use information from bases. And extract entity descriptions Freebase Wikipedia pages supplement our task. The not only provides more predicting relations, but also brings better representations module. We conduct three experiments widely used dataset experimental results show that approach outperforms all baseline systems significantly."
https://openalex.org/W2799125718,https://doi.org/10.1016/j.eswa.2018.07.032,Joint entity recognition and relation extraction as a multi-head selection problem,2018,"State-of-the-art models for joint entity recognition and relation extraction strongly rely on external natural language processing (NLP) tools such as POS (part-of-speech) taggers dependency parsers. Thus, the performance of depends quality features obtained from these NLP tools. However, are not always accurate various languages contexts. In this paper, we propose a neural model which performs simultaneously, without need any manually extracted or use tool. Specifically, task using CRF (Conditional Random Fields) layer multi-head selection problem (i.e., potentially identify multiple relations each entity). We present an extensive experimental setup, to demonstrate effectiveness our method datasets contexts news, biomedical, real estate) English, Dutch). Our outperforms previous that automatically features, while it within reasonable margin feature-based models, even beats them."
https://openalex.org/W2802105481,https://doi.org/10.18653/v1/s18-2005,Examining Gender and Race Bias in Two Hundred Sentiment Analysis Systems,2018,"Automatic machine learning systems can inadvertently accentuate and perpetuate inappropriate human biases. Past work on examining biases has largely focused just individual systems. Further, there is no benchmark dataset for in Here the first time, we present Equity Evaluation Corpus (EEC), which consists of 8,640 English sentences carefully chosen to tease out towards certain races genders. We use examine 219 automatic sentiment analysis that took part a recent shared task, SemEval-2018 Task 1 ‘Affect Tweets’. find several show statistically significant bias; is, they consistently provide slightly higher intensity predictions one race or gender. make EEC freely available."
https://openalex.org/W2949212908,https://doi.org/10.18653/v1/p19-1136,GraphRel: Modeling Text as Relational Graphs for Joint Entity and Relation Extraction,2019,"In this paper, we present GraphRel, an end-to-end relation extraction model which uses graph convolutional networks (GCNs) to jointly learn named entities and relations. contrast previous baselines, consider the interaction between relations via a 2nd-phase relation-weighted GCN better extract Linear dependency structures are both used sequential regional features of text, complete word is further utilized implicit among all pairs text. With graph-based approach, prediction for overlapping substantially improved over approaches. We evaluate GraphRel on two public datasets: NYT WebNLG. Results show that maintains high precision while increasing recall substantially. Also, outperforms work by 3.2% 5.8% (F1 score), achieving new state-of-the-art extraction."
https://openalex.org/W2962932155,https://doi.org/10.1080/0952813x.2017.1409284,Challenges in discriminating profanity from hate speech,2017,"In this study, we approach the problem of distinguishing general profanity from hate speech in social media, something which has not been widely considered. Using a new dataset annotated specifical..."
https://openalex.org/W2971034336,https://doi.org/10.18653/v1/d19-1051,Neural Text Summarization: A Critical Evaluation,2019,"Text summarization aims at compressing long documents into a shorter form that conveys the most important parts of original document. Despite increased interest in community and notable research effort, progress on benchmark datasets has stagnated. We critically evaluate key ingredients current setup: datasets, evaluation metrics, models, highlight three primary shortcomings: 1) automatically collected leave task underconstrained may contain noise detrimental to training evaluation, 2) protocol is weakly correlated with human judgment does not account for characteristics such as factual correctness, 3) models overfit layout biases offer limited diversity their outputs."
https://openalex.org/W2738015883,https://doi.org/10.48550/arxiv.1707.07328,Adversarial Examples for Evaluating Reading Comprehension Systems,2017,"Standard accuracy metrics indicate that reading comprehension systems are making rapid progress, but the extent to which these truly understand language remains unclear. To reward with real understanding abilities, we propose an adversarial evaluation scheme for Stanford Question Answering Dataset (SQuAD). Our method tests whether can answer questions about paragraphs contain adversarially inserted sentences, automatically generated distract computer without changing correct or misleading humans. In this setting, of sixteen published models drops from average $75\%$ F1 score $36\%$; when adversary is allowed add ungrammatical sequences words, on four decreases further $7\%$. We hope our insights will motivate development new more precisely."
https://openalex.org/W2795619303,https://doi.org/10.1109/cvpr.2018.00584,AON: Towards Arbitrarily-Oriented Text Recognition,2018,"Recognizing text from natural images is a hot research topic in computer vision due to its various applications. Despite the enduring of several decades on optical character recognition (OCR), recognizing texts still challenging task. This because scene are often irregular (e.g. curved, arbitrarily-oriented or seriously distorted) arrangements, which have not yet been well addressed literature. Existing methods mainly work with regular (horizontal and frontal) cannot be trivially generalized handle texts. In this paper, we develop arbitrary orientation network (AON) directly capture deep features texts, combined into an attention-based decoder generate sequence. The whole can trained end-to-end by using only word-level annotations. Extensive experiments benchmarks, including CUTE80, SVT-Perspective, IIIT5k, SVT ICDAR datasets, show that proposed AON-based method achieves the-state-of-the-art performance comparable major existing datasets."
https://openalex.org/W2890166583,https://doi.org/10.18653/v1/d18-1424,Paragraph-level Neural Question Generation with Maxout Pointer and Gated Self-attention Networks,2018,"Question generation, the task of automatically creating questions that can be answered by a certain span text within given passage, is important for question-answering and conversational systems in digital assistants such as Alexa, Cortana, Google Assistant Siri. Recent sequence to neural models have outperformed previous rule-based systems. Existing mainly focused on using one or two sentences input. Long has posed challenges question generation – worse performances were reported if whole paragraph (with multiple sentences) In reality, however, it often requires context order generate high quality questions. this paper, we propose maxout pointer mechanism with gated self-attention encoder address processing long inputs generation. With sentence-level inputs, our model outperforms approaches either paragraph-level inputs. Furthermore, effectively utilize paragraphs pushing state-of-the-art result from 13.9 16.3 (BLEU_4)."
https://openalex.org/W2150719495,https://doi.org/10.1111/cogs.12171,Processing Scalar Implicature: A Constraint-Based Approach,2015,"Three experiments investigated the processing of implicature associated with some using a ""gumball paradigm."" On each trial, participants saw an image gumball machine upper chamber 13 gumballs and empty lower chamber. Gumballs then dropped to evaluated statements, such as ""You got gumballs."" Experiment 1 established that is less natural for reference small sets (1, 2, 3 gumballs) unpartitioned (all compared intermediate (6-8). Partitive was than simple when used set. In including exact number descriptions lowered naturalness ratings but not size 3, from 2 predicted response times. The results are interpreted evidence Constraint-Based account scalar against both two-stage, Literal-First models pragmatic Default models."
https://openalex.org/W2252007242,https://doi.org/10.18653/v1/d15-1073,Neural Networks for Open Domain Targeted Sentiment,2015,"Open domain targeted sentiment is the joint information extraction task that finds target mentions together with towards each mention from a text corpus. The typically modeled as sequence labeling problem, and solved using state-of-the-art labelers such CRF. We empirically study effect of word embeddings automatic feature combinations on by extending CRF baseline neural networks, which have demonstrated large potentials for analysis. Results show model can give better results significantly increasing recall. In addition, we propose novel integration discrete features, combines their relative advantages, leading to higher compared both baselines."
https://openalex.org/W2739827909,https://doi.org/10.18653/v1/k17-3002,Stanford's Graph-based Neural Dependency Parser at the CoNLL 2017 Shared Task,2017,"This paper describes the neural dependency parser submitted by Stanford to CoNLL 2017 Shared Task on parsing Universal Dependencies. Our system uses relatively simple LSTM networks produce part of speech tags and labeled parses from segmented tokenized sequences words. In order address rare word problem that abounds in languages with complex morphology, we include a character-based representation an embeddings characters. was ranked first according all five relevant metrics for system: UPOS tagging (93.09%), XPOS (82.27%), unlabeled attachment score (81.30%), (76.30%), content (72.57%)."
https://openalex.org/W2963606136,https://doi.org/10.3115/v1/p15-1173,AutoExtend: Extending Word Embeddings to Embeddings for Synsets and Lexemes,2015,"Sascha Rothe, Hinrich Schütze. Proceedings of the 53rd Annual Meeting Association for Computational Linguistics and 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2015."
https://openalex.org/W2972735048,https://doi.org/10.18653/v1/w19-3504,Racial Bias in Hate Speech and Abusive Language Detection Datasets,2019,"Technologies for abusive language detection are being developed and applied with little consideration of their potential biases. We examine racial bias in five different sets Twitter data annotated hate speech language. train classifiers on these datasets compare the predictions tweets written African-American English those Standard American English. The results show evidence systematic all datasets, as trained them tend to predict that at substantially higher rates. If systems used field they will therefore have a disproportionate negative impact social media users. Consequently, may discriminate against groups who often targets abuse we trying detect."
https://openalex.org/W2890952074,https://doi.org/10.1109/tpami.2018.2889052,Deep Audio-visual Speech Recognition,2019,"The goal of this work is to recognise phrases and sentences being spoken by a talking face, with or without the audio. Unlike previous works that have focussed on recognising limited number words phrases, we tackle lip reading as an open-world problem - unconstrained natural language sentences, in wild videos. Our key contributions are: (1) compare two models for reading, one using CTC loss, other sequence-to-sequence loss. Both are built top transformer self-attention architecture; (2) investigate what extent complementary audio speech recognition, especially when signal noisy; (3) introduce publicly release new dataset audio-visual LRS2-BBC, consisting thousands from British television. train surpass performance all benchmark significant margin."
https://openalex.org/W2963078909,https://doi.org/10.18653/v1/n19-1063,On Measuring Social Biases in Sentence Encoders,2019,"The Word Embedding Association Test shows that GloVe and word2vec word embeddings exhibit human-like implicit biases based on gender, race, other social constructs (Caliskan et al., 2017). Meanwhile, research learning reusable text representations has begun to explore sentence-level texts, with some sentence encoders seeing enthusiastic adoption. Accordingly, we extend the measure bias in encoders. We then test several encoders, including state-of-the-art methods such as ELMo BERT, for studied prior work two important are difficult or impossible at level. observe mixed results suspicious patterns of sensitivity suggest test’s assumptions may not hold general. conclude by proposing directions future measuring"
https://openalex.org/W2963537482,https://doi.org/10.48550/arxiv.1711.03953,Breaking the Softmax Bottleneck: A High-Rank RNN Language Model,2017,"We formulate language modeling as a matrix factorization problem, and show that the expressiveness of Softmax-based models (including majority neural models) is limited by Softmax bottleneck. Given natural highly context-dependent, this further implies in practice with distributed word embeddings does not have enough capacity to model language. propose simple effective method address issue, improve state-of-the-art perplexities on Penn Treebank WikiText-2 47.69 40.68 respectively. The proposed also excels large-scale 1B Word dataset, outperforming baseline over 5.6 points perplexity."
https://openalex.org/W2963545005,https://doi.org/10.18653/v1/p18-1013,A Unified Model for Extractive and Abstractive Summarization using Inconsistency Loss,2018,"We propose a unified model combining the strength of extractive and abstractive summarization. On one hand, simple can obtain sentence-level attention with high ROUGE scores but less readable. other more complicated word-level dynamic to generate readable paragraph. In our model, is used modulate such that words in attended sentences are likely be generated. Moreover, novel inconsistency loss function introduced penalize between two levels attentions. By end-to-end training original losses models, we achieve state-of-the-art while being most informative summarization on CNN/Daily Mail dataset solid human evaluation."
https://openalex.org/W2979727876,https://doi.org/10.1109/cvpr.2019.01282,TOUCHDOWN: Natural Language Navigation and Spatial Reasoning in Visual Street Environments,2019,"We study the problem of jointly reasoning about language and vision through a navigation spatial task. introduce Touchdown task dataset, where an agent must first follow instructions in real-life visual urban environment, then identify location described natural to find hidden object at goal position. The data contains 9,326 examples English descriptions paired with demonstrations. Empirical analysis shows presents open challenge existing methods, qualitative linguistic that displays richer use compared related resources."
https://openalex.org/W2890961898,https://doi.org/10.18653/v1/d18-1455,Open Domain Question Answering Using Early Fusion of Knowledge Bases and Text,2018,"Open Domain Question Answering (QA) is evolving from complex pipelined systems to end-to-end deep neural networks. Specialized models have been developed for extracting answers either text alone or Knowledge Bases (KBs) alone. In this paper we look at a more practical setting, namely QA over the combination of KB and entity-linked text, which appropriate when an incomplete available with large corpus. Building on recent advances in graph representation learning propose novel model, GRAFT-Net, question-specific subgraph containing entities relations. We construct suite benchmark tasks problem, varying difficulty questions, amount training data, completeness. show that GRAFT-Net competitive state-of-the-art tested using KBs alone, vastly outperforms existing methods combined setting."
https://openalex.org/W2963270153,https://doi.org/10.18653/v1/p18-2023,Analogical Reasoning on Chinese Morphological and Semantic Relations,2018,"Analogical reasoning is effective in capturing linguistic regularities. This paper proposes an analogical task on Chinese. After delving into Chinese lexical knowledge, we sketch 68 implicit morphological relations and 28 explicit semantic relations. A big balanced dataset CA8 then built for this task, including 17813 questions. Furthermore, systematically explore the influences of vector representations, context features, corpora reasoning. With experiments, proved to be a reliable benchmark evaluating word embeddings."
https://openalex.org/W2964352358,https://doi.org/10.18653/v1/d16-1046,How Transferable are Neural Networks in NLP Applications?,2016,"Transfer learning is aimed to make use of valuable knowledge in a source domain help model performance target domain. It particularly important neural networks, which are very likely be overfitting. In some fields like image processing, many studies have shown the effectiveness network-based transfer learning. For NLP, however, existing only casually applied learning, and conclusions inconsistent. this paper, we conduct systematic case provide an illuminating picture on transferability networks NLP."
https://openalex.org/W2970442950,https://doi.org/10.18653/v1/d19-1107,Are We Modeling the Task or the Annotator? An Investigation of Annotator Bias in Natural Language Understanding Datasets,2019,"Crowdsourcing has been the prevalent paradigm for creating natural language understanding datasets in recent years. A common crowdsourcing practice is to recruit a small number of high-quality workers, and have them massively generate examples. Having only few workers majority examples raises concerns about data diversity, especially when freely sentences. In this paper, we perform series experiments showing these are evident three NLP datasets. We show that model performance improves training with annotator identifiers as features, models able recognize most productive annotators. Moreover, often do not generalize well from annotators did contribute set. Our findings suggest bias should be monitored during dataset creation, test set disjoint"
https://openalex.org/W2971220558,https://doi.org/10.18653/v1/d19-1464,Aspect-based Sentiment Classification with Aspect-specific Graph Convolutional Networks,2019,"Due to their inherent capability in semantic alignment of aspects and context words, attention mechanism Convolutional Neural Networks (CNNs) are widely applied for aspect-based sentiment classification. However, these models lack a account relevant syntactical constraints long-range word dependencies, hence may mistakenly recognize syntactically irrelevant contextual words as clues judging aspect sentiment. To tackle this problem, we propose build Graph Network (GCN) over the dependency tree sentence exploit information dependencies. Based on it, novel aspect-specific classification framework is raised. Experiments three benchmarking collections illustrate that our proposed model has comparable effectiveness range state-of-the-art models, further demonstrate both dependencies properly captured by graph convolution structure."
https://openalex.org/W2238728730,https://doi.org/10.1609/aaai.v29i1.9522,Topical Word Embeddings,2015,"Most word embedding models typically represent each using a single vector, which makes these indiscriminative for ubiquitous homonymy and polysemy. In order to enhance discriminativeness, we employ latent topic assign topics in the text corpus, learn topical embeddings (TWE) based on both words their topics. this way, contextual can be flexibly obtained measure similarity. We also build document representations, are more expressive than some widely-used such as models. experiments, evaluate TWE two tasks, similarity classification. The experimental results show that our outperform typical including multi-prototype version similarity, exceed other representative"
https://openalex.org/W2890502146,https://doi.org/10.18653/v1/d18-1015,Temporally Grounding Natural Sentence in Video,2018,"We introduce an effective and efficient method that grounds (i.e., localizes) natural sentences in long, untrimmed video sequences. Specifically, a novel Temporal GroundNet (TGN) is proposed to temporally capture the evolving fine-grained frame-by-word interactions between sentence. TGN sequentially scores set of temporal candidates ended at each frame based on exploited interactions, finally segment corresponding Unlike traditional methods treating overlapping segments separately sliding window fashion, aggregates historical information generates final grounding result one single pass. extensively evaluate our three public datasets with significant improvements over state-of-the-arts. further show consistent effectiveness efficiency through ablation study runtime test."
https://openalex.org/W2951520714,https://doi.org/10.48550/arxiv.1701.06547,Adversarial Learning for Neural Dialogue Generation,2017,"In this paper, drawing intuition from the Turing test, we propose using adversarial training for open-domain dialogue generation: system is trained to produce sequences that are indistinguishable human-generated utterances. We cast task as a reinforcement learning (RL) problem where jointly train two systems, generative model response sequences, and discriminator---analagous human evaluator in test--- distinguish between dialogues machine-generated ones. The outputs discriminator then used rewards model, pushing generate mostly resemble dialogues. addition describe {\em evaluation} uses success fooling an adversary evaluation metric, while avoiding number of potential pitfalls. Experimental results on several metrics, including evaluation, demonstrate adversarially-trained generates higher-quality responses than previous baselines."
https://openalex.org/W2788810909,https://doi.org/10.1609/aaai.v32i1.12048,Targeted Aspect-Based Sentiment Analysis via Embedding Commonsense Knowledge into an Attentive LSTM,2018,"Analyzing people’s opinions and sentiments towards certain aspects is an important task of natural language understanding. In this paper, we propose a novel solution to targeted aspect-based sentiment analysis, which tackles the challenges both analysis by exploiting commonsense knowledge. We augment long short-term memory (LSTM) network with hierarchical attention mechanism consisting target-level sentence-level attention. Commonsense knowledge sentiment-related concepts incorporated into end-to-end training deep neural for classification. order tightly integrate recurrent encoder, extension LSTM, termed Sentic LSTM. conduct experiments on two publicly released datasets, show that combination proposed architecture LSTM can outperform state-of-the-art methods in aspect tasks."
https://openalex.org/W2799020610,https://doi.org/10.1109/cvpr.2018.00812,Neural Sign Language Translation,2018,"Sign Language Recognition (SLR) has been an active research field for the last two decades. However, most to date considered SLR as a naive gesture recognition problem. seeks recognize sequence of continuous signs but neglects underlying rich grammatical and linguistic structures sign language that differ from spoken language. In contrast, we introduce Translation (SLT) Here, objective is generate translations videos, taking into account different word orders grammar. We formalize SLT in framework Neural Machine (NMT) both end-to-end pretrained settings (using expert knowledge). This allows us jointly learn spatial representations, model, mapping between To evaluate performance SLT, collected first publicly available Continuous dataset, RWTH-PHOENIX-Weather 2014T1. It provides gloss level annotations German videos weather broadcasts. Our dataset contains over .95M frames with >67K vocabulary >1K >99K words >2.8K. report quantitative qualitative results various setups underpin future this newly established field. The upper bound translation calculated at 19.26 BLEU-4, while our frame-level gloss-level tokenization networks were able achieve 9.58 18.13 respectively."
https://openalex.org/W2963706742,https://doi.org/10.18653/v1/p17-1194,Semi-supervised Multitask Learning for Sequence Labeling,2017,"We propose a sequence labeling framework with secondary training objective, learning to predict surrounding words for every word in the dataset. This language modeling objective incentivises system learn general-purpose patterns of semantic and syntactic composition, which are also useful improving accuracy on different tasks. The architecture was evaluated range datasets, covering tasks error detection learner texts, named entity recognition, chunking POS-tagging. novel provided consistent performance improvements benchmark, without requiring any additional annotated or unannotated data."
https://openalex.org/W3034727271,https://doi.org/10.1109/cvpr42600.2020.01045,12-in-1: Multi-Task Vision and Language Representation Learning,2020,"Much of vision-and-language research focuses on a small but diverse set independent tasks and supporting datasets often studied in isolation; however, the visually-grounded language understanding skills required for success at these overlap significantly. In this work, we investigate relationships between by developing large-scale, multi-task model. Our approach culminates single model 12 from four broad categories task including visual question answering, caption-based image retrieval, grounding referring expressions, multimodal verification. Compared to independently trained single-task models, represents reduction approximately 3 billion parameters 270 million while simultaneously improving performance 2.05 points average across tasks. We use our framework perform in-depth analysis effect joint training Further, show that finetuning task-specific models can lead further improvements, achieving or above state-of-the-art."
https://openalex.org/W2606333299,https://doi.org/10.1007/978-3-319-73618-1_56,Neural Question Generation from Text: A Preliminary Study,2017,"Automatic question generation aims to generate questions from a text passage where the generated can be answered by certain sub-spans of given passage. Traditional methods mainly use rigid heuristic rules transform sentence into related questions. In this work, we propose apply neural encoder-decoder model meaningful and diverse natural language sentences. The encoder reads input answer position, produce an answer-aware representation, which is fed decoder focused question. We conduct preliminary study on with SQuAD dataset, experiment results show that our method fluent"
https://openalex.org/W2607700676,https://doi.org/10.18653/v1/s17-2006,SemEval-2017 Task 8: RumourEval: Determining rumour veracity and support for rumours,2017,"Media is full of false claims. Even Oxford Dictionaries named post-truth as the word 2016. This makes it more important than ever to build systems that can identify veracity a story, and kind discourse there around it. RumourEval SemEval shared task aims handle rumours reactions them, in text. We present an annotation scheme, large dataset covering multiple topics - each having their own families claims replies use these pose two concrete challenges well results achieved by participants on challenges."
https://openalex.org/W2612773933,https://doi.org/10.18653/v1/d17-1277,Deep Joint Entity Disambiguation with Local Neural Attention,2017,"We propose a novel deep learning model for joint document-level entity disambiguation, which leverages learned neural representations. Key components are embeddings, attention mechanism over local context windows, and differentiable inference stage disambiguation. Our approach thereby combines benefits of with more traditional approaches such as graphical models probabilistic mention-entity maps. Extensive experiments show that we able to obtain competitive or state-of-the-art accuracy at moderate computational costs."
https://openalex.org/W2950670227,https://doi.org/10.18653/v1/p19-1500,Hierarchical Transformers for Multi-Document Summarization,2019,"In this paper, we develop a neural summarization model which can effectively process multiple input documents and distill Transformer architecture with the ability to encode in hierarchical manner. We represent cross-document relationships via an attention mechanism allows share information as opposed simply concatenating text spans processing them flat sequence. Our learns latent dependencies among textual units, but also take advantage of explicit graph representations focusing on similarity or discourse relations. Empirical results WikiSum dataset demonstrate that proposed brings substantial improvements over several strong baselines."
https://openalex.org/W2962805889,https://doi.org/10.18653/v1/w18-2706,Controllable Abstractive Summarization,2018,"Current models for document summarization disregard user preferences such as the desired length, style, entities that might be interested in, or how much of has already read. We present a neural model with simple but effective mechanism to enable users specify these high level attributes in order control shape final summaries better suit their needs. With input, our system can produce quality follow preferences. Without we set variables automatically – on full text CNN-Dailymail dataset, outperform state art abstractive systems (both terms F1-ROUGE1 40.38 vs. 39.53 F1-ROUGE and human evaluation."
https://openalex.org/W2964045325,https://doi.org/10.18653/v1/w17-5221,Explaining Recurrent Neural Network Predictions in Sentiment Analysis,2017,"Recently, a technique called Layer-wise Relevance Propagation (LRP) was shown to deliver insightful explanations in the form of input space relevances for un- derstanding feed-forward neural network classification decisions. In present work, we extend usage LRP recurrent networks. We propose specific propagation rule applicable multiplicative connections as they arise architectures such LSTMs and GRUs. apply our word-based bi-directional LSTM model on five-class sentiment prediction task, evaluate result- ing both qualitatively quantitatively, obtaining better results than gradient-based related method which used previous work."
https://openalex.org/W2964325845,https://doi.org/10.18653/v1/p16-1057,Latent Predictor Networks for Code Generation,2016,"Many language generation tasks require the production of text conditioned on both structured and unstructured inputs. We present a novel neural network architecture which generates an output sequence arbitrary number input functions. Crucially, our approach allows choice conditioning context granularity generation, for example characters or tokens, to be marginalised, thus permitting scalable effective training. Using this framework, we address problem generating programming code from mixed natural specification. create two new data sets paradigm derived collectible trading card games Magic Gathering Hearthstone. On these, third preexisting corpus, demonstrate that marginalising multiple predictors model outperform strong benchmarks."
https://openalex.org/W2518186251,https://doi.org/10.1162/tacl_a_00106,A Latent Variable Model Approach to PMI-based Word Embeddings,2016,"Semantic word embeddings represent the meaning of a via vector, and are created by diverse methods. Many use nonlinear operations on co-occurrence statistics, have hand-tuned hyperparameters reweighting This paper proposes new generative model, dynamic version log-linear topic model Mnih Hinton (2007). The methodological novelty is to prior compute closed form expressions for statistics. provides theoretical justification models like PMI, word2vec, GloVe, as well some hyperparameter choices. It also helps explain why low-dimensional semantic contain linear algebraic structure that allows solution analogies, shown Mikolov et al. (2013a) many subsequent papers. Experimental support provided assumptions, most important which latent vectors fairly uniformly dispersed in space."
https://openalex.org/W2899024931,https://doi.org/10.18653/v1/k18-2016,Universal Dependency Parsing from Scratch,2018,"This paper describes Stanford’s system at the CoNLL 2018 UD Shared Task. We introduce a complete neural pipeline that takes raw text as input, and performs all tasks required by shared task, ranging from tokenization sentence segmentation, to POS tagging dependency parsing. Our single submission achieved very competitive performance on big treebanks. Moreover, after fixing an unfortunate bug, our corrected would have placed 2nd, 1st, 3rd official evaluation metrics LAS, MLAS, BLEX, outperformed systems low-resource treebank categories large margin. further show effectiveness of different model components through extensive ablation studies."
https://openalex.org/W2912817604,https://doi.org/10.18653/v1/n19-4013,End-to-End Open-Domain Question Answering with,2019,"We demonstrate an end-to-end question answering system that integrates BERT with the open-source Anserini information retrieval toolkit. In contrast to most and reading comprehension models today, which operate over small amounts of input text, our best practices from IR a BERT-based reader identify answers large corpus Wikipedia articles in fashion. report improvements previous results on standard benchmark test collection, showing fine-tuning pretrained SQuAD is sufficient achieve high accuracy identifying answer spans."
https://openalex.org/W2963650529,https://doi.org/10.1109/cvpr.2017.469,Deep Variation-Structured Reinforcement Learning for Visual Relationship and Attribute Detection,2017,"Computers still struggle to understand the interdependency of objects in scene as a whole, e.g., relations between or their attributes. Existing methods often ignore global context cues capturing interactions among different object instances, and can only recognize handful types by exhaustively training individual detectors for all possible relationships. To capture such interdependency, we propose deep Variation-structured Re-inforcement Learning (VRL) framework sequentially discover relationships attributes whole image. First, directed semantic action graph is built using language priors provide rich compact representation correlations categories, predicates, Next, use variation-structured traversal over construct small, adaptive set each step based on current state historical actions. In particular, an ambiguity-aware mining scheme used resolve ambiguity categories that detector fails distinguish. We then make sequential predictions RL framework, incorporating embeddings previously extracted phrases vector. Our experiments Visual Relationship Detection (VRD) dataset large-scale Genome validate superiority VRL, which achieve significantly better detection results datasets involving thousands relationship attribute types. also demonstrate VRL able predict unseen embedded our learning shared nodes."
https://openalex.org/W3091588028,https://doi.org/10.1007/978-3-030-58577-8_8,Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks,2020,"Large-scale pre-training methods of learning cross-modal representations on image-text pairs are becoming popular for vision-language tasks. While existing simply concatenate image region features and text as input to the model be pre-trained use self-attention learn semantic alignments in a brute force manner, this paper, we propose new method Oscar (Object-Semantics Aligned Pre-training), which uses object tags detected images anchor points significantly ease alignments. Our is motivated by observation that salient objects an can accurately detected, often mentioned paired text. We pre-train public corpus 6.5 million text-image pairs, fine-tune it downstream tasks, creating state-of-the-arts six well-established understanding generation tasks (The code models released: https://github.com/microsoft/Oscar)."
https://openalex.org/W2221507685,https://doi.org/10.1109/iccv.2015.527,Learning Visual Clothing Style with Heterogeneous Dyadic Co-Occurrences,2015,"With the rapid proliferation of smart mobile devices, users now take millions photos every day. These include large numbers clothing and accessory images. We would like to answer questions 'What outfit goes well with this pair shoes?' To these types questions, one has go beyond learning visual similarity learn a notion compatibility across categories. In paper, we propose novel framework help questions. The main idea is feature transformation from images items into latent space that expresses compatibility. For transformation, use Siamese Convolutional Neural Network (CNN) architecture, where training examples are pairs either compatible or incompatible. model based on co-occurrence in large-scale user behavior data, particular co-purchase data Amazon.com. cross-category fit, introduce strategic method sample heterogeneous dyads, i.e., two elements belong different high-level While approach applicable wide variety settings, focus representative problem style. Our results indicate proposed capable semantic information about style able generate outfits clothes, categories, together."
https://openalex.org/W2251920663,https://doi.org/10.3115/v1/p15-2124,Harnessing Context Incongruity for Sarcasm Detection,2015,The relationship between context incongruity and sarcasm has been studied in linguistics. We present a computational system that harnesses as basis for detection. Our statistical classifiers incorporate two kinds of features: explicit implicit. show the benefit our features text forms tweets discussion forum posts. also outperforms past works (with Fscore improvement 10-20%). how can capture intersentential incongruity.
https://openalex.org/W2744012576,https://doi.org/10.1016/j.cnp.2017.07.002,A revised glossary of terms most commonly used by clinical electroencephalographers and updated proposal for the report format of the EEG findings. Revision 2017,2017,"This glossary includes the terms most commonly used in clinical EEG. It is based on previous proposals (Chatrian et al., 1974; Noachtar 1999) and necessary to describe EEG generate report. All phenomena should be described as precisely possible of frequency, amplitude, phase relation, waveform, localization, quantity, variability these parameters (Brazier 1961). The description independent recording such amplification, montages, computer program/display."
https://openalex.org/W2788667846,https://doi.org/10.1145/3178876.3186005,Large-Scale Hierarchical Text Classification with Recursively Regularized Deep Graph-CNN,2018,"Text classification to a hierarchical taxonomy of topics is common and practical problem. Traditional approaches simply use bag-of-words have achieved good results. However, when there are lot labels with different topical granularities, representation may not be enough. Deep learning models been proven effective automatically learn levels representations for image data. It interesting study what the best way represent texts. In this paper, we propose graph-CNN based deep model first convert texts graph-of-words, then graph convolution operations convolve word graph. Graph-of-words has advantage capturing non-consecutive long-distance semantics. CNN level To further leverage hierarchy labels, regularize architecture dependency among labels. Our results on both RCV1 NYTimes datasets show that can significantly improve large-scale text over traditional existing models."
https://openalex.org/W2791063712,https://doi.org/10.1002/wps.20491,Prediction of psychosis across protocols and risk cohorts using automated language analysis,2018,"Language and speech are the primary source of data for psychiatrists to diagnose treat mental disorders. In psychosis, very structure language can be disturbed, including semantic coherence (e.g., derailment tangentiality) syntactic complexity concreteness). Subtle disturbances in evident schizophrenia even prior first psychosis onset, during prodromal stages. Using computer-based natural processing analyses, we previously showed that, among English-speaking clinical ultra) high-risk youths, baseline reduction (the flow meaning speech) could predict subsequent onset with high accuracy. Herein, aimed cross-validate these automated linguistic analytic methods a second larger risk cohort, also English-speaking, discriminate from normal speech. We identified an machine-learning classifier - comprising decreased coherence, greater variance that reduced usage possessive pronouns had 83% accuracy predicting (intra-protocol), cross-validated 79% prediction original cohort (cross-protocol), 72% discriminating recent-onset patients healthy individuals. The was highly correlated manual predictors. Our findings support utility validity characterize semantics syntax across stages psychotic disorder. next steps will apply cohorts further test reproducibility, languages other than English, identify sources variability. This technology has potential improve outcome at-risk youths targets remediation preventive intervention. More broadly, analysis powerful tool diagnosis treatment neuropsychiatry."
https://openalex.org/W2962968458,https://doi.org/10.1109/cvpr.2018.00789,Multi-content GAN for Few-Shot Font Style Transfer,2018,"In this work, we focus on the challenge of taking partial observations highly-stylized text and generalizing to generate unobserved glyphs in ornamented typeface. To a set multi-content images following consistent style from very few examples, propose an end-to-end stacked conditional GAN model considering content along channels network layers. Our proposed transfers given contents unseen ones, capturing highly stylized fonts found real-world such as those movie posters or infographics. We seek transfer both typographic stylization (ex. serifs ears) well textual color gradients effects.) base our experiments collected data including 10,000 with different styles demonstrate effective generalization small number observed glyphs."
https://openalex.org/W2963655104,https://doi.org/10.18653/v1/p17-1053,Improved Neural Relation Detection for Knowledge Base Question Answering,2017,"Relation detection is a core component of many NLP applications including Knowledge Base Question Answering (KBQA). In this paper, we propose hierarchical recurrent neural network enhanced by residual learning which detects KB relations given an input question. Our method uses deep bidirectional LSTMs to compare questions and relation names via different levels abstraction. Additionally, simple KBQA system that integrates entity linking our proposed detector make the two components enhance each other. experimental results show approach not only achieves outstanding performance, but more importantly, it helps achieve state-of-the-art accuracy for both single-relation (SimpleQuestions) multi-relation (WebQSP) QA benchmarks."
https://openalex.org/W2963811339,https://doi.org/10.18653/v1/d16-1084,Stance Detection with Bidirectional Conditional Encoding,2016,"Stance detection is the task of classifying the
attitude expressed in a text towards target
such as “Climate Change Real Concern”
to be “positive”, “negative” or “neutral”. Previous
work has assumed that either target
is mentioned training data for
every target given. This paper considers the
more challenging version this task, where
targets are not always and no training
data available for test targets. We
experiment with conditional LSTM encoding,
which builds representation tweet that
is dependent on target, demonstrate
that it outperforms independent encoding
of target. Performance improves
even further when model is
augmented bidirectional encoding. The
method evaluated SemEval 2016
Task 6 Twitter Detection corpus and
achieves performance second best only to a
system trained semi-automatically labelled
tweets When such weak
supervision added, our approach achieves
state–of-the-art results."
https://openalex.org/W2741271950,https://doi.org/10.24963/ijcai.2017/406,Combining Knowledge with Deep Convolutional Neural Networks for Short Text Classification,2017,"Text classification is a fundamental task in NLP applications. Most existing work relied on either explicit or implicit text representation to address this problem. While these techniques well for sentences, they can not easily be applied short because of its shortness and sparsity. In paper, we propose framework based convolutional neural networks that combines representations classification. We first conceptualize as set relevant concepts using large taxonomy knowledge base. then obtain the embedding by coalescing words top pre-trained word vectors. further incorporate character level features into our model capture fine-grained subword information. Experimental results five commonly used datasets show proposed method significantly outperforms state-of-the-art methods."
https://openalex.org/W2962974452,https://doi.org/10.18653/v1/d17-2014,ParlAI: A Dialog Research Software Platform,2017,"We introduce ParlAI (pronounced “par-lay”), an open-source software platform for dialog research implemented in Python, available at http://parl.ai. Its goal is to provide a unified framework sharing, training and testing models; integration of Amazon Mechanical Turk data collection, human evaluation, online/reinforcement learning; repository machine learning models comparing with others’ models, improving upon existing architectures. Over 20 tasks are supported the first release, including popular datasets such as SQuAD, bAbI tasks, MCTest, WikiQA, QACNN, QADailyMail, CBT, Dialog, Ubuntu, OpenSubtitles VQA. Several integrated, neural memory networks, seq2seq attentive LSTMs."
https://openalex.org/W2963661253,https://doi.org/10.18653/v1/p16-1078,Tree-to-Sequence Attentional Neural Machine Translation,2016,"Most of the existing Neural Machine Translation (NMT) models focus on conversion sequential data and do not directly use syntactic information. We propose a novel end-to-end NMT model, extending sequence-to-sequence model with source-side phrase structure. Our has an attention mechanism that enables decoder to generate translated word while softly aligning it phrases as well words source sentence. Experimental results WAT'15 English-to-Japanese dataset demonstrate our proposed considerably outperforms attentional compares favorably state-of-the-art tree-to-string SMT system."
https://openalex.org/W2964303913,https://doi.org/10.1109/tpami.2017.2754246,FVQA: Fact-Based Visual Question Answering,2018,"Visual Question Answering (VQA) has attracted much attention in both computer vision and natural language processing communities, not least because it offers insight into the relationships between two important sources of information. Current datasets, models built upon them, have focused on questions which are answerable by direct analysis question image alone. The set such that require no external information to answer is interesting, but very limited. It excludes common sense, or basic factual knowledge answer, for example. Here we introduce FVQA (Fact-based VQA), a VQA dataset requires, supports, deeper reasoning. primarily contains answer. We thus extend conventional visual answering dataset, image-question-answer triplets, through additional image-question-answer-supporting fact tuples. Each supporting-fact represented as structural triplet, <Cat,CapableOf,ClimbingTrees>."
https://openalex.org/W2970785793,https://doi.org/10.18653/v1/d19-1053,MoverScore: Text Generation Evaluating with Contextualized Embeddings and Earth Mover Distance,2019,"A robust evaluation metric has a profound impact on the development of text generation systems. desirable compares system output against references based their semantics rather than surface forms. In this paper we investigate strategies to encode and reference texts devise that shows high correlation with human judgment quality. We validate our new metric, namely MoverScore, number tasks including summarization, machine translation, image captioning, data-to-text generation, where outputs are produced by variety neural non-neural Our findings suggest metrics combining contextualized representations distance measure perform best. Such also demonstrate strong generalization capability across tasks. For ease-of-use make available as web service."
https://openalex.org/W2962990575,https://doi.org/10.18653/v1/d18-1302,Reducing Gender Bias in Abusive Language Detection,2018,"Abusive language detection models tend to have a problem of being biased toward identity words certain group people because imbalanced training datasets. For example, “You are good woman” was considered “sexist” when trained on an existing dataset. Such model bias is obstacle for be robust enough practical use. In this work, we measure them with different datasets, while analyzing the effect pre-trained word embeddings and architectures. We also experiment three mitigation methods: (1) debiased embeddings, (2) gender swap data augmentation, (3) fine-tuning larger corpus. These methods can effectively reduce by 90-98% extended correct in other scenarios."
https://openalex.org/W2962996600,https://doi.org/10.18653/v1/d15-1229,LCSTS: A Large Scale Chinese Short Text Summarization Dataset,2015,"Automatic text summarization is widely regarded as the highly difficult problem, partially because of lack large data set. Due to great challenge constructing scale summaries for full text, in this paper, we introduce a corpus Chinese short dataset constructed from microblogging website Sina Weibo, which released public 1 . This consists over 2 million real texts with given by author each text. We also manually tagged relevance 10,666 their corresponding texts. Based on corpus, recurrent neural network summary generation and achieve promising results, not only shows usefulness proposed research, but provides baseline further research topic."
https://openalex.org/W2964232431,https://doi.org/10.18653/v1/n16-1018,Counter-fitting Word Vectors to Linguistic Constraints,2016,"©2016 Association for Computational Linguistics.In this work, we present a novel counter-fitting method which injects antonymy and synonymy constraints into vector space representations in order to improve the vectors' capability judging semantic similarity. Applying publicly available pre-trained word vectors leads new state of art performance on SimLex-999 dataset. We also show how can be used tailor downstream task dialogue tracking, resulting robust improvements across different domains."
https://openalex.org/W3035503910,https://doi.org/10.18653/v1/2020.acl-main.408,ERASER: A Benchmark to Evaluate Rationalized NLP Models,2020,"State-of-the-art models in NLP are now predominantly based on deep neural networks that opaque terms of how they come to make predictions. This limitation has increased interest designing more interpretable for reveal the `reasoning' behind model outputs. But work this direction been conducted different datasets and tasks with correspondingly unique aims metrics; makes it difficult track progress. We propose Evaluating Rationales And Simple English Reasoning (ERASER\, a benchmark advance research NLP. comprises multiple which human annotations ``rationales'' (supporting evidence) have collected. several metrics aim capture well rationales provided by align rationales, also faithful these (i.e., degree influenced corresponding predictions). Our hope is releasing facilitates progress systems. The benchmark, code, documentation available at https://www.eraserbenchmark.com/"
https://openalex.org/W2119615570,https://doi.org/10.1109/icassp.2015.7178826,Constructing long short-term memory based deep recurrent neural networks for large vocabulary speech recognition,2015,"Long short-term memory (LSTM) based acoustic modeling methods have recently been shown to give state-of-the-art performance on some speech recognition tasks. To achieve a further improvement, in this research, deep extensions LSTM are investigated considering that hierarchical model has turned out be more efficient than shallow one. Motivated by previous research constructing recurrent neural networks (RNNs), alternative architectures proposed and empirically evaluated large vocabulary conversational telephone task. Meanwhile, regarding multi-GPU devices, the training process for is introduced discussed. Experimental results demonstrate benefit from depth yield"
https://openalex.org/W2746348275,https://doi.org/10.1063/1.4999454,High-throughput powder diffraction measurement system consisting of multiple MYTHEN detectors at beamline BL02B2 of SPring-8,2017,"In this study, we developed a user-friendly automatic powder diffraction measurement system for Debye-Scherrer geometry using capillary sample at beamline BL02B2 of SPring-8. The consists six one-dimensional solid-state (MYTHEN) detectors, compact auto-sampler, wide-range temperature control systems, and gas handling system. This enables to do the dependence patterns multiple samples. We introduced two modes in MYTHEN new attachments environment such as can offer situ and/or time-resolved measurements an extended range between 25 K 1473 various atmospheres pressures. results commissioning performance reference materials (NIST CeO2 674b Si 640c), V2O3 Ti2O3, nanoporous coordination polymer are presented."
https://openalex.org/W2963290645,https://doi.org/10.1609/aaai.v33i01.33019299,Talking Face Generation by Adversarially Disentangled Audio-Visual Representation,2019,"Talking face generation aims to synthesize a sequence of images that correspond clip speech. This is challenging task because appearance variation and semantics speech are coupled together in the subtle movements talking regions. Existing works either construct specific model on subjects or transformation between lip motion In this work, we integrate both aspects enable arbitrary-subject by learning disentangled audio-visual representation. We find actually composition subject-related information speech-related information. These two spaces then explicitly through novel associative-and-adversarial training process. representation has an advantage where audio video can serve as inputs for generation. Extensive experiments show proposed approach generates realistic sequences arbitrary with much clearer patterns than previous work. also demonstrate learned extremely useful tasks automatic reading audio-video retrieval."
https://openalex.org/W2963560594,https://doi.org/10.1609/aaai.v32i1.11928,Deep Semantic Role Labeling With Self-Attention,2018,"Semantic Role Labeling (SRL) is believed to be a crucial step towards natural language understanding and has been widely studied. Recent years, end-to-end SRL with recurrent neural networks (RNN) gained increasing attention. However, it remains major challenge for RNNs handle structural information long range dependencies. In this paper, we present simple effective architecture which aims address these problems. Our model based on self-attention can directly capture the relationships between two tokens regardless of their distance. single achieves F1=83.4 CoNLL-2005 shared task dataset F1=82.7 CoNLL-2012 dataset, outperforms previous state-of-the-art results by 1.8 1.0 F1 score respectively. Besides, our computationally efficient, parsing speed 50K per second Titan X GPU."
https://openalex.org/W2964120214,https://doi.org/10.1109/cvpr.2018.00645,Learning Semantic Concepts and Order for Image and Sentence Matching,2018,"Image and sentence matching has made great progress recently, but it remains challenging due to the large visual-semantic discrepancy. This mainly arises from that representation of pixel-level image usually lacks high-level semantic information as in its matched sentence. In this work, we propose a semantic-enhanced model, which can improve by learning concepts then organizing them correct order. Given an image, first use multi-regional multi-label CNN predict concepts, including objects, properties, actions, etc. Then, considering different orders lead diverse meanings, context-gated generation scheme for order learning. It simultaneously uses global context containing concept relations reference groundtruth supervision. After obtaining improved representation, learn with conventional LSTM, jointly perform model Extensive experiments demonstrate effectiveness our learned order, achieving state-of-the-art results on two public benchmark datasets."
https://openalex.org/W1551842868,https://doi.org/10.18653/v1/d15-1062,Semantic Relation Classification via Convolutional Neural Networks with Simple Negative Sampling,2015,"Syntactic features play an essential role in identifying relationship a sentence. Previous neural network models often suffer from irrelevant information introduced when subjects and objects are long distance. In this paper, we propose to learn more robust relation representations the shortest dependency path through convolution network. We further straightforward negative sampling strategy improve assignment of objects. Experimental results show that our method outperforms state-of-the-art methods on SemEval-2010 Task 8 dataset."
https://openalex.org/W1808966389,https://doi.org/10.1016/j.jvcir.2015.10.012,Beyond pixels: A comprehensive survey from bottom-up to semantic image segmentation and cosegmentation,2016,"Abstract Image segmentation refers to the process divide an image into meaningful non-overlapping regions according human perception, which has become a classic topic since early ages of computer vision. A lot research been conducted and resulted in many applications. While algorithms exist, there are only few sparse outdated summarizations available. Thus, this paper, we aim provide comprehensive review recent progress field. Covering 190 publications, give overview broad topics including not unsupervised methods, but also weakly-/semi-supervised methods fully-supervised methods. In addition, existing influential datasets evaluation metrics. We suggest some design choices directions for future segmentation."
https://openalex.org/W2606982687,https://doi.org/10.1109/cvpr.2017.149,TGIF-QA: Toward Spatio-Temporal Reasoning in Visual Question Answering,2017,"Vision and language understanding has emerged as a subject undergoing intense study in Artificial Intelligence. Among many tasks this line of research, visual question answering (VQA) been one the most successful ones, where goal is to learn model that understands content at region-level details finds their associations with pairs questions answers natural form. Despite rapid progress past few years, existing work VQA have focused primarily on images. In paper, we focus extending video domain contribute literature three important ways. First, propose new designed specifically for VQA, which require spatio-temporal reasoning from videos answer correctly. Next, introduce large-scale dataset named TGIF-QA extends our tasks. Finally, dual-LSTM based approach both spatial temporal attention, show its effectiveness over conventional techniques through empirical evaluations."
https://openalex.org/W2963021258,https://doi.org/10.18653/v1/n18-1080,Simultaneously Self-Attending to All Mentions for Full-Abstract Biological Relation Extraction,2018,"Most work in relation extraction forms a prediction by looking at short span of text within single sentence containing entity pair mention. This approach often does not consider interactions across mentions, requires redundant computation for each mention pair, and ignores relationships expressed boundaries. These problems are exacerbated the document- (rather than sentence-) level annotation common biological text. In response, we propose model which simultaneously predicts between all pairs document. We form pairwise predictions over entire paper abstracts using an efficient self-attention encoder. All-pairs scores allow us to perform multi-instance learning aggregating mentions representations. further adapt settings without mention-level jointly training predict named entities adding corpus weakly labeled data. experiments on two Biocreative benchmark datasets, achieve state art performance V Chemical Disease Relation dataset models external KB resources. also introduce new order magnitude larger existing human-annotated information datasets more accurate distantly supervised alternatives."
https://openalex.org/W2963299604,https://doi.org/10.1109/icdar.2017.157,Total-Text: A Comprehensive Dataset for Scene Text Detection and Recognition,2017,"Text in curve orientation, despite being one of the common text orientations real world environment, has close to zero existence well received scene datasets such as ICDAR'13 and MSRA-TD500. The main motivation Total-Text is fill this gap facilitate a new research direction for community. On top conventional horizontal multi-oriented text, it features curved-oriented text. highly diversified orientations, more than half its images have combination two orientations. Recently, breed solutions that casted detection segmentation problem demonstrated their effectiveness against In order evaluate robustness curved we fine-tuned DeconvNet benchmark on Total-Text. with annotation available at https://github.com/cs-chan/Total-Text-Dataset."
https://openalex.org/W2963955422,https://doi.org/10.1109/cvpr.2018.00115,Zero-Shot Visual Recognition Using Semantics-Preserving Adversarial Embedding Networks,2018,"We propose a novel framework called Semantics-Preserving Adversarial Embedding Network (SP-AEN) for zero-shot visual recognition (ZSL), where test images and their classes are both unseen during training. SP-AEN aims to tackle the inherent problem - semantic loss in prevailing family of embedding-based ZSL, some semantics would be discarded training if they non-discriminative classes, but could become critical recognizing classes. Specifically, prevents by introducing an independent visual-to-semantic space embedder which disentangles into two subspaces arguably conflicting objectives: classification reconstruction. Through adversarial learning subspaces, can transfer from reconstructive subspace discriminative one, accomplishing improved Comparing"
https://openalex.org/W2963993537,https://doi.org/10.18653/v1/n18-1032,Universal Neural Machine Translation for Extremely Low Resource Languages,2018,"In this paper, we propose a new universal machine translation approach focusing on languages with limited amount of parallel data. Our proposed utilizes transfer-learning to share lexical and sentence level representations across multiple source into one target language. The part is shared through Universal Lexical Representation support multilingual word-level sharing. sentence-level sharing represented by model experts from all that the encoders other languages. This enables low-resource language utilize higher resource able achieve 23 BLEU Romanian-English WMT2016 using tiny corpus 6k sentences, compared 18 strong baseline system which uses training back-translation. Furthermore, show can almost 20 same dataset fine-tuning pre-trained multi-lingual in zero-shot setting."
https://openalex.org/W2517782820,https://doi.org/10.18653/v1/p16-1044,Improved Representation Learning for Question Answer Matching,2016,"Passage-level question answer matching is a challenging task since it requires effective representations that capture the complex semantic relations between questions and answers. In this work, we propose series of deep learning models to address passage selection. To match answers accommodating their relations, unlike most previous work utilizes single structure, develop hybrid process text using both convolutional recurrent neural networks, combining merits on extracting linguistic information from structures. Additionally, also simple but attention mechanism for purpose constructing better according input question, which imperative modeling long sequences. The results two public benchmark datasets, InsuranceQA TREC-QA, show our proposed outperform variety strong baselines."
https://openalex.org/W2739874095,https://doi.org/10.18653/v1/p17-1017,Creating Training Corpora for NLG Micro-Planners,2017,"In this paper, we present a novel framework for semi-automatically creating linguistically challenging micro-planning data-to-text corpora from existing Knowledge Bases. Because our method pairs data of varying size and shape with texts ranging simple clauses to short texts, dataset created using provides benchmark microplanning. Another feature is that it can be applied any large scale knowledge base therefore used train learn KB verbalisers. We apply DBpedia compare the resulting Wen et al. 2016’s. show while al.’s more than twice larger ours, less diverse both in terms input text. thus propose corpus generation as sets which NLG models learned are capable handling complex interactions occurring during between lexicalisation, aggregation, surface realisation, referring expression sentence segmentation. To encourage researchers take up challenge, made available 21,855 data/text context WebNLG shared task."
https://openalex.org/W2747187574,https://doi.org/10.18653/v1/w17-3008,Abusive Language Detection on Arabic Social Media,2017,"In this paper, we present our work on detecting abusive language Arabic social media. We extract a list of obscene words and hashtags using common patterns used in offensive rude communications. also classify Twitter users according to whether they use any these or not their tweets. expand the classification, report results newly created dataset classified tweets (obscene, offensive, clean). make freely available for research, addition hashtags. are publicly releasing large corpus user comments that were deleted from popular news site due violations site’s rules guidelines."
https://openalex.org/W2757978590,https://doi.org/10.18653/v1/d17-1090,Question Generation for Question Answering,2017,"This paper presents how to generate questions from given passages using neural networks, where large scale QA pairs are automatically crawled and processed Community-QA website, used as training data. The contribution of the is 2-fold: First, two types question generation approaches proposed, one a retrieval-based method convolution network (CNN), other generation-based recurrent (RNN); Second, we show leverage generated improve existing answering systems. We evaluate our for answer sentence selection task on three benchmark datasets, including SQuAD, MS MARCO, WikiQA. Experimental results that, by an extra signal, significant improvement can be achieved."
https://openalex.org/W2888159079,https://doi.org/10.18653/v1/d18-1512,Has Machine Translation Achieved Human Parity? A Case for Document-level Evaluation,2018,"Recent research suggests that neural machine translation achieves parity with professional human on the WMT Chinese–English news task. We empirically test this claim alternative evaluation protocols, contrasting of single sentences and entire documents. In a pairwise ranking experiment, raters assessing adequacy fluency show stronger preference for over when evaluating documents as compared to isolated sentences. Our findings emphasise need shift towards document-level improves degree errors which are hard or impossible spot at sentence-level become decisive in discriminating quality different outputs."
https://openalex.org/W2962788902,https://doi.org/10.18653/v1/w17-4912,Controlling Linguistic Style Aspects in Neural Language Generation,2017,"Most work on neural natural language generation (NNLG) focus controlling the content of generated text. We experiment with controling several stylistic aspects text, in addition to its content. The method is based conditioned RNN model, where desired as well parameters serve conditioning contexts. demonstrate approach movie reviews domain and show that it successful generating coherent sentences corresponding required linguistic style"
https://openalex.org/W2963176022,https://doi.org/10.1109/cvpr.2018.00637,Improved Fusion of Visual and Language Representations by Dense Symmetric Co-attention for Visual Question Answering,2018,"A key solution to visual question answering (VQA) exists in how fuse and language features extracted from an input image question. We show that attention mechanism enables dense, bi-directional interactions between the two modalities contributes boost accuracy of prediction answers. Specifically, we present a simple architecture is fully symmetric representations, which each word attends on regions region words. It can be stacked form hierarchy for multi-step image-question pair. through experiments proposed achieves new state-of-the-art VQA 2.0 despite its small size. also qualitative evaluation, demonstrating generate reasonable maps images questions, leads correct answer prediction."
https://openalex.org/W2963622213,https://doi.org/10.1109/cvpr.2018.00380,VizWiz Grand Challenge: Answering Visual Questions from Blind People,2018,"The study of algorithms to automatically answer visual questions currently is motivated by question answering (VQA) datasets constructed in artificial VQA settings. We propose VizWiz, the first goal-oriented dataset arising from a natural setting. VizWiz consists over 31,000 originating blind people who each took picture using mobile phone and recorded spoken about it, together with 10 crowdsourced answers per question. differs many existing because (1) images are captured photographers so often poor quality, (2) more conversational, (3) cannot be answered. Evaluation modern for deciding if answerable reveals that challenging dataset. introduce this encourage larger community develop generalized can assist people."
https://openalex.org/W1988680150,https://doi.org/10.1016/j.eswa.2015.02.055,An analysis of the coherence of descriptors in topic modeling,2015,"We evaluate the coherence and generality of topic descriptors found by LDA NMF.Six new existing corpora were specifically compiled for this evaluation.A measure using word2vec-modeled term vector similarity is proposed.NMF regularly produces more coherent topics, where weighting influential.NMF may be suitable modeling niche or non-mainstream corpora. In recent years, has become an established method in analysis text corpora, with probabilistic techniques such as latent Dirichlet allocation (LDA) commonly employed purpose. However, it might argued that adequate attention often not paid to issue coherence, semantic interpretability top terms usually used describe discovered topics. Nevertheless, a number studies have proposed measures analyzing these been largely focused on topics LDA, matrix decomposition Non-negative Matrix Factorization (NMF) being somewhat overlooked comparison. This motivates current work, we compare analyze popular variants both NMF multiple their associated generality, combination measures, including one based distributional semantics. Two out three find produce higher levels redundancy observed descriptors. all cases, observe strategy plays major role. The results suggest when certain those domains."
https://openalex.org/W2469060249,https://doi.org/10.18653/v1/n16-1108,Pairwise Word Interaction Modeling with Deep Neural Networks for Semantic Similarity Measurement,2016,"Textual similarity measurement is a challenging problem, as it requires understanding the semantics of input sentences. Most previous neural network models use coarse-grained sentence modeling, which has difficulty capturing fine-grained word-level information for semantic comparisons. As an alternative, we propose to explicitly model pairwise word interactions and present novel focus mechanism identify important correspondences better measurement. Our ideas are implemented in architecture that demonstrates state-ofthe-art accuracy on three SemEval tasks two answer selection tasks."
https://openalex.org/W2532494225,https://doi.org/10.1109/apsipa.2016.7820786,Voice conversion from non-parallel corpora using variational auto-encoder,2016,"We propose a flexible framework for spectral conversion (SC) that facilitates training with unaligned corpora. Many SC frameworks require parallel corpora, phonetic alignments, or explicit frame-wise correspondence learning functions synthesizing target spectrum the aid of alignments. However, these requirements gravely limit scope practical applications due to scarcity even unavailability an based on variational auto-encoder which enables us exploit non-parallel The comprises encoder learns speaker-independent representations and decoder reconstruct designated speaker. It removes requirement corpora alignments train system. report objective subjective evaluations validate our proposed method compare it methods have access aligned"
https://openalex.org/W2741049976,https://doi.org/10.18653/v1/e17-1083,Paraphrasing Revisited with Neural Machine Translation,2017,"Recognizing and generating paraphrases is an important component in many natural language processing applications. A well-established technique for automatically extracting leverages bilingual corpora to find meaning-equivalent phrases a single by “pivoting” over shared translation another language. In this paper we revisit pivoting the context of neural machine present paraphrasing model based purely on networks. Our represents continuous space, estimates degree semantic relatedness between text segments arbitrary length, generates candidate any source input. Experimental results across tasks datasets show that outperform those obtained with conventional phrase-based approaches."
https://openalex.org/W2962993339,https://doi.org/10.26615/978-954-452-049-6_062,Detecting Hate Speech in Social Media,2017,"In this paper we examine methods to detect hate speech in social media, while distinguishing from general profanity. We aim establish lexical baselines for task by applying supervised classification using a recently released dataset annotated purpose. As features, our system uses character n-grams, word n-grams and skip-grams. obtain results of 78% accuracy identifying posts across three classes. Results demonstrate that the main challenge lies discriminating profanity each other. A number directions future work are discussed."
https://openalex.org/W2586602577,https://doi.org/10.1016/j.csl.2017.01.005,Multilingual processing of speech via web services,2017,"New software paradigm for linguistic/phonetic tools: webservices.Webservices as building blocks complex systems.BAS CLARIN webservices: a free service to the scientific community.Multilingual automatic segmentation and labelling of speech into words phones.Multilingual text-to-phoneme conversion webservice. A new `Software Service' based on web services is proposed multilingual linguistic tools exemplified with BAS services. Instead traditional tool development distribution functionality implemented highly available server that users or applications access via HTTP requests. As examples we describe in detail five science operational since 2012 discuss benefits drawbacks well our experiences user acceptance implementation problems. The include speech, grapheme-to-phoneme conversion, syllabification, synthesis, optimal symbol sequence alignment."
https://openalex.org/W2888557792,https://doi.org/10.1145/3238147.3238206,Improving automatic source code summarization via deep reinforcement learning,2018,"Code summarization provides a high level natural language description of the function performed by code, as it can benefit software maintenance, code categorization and retrieval. To best our knowledge, most state-of-the-art approaches follow an encoder-decoder framework which encodes into hidden space then decode space, suffering from two major drawbacks: a) Their encoders only consider sequential content ignoring tree structure is also critical for task summarization; b) decoders are typically trained to predict next word maximizing likelihood ground-truth with previous given. However, expected generate entire sequence scratch at test time. This discrepancy cause exposure bias issue, making learnt decoder suboptimal. In this paper, we incorporate abstract syntax well snippets deep reinforcement learning (i.e., actor-critic network). The actor network confidence predicting according current state. On other hand, critic evaluates reward value all possible extensions state provide global guidance explorations. We employ advantage composed BLEU metric train both networks. Comprehensive experiments on real-world dataset show effectiveness proposed model when compared some methods."
https://openalex.org/W2945127593,https://doi.org/10.1145/3331184.3331303,Deeper Text Understanding for IR with Contextual Neural Language Modeling,2019,"Neural networks provide new possibilities to automatically learn complex language patterns and query-document relations. IR models have achieved promising results in learning relevance patterns, but few explorations been done on understanding the text content of a query or document. This paper studies leveraging recently-proposed contextual neural model, BERT, deeper for IR. Experimental demonstrate that representations from BERT are more effective than traditional word embeddings. Compared bag-of-words retrieval models, model can better leverage structures, bringing large improvements queries written natural languages. Combining ability with search knowledge leads an enhanced pre-trained benefit related tasks where training data limited."
https://openalex.org/W2969662548,https://doi.org/10.1371/journal.pone.0221152,Hate speech detection: Challenges and solutions,2019,"As online content continues to grow, so does the spread of hate speech. We identify and examine challenges faced by automatic approaches for speech detection in text. Among these difficulties are subtleties language, differing definitions on what constitutes speech, limitations data availability training testing systems. Furthermore, many recent suffer from an interpretability problem-that is, it can be difficult understand why systems make decisions that they do. propose a multi-view SVM approach achieves near state-of-the-art performance, while being simpler producing more easily interpretable than neural methods. also discuss both technical practical remain this task."
https://openalex.org/W2308720496,https://doi.org/10.18653/v1/p16-1139,A Fast Unified Model for Parsing and Sentence Understanding,2016,"Tree-structured neural networks exploit valuable syntactic parse information as they interpret the meanings of sentences. However, suer from two key technical problems that make them slow and unwieldyforlarge-scaleNLPtasks: theyusually operate on parsed sentences do not directly support batched computation. We address these issues by introducingtheStack-augmentedParser-Interpreter NeuralNetwork(SPINN),whichcombines parsing interpretation within a single tree-sequence hybrid model integrating tree-structured sentence into linear sequential structure shiftreduceparser. Ourmodelsupportsbatched computation for speedup up to 25◊ over other models, its integrated parser can unparsed data with little loss in accuracy. evaluate it Stanford NLI entailment task show significantly outperforms sentence-encoding models."
https://openalex.org/W2798416089,https://doi.org/10.18653/v1/p18-1224,Neural Natural Language Inference Models Enhanced with External Knowledge,2018,"Modeling natural language inference is a very challenging task. With the availability of large annotated data, it has recently become feasible to train complex models such as neural-network-based models, which have shown achieve state-of-the-art performance. Although there exist relatively can machines learn all knowledge needed perform (NLI) from these data? If not, how NLI benefit external and build leverage it? In this paper, we enrich neural with knowledge. We demonstrate that proposed improve performance on SNLI MultiNLI datasets."
https://openalex.org/W2962833140,https://doi.org/10.18653/v1/p19-1487,Explain Yourself! Leveraging Language Models for Commonsense Reasoning,2019,"Deep learning models perform poorly on tasks that require commonsense reasoning, which often necessitates some form of world-knowledge or reasoning over information not immediately present in the input. We collect human explanations for natural language sequences and highlighted annotations a new dataset called Common Sense Explanations (CoS-E). use CoS-E to train automatically generate can be used during training inference novel Commonsense Auto-Generated Explanation (CAGE) framework. CAGE improves state-of-the-art by 10% challenging CommonsenseQA task. further study DNNs using both auto-generated including transfer out-of-domain tasks. Empirical results indicate we effectively leverage reasoning."
https://openalex.org/W2970529259,https://doi.org/10.18653/v1/d19-1279,"75 Languages, 1 Model: Parsing Universal Dependencies Universally",2019,"We present UDify, a multilingual multi-task model capable of accurately predicting universal part-of-speech, morphological features, lemmas, and dependency trees simultaneously for all 124 Universal Dependencies treebanks across 75 languages. By leveraging BERT self-attention pretrained on 104 languages, we found that fine-tuning it datasets concatenated together with simple softmax classifiers each UD task can meet or exceed state-of-the-art UPOS, UFeats, Lemmas, (and especially) UAS, LAS scores, without requiring any recurrent language-specific components. evaluate UDify learning, showing low-resource languages benefit the most from cross-linguistic annotations. also zero-shot results suggesting training provides strong predictions even neither nor have ever been trained on."
https://openalex.org/W2995514860,https://doi.org/10.1186/s12859-019-3220-8,Modeling aspects of the language of life through transfer-learning protein sequences,2019,"Abstract Background Predicting protein function and structure from sequence is one important challenge for computational biology. For 26 years, most state-of-the-art approaches combined machine learning evolutionary information. However, some applications retrieving related proteins becoming too time-consuming. Additionally, information less powerful small families, e.g. the Dark Proteome . Both these problems are addressed by new methodology introduced here. Results We a novel way to represent sequences as continuous vectors ( embeddings ) using language model ELMo taken natural processing. By modeling sequences, effectively captured biophysical properties of life unlabeled big data (UniRef50). refer SeqVec Seq uence-to- Vec tor) demonstrate their effectiveness training simple neural networks two different tasks. At per-residue level, secondary (Q3 = 79% ± 1, Q8 68% 1) regions with intrinsic disorder (MCC 0.59 0.03) were predicted significantly better than through one-hot encoding or Word2vec-like approaches. per-protein subcellular localization was in ten classes (Q10 membrane-bound distinguished water-soluble (Q2 87% 1). Although generated best predictions single no solution improved over existing method Nevertheless, our approach popular methods even did beat best. Thus, they prove condense underlying principles sequences. Overall, novelty speed: where lightning-fast HHblits needed on average about minutes generate target protein, created 0.03 s. As this speed-up independent size growing databases, provides highly scalable analysis proteomics, i.e. microbiome metaproteome analysis. Conclusion Transfer-learning succeeded extract databases relevant various prediction modeled life, namely any features suggested textbooks methods. The exception information, however, that not available level sequence."
https://openalex.org/W2149369282,https://doi.org/10.1186/1758-2946-7-s1-s2,The CHEMDNER corpus of chemicals and drugs and its annotation principles,2015,"The automatic extraction of chemical information from text requires the recognition entity mentions as one its key steps. When developing supervised named (NER) systems, availability a large, manually annotated corpus is desirable. Furthermore, large corpora permit robust evaluation and comparison different approaches that detect chemicals in documents. We present CHEMDNER corpus, collection 10,000 PubMed abstracts contain total 84,355 labeled by expert chemistry literature curators, following annotation guidelines specifically defined for this task. were selected to be representative all major disciplines. Each was according structure-associated mention (SACEM) class: abbreviation, family, formula, identifier, multiple, systematic trivial. difficulty consistency tagging measured using an agreement study between annotators, obtaining percentage 91. For subset (the test set 3,000 abstracts) we provide not only Gold Standard manual annotations, but also automatically detected 26 teams participated BioCreative IV In addition, release silver standard extracted 17,000 randomly abstracts. A version BioC format has been generated well. propose required minimum about annotations construction domain specific on drug entities. are available at: http://www.biocreative.org/resources/biocreative-iv/chemdner-corpus/."
https://openalex.org/W2798734500,https://doi.org/10.18653/v1/p18-1047,Extracting Relational Facts by an End-to-End Neural Model with Copy Mechanism,2018,"The relational facts in sentences are often complicated. Different triplets may have overlaps a sentence. We divided the into three types according to triplet overlap degree, including Normal, EntityPairOverlap and SingleEntiyOverlap. Existing methods mainly focus on Normal class fail extract precisely. In this paper, we propose an end-to-end model based sequence-to-sequence learning with copy mechanism, which can jointly from of any these classes. adopt two different strategies decoding process: employing only one united decoder or applying multiple separated decoders. test our models public datasets outperform baseline method significantly."
https://openalex.org/W2888539709,https://doi.org/10.18653/v1/d18-1458,Why Self-Attention? A Targeted Evaluation of Neural Machine Translation Architectures,2018,"Recently, non-recurrent architectures (convolutional, self-attentional) have outperformed RNNs in neural machine translation. CNNs and self-attentional networks can connect distant words via shorter network paths than RNNs, it has been speculated that this improves their ability to model long-range dependencies. However, theoretical argument not tested empirically, nor alternative explanations for strong performance explored in-depth. We hypothesize the of could also be due extract semantic features from source text, we evaluate self-attention on two tasks: subject-verb agreement (where capturing dependencies is required) word sense disambiguation feature extraction required). Our experimental results show that: 1) do outperform modeling over long distances; 2) perform distinctly better disambiguation."
https://openalex.org/W2914699769,https://doi.org/10.1007/s11263-016-0987-1,Movie Description,2017,"Audio description (AD) provides linguistic descriptions of movies and allows visually impaired people to follow a movie along with their peers. Such are by design mainly visual thus naturally form an interesting data source for computer vision computational linguistics. In this work we propose novel dataset which contains transcribed ADs, temporally aligned full length movies. addition also collected scripts used in prior compare the two sources descriptions. We introduce Large Scale Movie Description Challenge (LSMDC) parallel corpus 128,118 sentences video clips from 200 (around 150 h total). The goal challenge is automatically generate clips. First characterize benchmarking different approaches generating Comparing ADs scripts, find that more describe precisely what shown rather than should happen according created production. Furthermore, present results several teams who participated challenges organized context workshops at ICCV 2015 ECCV 2016."
https://openalex.org/W2949474740,https://doi.org/10.48550/arxiv.1602.07332,"Visual Genome: Connecting Language and Vision Using Crowdsourced Dense
  Image Annotations",2016,"Despite progress in perceptual tasks such as image classification, computers still perform poorly on cognitive description and question answering. Cognition is core to that involve not just recognizing, but reasoning about our visual world. However, models used tackle the rich content images for are being trained using same datasets designed tasks. To achieve success at tasks, need understand interactions relationships between objects an image. When asked ""What vehicle person riding?"", will identify well riding(man, carriage) pulling(horse, order answer correctly ""the riding a horse-drawn carriage"". In this paper, we present Visual Genome dataset enable modeling of relationships. We collect dense annotations objects, attributes, within each learn these models. Specifically, contains over 100K where has average 21 18 pairwise objects. canonicalize relationships, noun phrases region descriptions questions pairs WordNet synsets. Together, represent densest largest descriptions, answers."
https://openalex.org/W2926555354,https://doi.org/10.18653/v1/n19-1064,Gender Bias in Contextualized Word Embeddings,2019,"In this paper, we quantify, analyze and mitigate gender bias exhibited in ELMo's contextualized word vectors. First, conduct several intrinsic analyses find that (1) training data for ELMo contains significantly more male than female entities, (2) the trained embeddings systematically encode information (3) unequally encodes about entities. Then, show a state-of-the-art coreference system depends on inherits its demonstrates significant WinoBias probing corpus. Finally, explore two methods to such demonstrated can be eliminated."
https://openalex.org/W2940320374,https://doi.org/10.1016/j.patcog.2019.04.014,How much can k-means be improved by using better initialization and repeats?,2019,"Abstract In this paper, we study what are the most important factors that deteriorate performance of k-means algorithm, and how much deterioration can be overcome either by using a better initialization technique, or repeating (restarting) algorithm. Our main finding is when clusters overlap, significantly improved these two tricks. Simple furthest point heuristic (Maxmin) reduces number erroneous from 15% to 6%, on average, with our clustering benchmark. Repeating algorithm 100 times it further down 1%. This accuracy more than enough for pattern recognition applications. However, data has well separated clusters, depends completely goodness initialization. Therefore, if high needed, should used instead."
https://openalex.org/W2963223838,https://doi.org/10.18653/v1/w17-5205,WASSA-2017 Shared Task on Emotion Intensity,2017,"We present the first shared task on de- tecting intensity of emotion felt by speaker a tweet. create datasets tweets annotated for anger, fear, joy, and sadness intensities using technique called best–worst scal- ing (BWS). show that annota- tions lead to reliable fine-grained scores (rankings intensity). The data was partitioned into training, velopment, test sets compe- tition. Twenty-two teams participated in task, with best system ob- taining Pearson correlation 0.747 gold scores. summarize machine learning setups, resources, tools used participating teams, focus techniques re- sources are particularly useful task. dataset helping improve our under- standing how we convey more or less intense emotions through language."
https://openalex.org/W2963530300,https://doi.org/10.18653/v1/p19-1644,A Corpus for Reasoning about Natural Language Grounded in Photographs,2019,"We introduce a new dataset for joint reasoning about natural language and images, with focus on semantic diversity, compositionality, visual challenges. The data contains 107,292 examples of English sentences paired web photographs. task is to determine whether caption true pair crowdsource the using sets visually rich images compare-and-contrast elicit linguistically diverse language. Qualitative analysis shows requires compositional reasoning, including quantities, comparisons, relations. Evaluation state-of-the-art methods presents strong challenge."
https://openalex.org/W2988823324,https://doi.org/10.1109/iccv.2019.00475,Visual Semantic Reasoning for Image-Text Matching,2019,"Image-text matching has been a hot research topic bridging the vision and language areas. It remains challenging because current representation of image usually lacks global semantic concepts as in its corresponding text caption. To address this issue, we propose simple interpretable reasoning model to generate visual that captures key objects scene. Specifically, first build up connections between regions perform with Graph Convolutional Networks features relationships. Then, use gate memory mechanism on these relationship-enhanced features, select discriminative information gradually for whole Experiments validate our method achieves new state-of-the-art image-text MS-COCO Flickr30K datasets. outperforms best by 6.8% relatively retrieval 4.8% caption (Recall@1 using 1K test set). On Flickr30K, improves 12.6% 5.8% (Recall@1). Our code is available at https://github.com/KunpengLi1994/VSRN."
https://openalex.org/W3104186312,https://doi.org/10.18653/v1/2020.emnlp-demos.2,BERTweet: A pre-trained language model for English Tweets,2020,"We present BERTweet, the first public large-scale pre-trained language model for English Tweets. Our having same architecture as BERT-base (Devlin et al., 2019), is trained using RoBERTa pre-training procedure (Liu 2019). Experiments show that BERTweet outperforms strong baselines RoBERTa-base and XLM-R-base (Conneau 2020), producing better performance results than previous state-of-the-art models on three Tweet NLP tasks: Part-of-speech tagging, Named-entity recognition text classification. release under MIT License to facilitate future research applications data. available at https://github.com/VinAIResearch/BERTweet"
https://openalex.org/W3106234277,https://doi.org/10.18653/v1/2020.emnlp-main.750,Evaluating the Factual Consistency of Abstractive Text Summarization,2020,"The most common metrics for assessing summarization algorithms do not account whether summaries are factually consistent with source documents. We propose a weakly-supervised, model-based approach verifying factual consistency and identifying conflicts between documents generated summaries. Training data is by applying series of rule-based transformations to the sentences documents.The model then trained jointly three tasks: 1) predict each summary sentence or not, 2) in either case, extract span document support this prediction, 3) that deemed inconsistent, inconsistent from it. Transferring several neural models reveals highly scalable outperforms previous models, including those strong supervision using datasets related domains, such as natural language inference fact checking. Additionally, human evaluation shows auxiliary extraction tasks provide useful assistance process consistency. also release manually annotated dataset verification, code training generation, weights at https://github.com/salesforce/factCC."
https://openalex.org/W655477013,https://doi.org/10.3115/v1/n15-1114,Toward Abstractive Summarization Using Semantic Representations,2015,"We present a novel abstractive summarization framework that draws on the recent development of treebank for Abstract Meaning Representation (AMR). In this framework, source text is parsed to set AMR graphs, graphs are transformed into summary graph, and then generated from graph. focus graph-tograph transformation reduces semantic graph making use an existing parser assuming eventual availability AMR-totext generator. The data-driven, trainable, not specifically designed particular domain. Experiments goldstandard annotations system parses show promising results. Code available at: https://github.com/summarization"
https://openalex.org/W2145870108,https://doi.org/10.1186/1758-2946-7-s1-s1,CHEMDNER: The drugs and chemical names extraction challenge,2015,"Natural language processing (NLP) and text mining technologies for the chemical domain (ChemNLP or mining) are key to improve access integration of information from unstructured data such as patents scientific literature. Therefore, BioCreative organizers posed CHEMDNER (chemical compound drug name recognition) community challenge, which promoted development novel, competitive accessible systems. This task allowed a comparative assessment performance various methodologies using carefully prepared collection manually labeled by specially trained chemists Gold Standard data. We evaluated two important aspects: one covered indexing documents with chemicals document - CDI task), other was concerned finding exact mentions in entity mention recognition CEM task). 27 teams (23 academic 4 commercial, total 87 researchers) returned results tasks: 26 23 task. Top scoring obtained an F-score 87.39% 88.20% task, very promising result when compared agreement between human annotators (91%). The strategies used detect included machine learning methods (e.g. conditional random fields) variety features, chemistry lexica, domain-specific rules. expect that tools resources resulting this effort will have impact future developments applications form basis find related detected entities, toxicological pharmacogenomic properties."
https://openalex.org/W2278786050,https://doi.org/10.1007/s00138-015-0737-3,Leaf segmentation in plant phenotyping: a collation study,2016,"Image-based plant phenotyping is a growing application area of computer vision in agriculture. A key task the segmentation all individual leaves images. Here we focus on most common rosette model plants, Arabidopsis and young tobacco. Although do share appearance shape characteristics, presence occlusions variability leaf pose, as well imaging conditions, render this problem challenging. The aim paper to compare several solutions unique first-of-its-kind dataset containing images from typical experiments. In particular, report discuss methods findings collection submissions for first Leaf Segmentation Challenge Computer Vision Problems Plant Phenotyping workshop 2014. Four are presented: three segment by processing distance transform an unsupervised fashion, other via optimal template selection Chamfer matching. Overall, find that although separating background can be accomplished with satisfactory accuracy ($$>$$>90 % Dice score), counting remain challenging when overlap. Additionally, lower younger leaves. We also datasets does affect outcomes. Our motivate further investigations development specialized algorithms particular application, challenges form ideally suited advancing state art. Data publicly available (online at http://www.plant-phenotyping.org/datasets) support future beyond within domain."
https://openalex.org/W2763088512,https://doi.org/10.1146/annurev-linguistics-030514-125254,Distributional Models of Word Meaning,2018,"Distributional semantics is a usage-based model of meaning, based on the assumption that statistical distribution linguistic items in context plays key role characterizing their semantic behavior. models build representations by extracting co-occurrences from corpora and have become mainstream research paradigm computational linguistics. In this review, I present state art distributional semantics, focusing its assets limits as meaning method for analysis."
https://openalex.org/W2962712961,https://doi.org/10.18653/v1/d18-1049,Improving the Transformer Translation Model with Document-Level Context,2018,"Although the Transformer translation model (Vaswani et al., 2017) has achieved state-of-the-art performance in a variety of tasks, how to use document-level context deal with discourse phenomena problematic for still remains challenge. In this work, we extend new encoder represent context, which is then incorporated into original and decoder. As large-scale parallel corpora are usually not available, introduce two-step training method take full advantage abundant sentence-level limited corpora. Experiments on NIST Chinese-English datasets IWSLT French-English show that our approach improves over significantly."
https://openalex.org/W2979382951,https://doi.org/10.1109/cvpr.2019.00851,Towards VQA Models That Can Read,2019,"Studies have shown that a dominant class of questions asked by visually impaired users on images their surroundings involves reading text in the image. But today’s VQA models can not read! Our paper takes first step towards addressing this problem. First, we introduce new “TextVQA” dataset to facilitate progress important Existing datasets either small proportion about (e.g., dataset) or are too VizWiz dataset). TextVQA contains 45,336 28,408 require reasoning answer. Second, novel model architecture reads image, reasons it context image and question, predicts an answer which might be deduction based composed strings found Consequently, call our approach Look, Read, Reason & Answer (LoRRA). We show LoRRA outperforms existing state-of-the-art dataset. find gap between human performance machine is significantly larger than 2.0, suggesting well-suited benchmark along directions complementary 2.0."
https://openalex.org/W2517456239,https://doi.org/10.1016/j.artint.2016.07.005,Nasari: Integrating explicit knowledge and corpus statistics for a multilingual representation of concepts and entities,2016,"Owing to the need for a deep understanding of linguistic items, semantic representation is considered be one fundamental components several applications in Natural Language Processing and Artificial Intelligence. As result, has been prominent research areas lexical semantics over past decades. However, due mainly lack large sense-annotated corpora, most existing techniques are limited level thus cannot effectively applied individual word senses. In this paper we put forward novel multilingual vector representation, called Nasari , which not only enables accurate senses different languages, but it also provides two main advantages approaches: (1) high coverage, including both concepts named entities, (2) comparability across languages levels (i.e., words, concepts), thanks items single unified space joint embedded space, respectively. Moreover, our representations flexible, can multiple freely available at http://lcl.uniroma1.it/nasari/ . evaluation benchmark, opted four tasks, namely, similarity, sense clustering, domain labeling, Word Sense Disambiguation, each report state-of-the-art performance on standard datasets languages."
https://openalex.org/W2757205734,https://doi.org/10.18653/v1/d17-1120,Neural Sequence Learning Models for Word Sense Disambiguation,2017,"Word Sense Disambiguation models exist in many flavors. Even though supervised ones tend to perform best terms of accuracy, they often lose ground more flexible knowledge-based solutions, which do not require training by a word expert for every disambiguation target. To bridge this gap we adopt different perspective and rely on sequence learning frame the problem: propose study depth series end-to-end neural architectures directly tailored task, from bidirectional Long Short-Term Memory encoder-decoder models. Our extensive evaluation over standard benchmarks multiple languages shows that enables versatile all-words consistently lead state-of-the-art results, even against experts with engineered features."
https://openalex.org/W2804778516,https://doi.org/10.18653/v1/n18-1081,Supervised Open Information Extraction,2018,"We present data and methods that enable a supervised learning approach to Open Information Extraction (Open IE). Central the is novel formulation of IE as sequence tagging problem, addressing challenges such encoding multiple extractions for predicate. also develop bi-LSTM transducer, extending recent deep Semantic Role Labeling models extract tuples provide confidence scores tuning their precision-recall tradeoff. Furthermore, we show recently released Question-Answer Meaning Representation dataset can be automatically converted into an corpus which significantly increases amount available training data. Our model outperforms existing state-of-the-art systems on benchmark datasets."
https://openalex.org/W2963662654,https://doi.org/10.18653/v1/n19-1240,Question Answering by Reasoning Across Documents with Graph Convolutional Networks,2019,"Most research in reading comprehension has focused on answering questions based individual documents or even single paragraphs. We introduce a neural model which integrates and reasons relying information spread within across multiple documents. frame it as an inference problem graph. Mentions of entities are nodes this graph while edges encode relations between different mentions (e.g., within- cross-document co-reference). Graph convolutional networks (GCNs) applied to these graphs trained perform multi-step reasoning. Our Entity-GCN method is scalable compact, achieves state-of-the-art results multi-document question dataset, WikiHop (Welbl et al., 2018)."
https://openalex.org/W2963676814,https://doi.org/10.48550/arxiv.1711.04434,Faithful to the Original: Fact Aware Neural Abstractive Summarization,2018,"Unlike extractive summarization, abstractive summarization has to fuse different parts of the source text, which inclines create fake facts. Our preliminary study reveals nearly 30% outputs from a state-of-the-art neural system suffer this problem. While previous approaches usually focus on improvement informativeness, we argue that faithfulness is also vital prerequisite for practical system. To avoid generating facts in summary, leverage open information extraction and dependency parse technologies extract actual fact descriptions text. The dual-attention sequence-to-sequence framework then proposed force generation conditioned both text extracted descriptions. Experiments Gigaword benchmark dataset demonstrate our model can greatly reduce summaries by 80%. Notably, bring significant informativeness since they often condense meaning"
https://openalex.org/W2972943112,https://doi.org/10.21437/interspeech.2019-1473,An Unsupervised Autoregressive Model for Speech Representation Learning,2019,"This paper proposes a novel unsupervised autoregressive neural model for learning generic speech representations. In contrast to other representation methods that aim remove noise or speaker variabilities, ours is designed preserve information wide range of downstream tasks. addition, the proposed does not require any phonetic word boundary labels, allowing benefit from large quantities unlabeled data. Speech representations learned by our significantly improve performance on both phone classification and verification over surface features supervised approaches. Further analysis shows different levels are captured at layers. particular, lower layers tend be more discriminative speakers, while upper provide content."
https://openalex.org/W2545177271,https://doi.org/10.21437/interspeech.2017-1566,Neural Speech Recognizer: Acoustic-to-Word LSTM Model for Large Vocabulary Speech Recognition,2017,"We present results that show it is possible to build a competitive, greatly simplified, large vocabulary continuous speech recognition system with whole words as acoustic units. model the output of about 100,000 directly using deep bi-directional LSTM RNNs CTC loss. The trained on 125,000 hours semi-supervised training data, which enables us alleviate data sparsity problem for word models. models work very well an end-to-end all-neural without use traditional context-dependent sub-word phone units require pronunciation lexicon, and any language removing need decode. demonstrate perform better than strong, more complex, state-of-the-art baseline"
https://openalex.org/W2605736949,https://doi.org/10.1109/cvpr.2017.766,ViP-CNN: Visual Phrase Guided Convolutional Neural Network,2017,"As the intermediate level task connecting image captioning and object detection, visual relationship detection started to catch researchers' attention because of its descriptive power clear structure. It detects objects captures their pair-wise interactions with a subject-predicate-object triplet, e.g. person-ride-horse. In this paper, each is considered as phrase three components. We formulate inter-connected recognition problems propose Visual Phrase guided Convolutional Neural Network (ViP-CNN) address them simultaneously. ViP-CNN, we present Phrase-guided Message Passing Structure (PMPS) establish connection among components help model consider jointly. Corresponding non-maximum suppression method training strategy are also proposed. Experimental results show that our ViP-CNN outperforms state-of-art both in speed accuracy. further pretrain on cleansed Genome Relationship dataset, which found perform better than pretraining ImageNet for task."
https://openalex.org/W2945102109,https://doi.org/10.18653/v1/p19-1444,Towards Complex Text-to-SQL in Cross-Domain Database with Intermediate Representation,2019,"We present a neural approach called IRNet for complex and cross-domain Text-to-SQL. aims to address two challenges: 1) the mismatch between intents expressed in natural language (NL) implementation details SQL; 2) challenge predicting columns caused by large number of out-of-domain words. Instead end-to-end synthesizing SQL query, decomposes synthesis process into three phases. In first phase, performs schema linking over question database schema. Then, adopts grammar-based model synthesize SemQL query which is an intermediate representation that we design bridge NL SQL. Finally, deterministically infers from synthesized with domain knowledge. On challenging Text-to-SQL benchmark Spider, achieves 46.7% accuracy, obtaining 19.5% absolute improvement previous state-of-the-art approaches. At time writing, position on Spider leaderboard."
https://openalex.org/W2955483668,https://doi.org/10.1093/jamia/ocz096,Enhancing clinical concept extraction with contextual embeddings,2019,"Neural network-based representations (""embeddings"") have dramatically advanced natural language processing (NLP) tasks, including clinical NLP tasks such as concept extraction. Recently, however, more embedding methods and (e.g., ELMo, BERT) further pushed the state-of-the-art in NLP, yet there are no common best practices for how to integrate these into tasks. The purpose of this study, then, is explore space possible options utilizing new models extraction, comparing traditional word (word2vec, GloVe, fastText). Both off-the-shelf open-domain embeddings pre-trained from MIMIC-III evaluated. We a battery consisting contextual embeddings, compare on four extraction corpora: i2b2 2010, 2012, SemEval 2014, 2015. also analyze impact pre-training time large model like ELMo or BERT performance. Last, we present an intuitive way understand semantic information encoded by embeddings. Contextual corpus achieves performances across all best-performing outperforms with respective F1-measures 90.25, 93.18 (partial), 80.74, 81.65. demonstrate potential through performance achieve Additionally, encode valuable not accounted representations."
https://openalex.org/W2964318046,https://doi.org/10.1109/iccv.2017.186,Be Your Own Prada: Fashion Synthesis with Structural Coherence,2017,"We present a novel and effective approach for generating new clothing on wearer through generative adversarial learning. Given an input image of person sentence describing different outfit, our model ""redresses"" the as desired, while at same time keeping her/his pose unchanged. Generating outfits with precise regions conforming to language description retaining wearer's body structure is challenging task. Existing networks are not ideal in ensuring global coherence given both photograph conditions. address this challenge by decomposing complex process into two conditional stages. In first stage, we generate plausible semantic segmentation map that obeys latent spatial arrangement. An constraint formulated guide generation map. second newly proposed compositional mapping layer used render final textures conditioned extended DeepFashion dataset [8] collecting descriptions 79K images. demonstrate effectiveness quantitative qualitative evaluations. A user study also conducted. The codes data available http://mmlab.ie.cuhk. edu.hk/projects/FashionGAN/."
https://openalex.org/W1974804104,https://doi.org/10.1111/desc.12202,Sentence repetition is a measure of children's language skills rather than working memory limitations,2015,"Sentence repetition tasks are widely used in the diagnosis and assessment of children with language difficulties. This paper seeks to clarify nature sentence their relationship other skills. We present results from a 2-year longitudinal study 216 children. Children were assessed on measures repetition, vocabulary knowledge grammatical skills three times at approximately yearly intervals starting age 4. was not unique predictor growth A unidimensional latent factor (defined by skills) provided an excellent fit data, abilities showed high degree stability. is best seen as reflection underlying ability rather than measure separate construct specific role processing. appears be valuable tool for because it draws upon wide range processing"
https://openalex.org/W2962708992,https://doi.org/10.18653/v1/d16-1162,Incorporating Discrete Translation Lexicons into Neural Machine Translation,2016,"Neural machine translation (NMT) often makes mistakes in translating low-frequency content words that are essential to understanding the meaning of sentence. We propose a method alleviate this problem by augmenting NMT systems with discrete lexicons efficiently encode translations these words. describe calculate lexicon probability next word candidate using attention vector model select which source lexical probabilities should focus on. test two methods combine standard probability: (1) it as bias, and (2) linear interpolation. Experiments on corpora show an improvement 2.0-2.3 BLEU 0.13-0.44 NIST score, faster convergence time."
https://openalex.org/W3035497479,https://doi.org/10.18653/v1/2020.acl-main.653,MLQA: Evaluating Cross-lingual Extractive Question Answering,2020,"Question answering (QA) models have shown rapid progress enabled by the availability of large, high-quality benchmark datasets. Such annotated datasets are difficult and costly to collect, rarely exist in languages other than English, making building QA systems that work well challenging. In order develop such systems, it is crucial invest high quality multilingual evaluation benchmarks measure progress. We present MLQA, a multi-way aligned extractive intended spur research this area. MLQA contains instances 7 languages, Arabic, German, Spanish, Hindi, Vietnamese Simplified Chinese. has over 12K English 5K each language, with instance parallel between 4 on average. evaluate state-of-the-art cross-lingual machine-translation-based baselines MLQA. all cases, transfer results be significantly behind training-language performance."
https://openalex.org/W2250741688,https://doi.org/10.3115/v1/p15-1119,Cross-lingual Dependency Parsing Based on Distributed Representations,2015,"Jiang Guo, Wanxiang Che, David Yarowsky, Haifeng Wang, Ting Liu. Proceedings of the 53rd Annual Meeting Association for Computational Linguistics and 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2015."
https://openalex.org/W2273847690,https://doi.org/10.1145/2858036.2858535,Empath,2016,"Human language is colored by a broad range of topics, but existing text analysis tools only focus on small number them. We present Empath, tool that can generate and validate new lexical categories demand from set seed terms (like ""bleed"" ""punch"" to the category violence). Empath draws connotations between words phrases deep learning neural embedding across more than 1.8 billion modern fiction. Given characterize category, uses its discover related terms, then validates with crowd-powered filter. also analyzes 200 built-in, pre-validated we have generated common topics in our web dataset, like neglect, government, social media. show Empath's data-driven, human validated are highly correlated (r=0.906) similar LIWC."
https://openalex.org/W2590061102,https://doi.org/10.1109/access.2017.2672677,Comparison Research on Text Pre-processing Methods on Twitter Sentiment Analysis,2017,"Twitter sentiment analysis offers organizations ability to monitor public feeling towards the products and events related them in real time. The first step of is text pre-processing data. Most existing researches about are focused on extraction new features. However, select method ignored. This paper discussed effects classification performance two types tasks, summed up performances six methods using feature models four classifiers five datasets. experiments show that accuracy F1-measure classifier improved when expanding acronyms replacing negation, but barely changes removing URLs, numbers or stop words. Naive Bayes Random Forest more sensitive than Logistic Regression support vector machine various were applied."
https://openalex.org/W2805662932,https://doi.org/10.18653/v1/n18-1193,Conversational Memory Network for Emotion Recognition in Dyadic Dialogue Videos,2018,"Emotion recognition in conversations is crucial for the development of empathetic machines. Present methods mostly ignore role inter-speaker dependency relations while classifying emotions conversations. In this paper, we address recognizing utterance-level dyadic conversational videos. We propose a deep neural framework, termed memory network, which leverages contextual information from conversation history. The framework takes multimodal approach comprising audio, visual and textual features with gated recurrent units to model past utterances each speaker into memories. Such memories are then merged using attention-based hops capture dependencies. Experiments show an accuracy improvement 3-4% over state art."
https://openalex.org/W2941799245,https://doi.org/10.1109/access.2019.2909919,Sentiment Analysis of Comment Texts Based on BiLSTM,2019,"With the rapid development of Internet technology and social networks, a large number comment texts are generated on Web. In era big data, mining emotional tendency comments through artificial intelligence is helpful for timely understanding network public opinion. The sentiment analysis part intelligence, its research very meaningful obtaining trend comments. essence text classification task, different words have contributions to classification. current studies, distributed word representation mostly used. However, only considers semantic information word, but ignore word. this paper, an improved method proposed, which integrates contribution into traditional TF-IDF algorithm generates weighted vectors. vectors input bidirectional long short term memory (BiLSTM) capture context effectively, better represented. obtained by feedforward neural classifier. Under same conditions, proposed compared with methods RNN, CNN, LSTM, NB. experimental results show that has higher precision, recall, F1 score. proved be effective high accuracy"
https://openalex.org/W2962802109,https://doi.org/10.18653/v1/w17-4811,Neural Machine Translation with Extended Context,2017,"We investigate the use of extended context in attention-based neural machine translation. base our experiments on translated movie subtitles and discuss effect increasing segments beyond single translation units. study source language as well bilingual extensions. The models learn to distinguish between information from different are surprisingly robust with respect quality. In this pilot study, we observe interesting cross-sentential attention patterns that improve textual coherence at least some selected cases."
https://openalex.org/W2963208801,https://doi.org/10.18653/v1/n16-1155,Multilingual Language Processing From Bytes,2016,"We describe an LSTM-based model which we call Byte-to-Span (BTS) that reads text as bytes and outputs span annotations of the form [start, length, label] where start positions, lengths, labels are separate entries in our vocabulary. Because operate directly on unicode rather than language-specific words or characters, can analyze many languages with a single model. Due to small vocabulary size, these multilingual models very compact, but produce results similar better state-of- the-art Part-of-Speech tagging Named Entity Recognition use only provided training datasets (no external data sources). Our learning ""from scratch"" they do not rely any elements standard pipeline Natural Language Processing (including tokenization), thus run standalone fashion raw text."
https://openalex.org/W2966351171,https://doi.org/10.2196/12239,Natural Language Processing of Clinical Notes on Chronic Diseases: Systematic Review,2019,"Background: Novel approaches that complement and go beyond evidence-based medicine are required in the domain of chronic diseases, given growing incidence such conditions on worldwide population. A promising avenue is secondary use electronic health records (EHRs), where patient data analyzed to conduct clinical translational research. Methods based machine learning process EHRs resulting improved understanding trajectories disease risk prediction, creating a unique opportunity derive previously unknown insights. However, wealth histories remains locked behind narratives free-form text. Consequently, unlocking full potential EHR contingent development natural language processing (NLP) methods automatically transform text into structured can guide decisions potentially delay or prevent onset. Objective: The goal research was provide comprehensive overview uptake NLP applied free-text notes related including investigation challenges faced by methodologies narratives. Methods: Preferred Reporting Items for Systematic Reviews Meta-Analyses (PRISMA) guidelines were followed searches conducted 5 databases using “clinical notes,” “natural processing,” “chronic disease” their variations as keywords maximize coverage articles. Results: Of 2652 articles considered, 106 met inclusion criteria. Review included papers resulted identification 43 which then further classified 10 categories International Classification Diseases, 10th Revision. majority studies focused diseases circulatory system (n=38) while endocrine metabolic fewest (n=14). This due structure typically contain much more data, compared with medical system, focus unstructured consequently have seen stronger NLP. review has shown there significant increase rule-based approaches; however, deep remain emergent (n=3). works classification phenotype only handful addressing extraction comorbidities from free integration data. There notable relatively simple methods, shallow classifiers (or combination methods), interpretability predictions, still represents issue complex methods. Finally, scarcity publicly available may also contributed insufficient advanced word embeddings notes. Conclusions: Efforts improve (1) progression toward understanding; (2) recognition relations among entities rather than isolation; (3) temporal understand past, current, future events; (4) exploitation alternative sources knowledge; (5) availability large-scale, de-identified corpora."
https://openalex.org/W2970780738,https://doi.org/10.18653/v1/d19-1243,Cosmos QA: Machine Reading Comprehension with Contextual Commonsense Reasoning,2019,"Understanding narratives requires reading between the lines, which in turn, interpreting likely causes and effects of events, even when they are not mentioned explicitly. In this paper, we introduce Cosmos QA, a large-scale dataset 35,600 problems that require commonsense-based comprehension, formulated as multiple-choice questions. stark contrast to most existing comprehension datasets where questions focus on factual literal understanding context paragraph, our focuses lines over diverse collection people’s everyday narratives, asking such “what might be possible reason ...?, or would have happened if ... reasoning beyond exact text spans context. To establish baseline performances experiment with several state-of-the-art neural architectures for also propose new architecture improves competitive baselines. Experimental results demonstrate significant gap machine (68.4%) human performance (94%), pointing avenues future research commonsense comprehension. Dataset, code leaderboard is publicly available at https://wilburone.github.io/cosmos."
https://openalex.org/W3169483174,https://doi.org/10.18653/v1/2021.naacl-main.41,mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer,2021,"Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Barua, Colin Raffel. Proceedings of the 2021 Conference North American Chapter Association for Computational Linguistics: Human Language Technologies. 2021."
https://openalex.org/W2250646737,https://doi.org/10.3115/v1/p15-1027,Hubness and Pollution: Delving into Cross-Space Mapping for Zero-Shot Learning,2015,"Zero-shot methods in language, vision and other domains rely on a cross-space mapping function that projects vectors from the relevant feature space (e.g., visualfeature-based image representations) to large semantic word (induced an unsupervised way corpus data), where entities of interest objects images depict) are labeled with words associated nearest neighbours mapped vectors. hold great promise as scale up annotation tasks well beyond labels training data recognizing were never seen training). However, current performance functions is still quite low, so strategy not yet usable practical applications. In this paper, we explore some general properties, both theoretical empirical, function, build them propose better estimate it. way, attain improvements over state art, cross-linguistic (word translation) cross-modal (image labeling) zero-shot experiments."
https://openalex.org/W2342430154,https://doi.org/10.1080/18756891.2016.1180821,An Overview on Fuzzy Modelling of Complex Linguistic Preferences in Decision Making,2016,"AbstractDecision makers involved in complex decision making problems usually provide information about their preferences by eliciting knowledge with different assessments. Usually, the complexity of these implies uncertainty that many occasions has been successfully modelled means linguistic information, mainly based on fuzzy approaches. However, classically approaches just allow elicitation simple assessments composed either one label or a modifier label. Nevertheless, necessity more expressions for makers’ led to some extensions classical construction and closer way human beings cognitive process. This paper provides an overview broadest modelling together challenges future proposals should achieve improve ..."
https://openalex.org/W2740811004,https://doi.org/10.18653/v1/p17-1102,PositionRank: An Unsupervised Approach to Keyphrase Extraction from Scholarly Documents,2017,"This paper proposes PositionRank, an unsupervised model for keyphrase extraction from scholarly documents that incorporates information all positions of a word's occurrences into biased PageRank."
https://openalex.org/W2894280539,https://doi.org/10.1609/aaai.v33i01.33019062,Multilevel Language and Vision Integration for Text-to-Clip Retrieval,2019,"We address the problem of text-based activity retrieval in video. Given a sentence describing an activity, our task is to retrieve matching clips from untrimmed To capture inherent structures present both text and video, we introduce multilevel model that integrates vision language features earlier more tightly than prior work. First, inject early on when generating clip proposals, help eliminate unlikely thus speed up processing boost performance. Second, learn fine-grained similarity metric for retrieval, use visual modulate query sentences at word level recurrent neural network. A multi-task loss also employed by adding re-generation as auxiliary task. Our approach significantly outperforms work two challenging benchmarks: Charades-STA ActivityNet Captions."
https://openalex.org/W2964291396,https://doi.org/10.18653/v1/n18-1118,Evaluating Discourse Phenomena in Neural Machine Translation,2018,"For machine translation to tackle discourse phenomena, models must have access extra-sentential linguistic context. There has been recent interest in modelling context neural (NMT), but principally evaluated with standard automatic metrics, poorly adapted evaluating phenomena. In this article, we present hand-crafted, test sets, designed the models{'} ability exploit previous source and target sentences. We investigate performance of recently proposed multi-encoder NMT trained on subtitles for English French. also explore a novel way exploiting from sentence. Despite gains using BLEU, give limited improvement handling phenomena: 50{\%} accuracy our coreference set 53.5{\%} coherence/cohesion (compared non-contextual baseline 50{\%}). A simple strategy decoding concatenation current sentence leads good performance, multi-encoding two sentences best (72.5{\%} 57{\%} coherence/cohesion), highlighting importance target-side"
https://openalex.org/W3100779964,https://doi.org/10.1140/epjds/s13688-016-0093-1,The emotional arcs of stories are dominated by six basic shapes,2016,"Advances in computing power, natural language processing, and digitization of text now make it possible to study a culture's evolution through its texts using ""big data"" lens. Our ability communicate relies part upon shared emotional experience, with stories often following distinct trajectories forming patterns that are meaningful us. Here, by classifying the arcs for filtered subset 1,327 from Project Gutenberg's fiction collection, we find set six core which form essential building blocks complex trajectories. We strengthen our findings separately applying Matrix decomposition, supervised learning, unsupervised learning. For each these arcs, examine closest characteristic publication today particular enjoy greater success, as measured downloads."
https://openalex.org/W2131540451,https://doi.org/10.18653/v1/s15-2049,SemEval-2015 Task 13: Multilingual All-Words Sense Disambiguation and Entity Linking,2015,"In this paper we present the Multilingual AllWords Sense Disambiguation and Entity Linking task. Word (WSD) (EL) are well-known problems in Natural Language Processing field both address lexical ambiguity of language. Their main difference lies kind meaning inventories that used: EL uses encyclopedic knowledge, while WSD lexicographic information. Our aim with task is to analyze whether, if so, how, using a resource integrates kinds (i.e., BabelNet 2.5.1) might enable be solved by means similar (even, same) methods. Moreover, investigate multilingual setting for some specific domains."
https://openalex.org/W2251658415,https://doi.org/10.18653/v1/d15-1306,That's So Annoying!!!: A Lexical and Frame-Semantic Embedding Based Data Augmentation Approach to Automatic Categorization of Annoying Behaviors using #petpeeve Tweets,2015,"We propose a novel data augmentation approach to enhance computational behavioral analysis using social media text. In particular, we collect Twitter corpus of the descriptions annoying behaviors #petpeeve hashtags. qualitative analysis, study language use in these tweets, with special focus on fine-grained categories and geographic variation language. quantitative show that lexical syntactic features are useful for automatic categorization behaviors, frame-semantic further boost performance; leveraging large embeddings create additional training instances significantly improves model; incorporating embedding achieves best overall performance."
https://openalex.org/W2397198482,https://doi.org/10.18653/v1/p16-1113,Neural Semantic Role Labeling with Dependency Path Embeddings,2016,"This paper introduces a novel model for semantic role labeling that makes use of neural sequence modeling techniques. Our approach is motivated by the observation complex syntactic structures and related phenomena, such as nested subordinations nominal predicates, are not handled well existing models. treats instances subsequences lexicalized dependency paths learns suitable embedding representations. We experimentally demonstrate embeddings can improve results over previous state-of-the-art labelers, showcase qualitative improvements obtained our method."
https://openalex.org/W1713614699,https://doi.org/10.1162/tacl_a_00141,Design Challenges for Entity Linking,2015,"Recent research on entity linking (EL) has introduced a plethora of promising techniques, ranging from deep neural networks to joint inference. But despite numerous papers there is surprisingly little understanding the state art in EL. We attack this confusion by analyzing differences between several versions EL problem and presenting simple yet effective, modular, unsupervised system, called Vinculum, for linking. conduct an extensive evaluation nine data sets, comparing Vinculum with two state-of-the-art systems, elucidate key aspects system that include mention extraction, candidate generation, type prediction, coreference, coherence."
https://openalex.org/W2741750617,https://doi.org/10.24963/ijcai.2017/595,Iterative Entity Alignment via Joint Knowledge Embeddings,2017,"Entity alignment aims to link entities and their counterparts among multiple knowledge graphs (KGs). Most existing methods typically rely on external information of such as Wikipedia links require costly manual feature construction complete alignment. In this paper, we present a novel approach for entity via joint embeddings. Our method jointly encodes both relations various KGs into unified low-dimensional semantic space according small seed set aligned entities. During process, can align distance in space. More specifically, an iterative parameter sharing improve performance. Experiment results real-world datasets show that, compared baselines, our achieves significant improvements alignment, further graph completion performance with the favor"
https://openalex.org/W2757369719,https://doi.org/10.18653/v1/d17-1284,"Entity Linking via Joint Encoding of Types, Descriptions, and Context",2017,"For accurate entity linking, we need to capture various information aspects of an entity, such as its description in a KB, contexts which it is mentioned, and structured knowledge. Additionally, linking system should work on texts from different domains without requiring domain-specific training data or hand-engineered features. In this present neural, modular that learns unified dense representation for each using multiple sources information, description, around mentions, fine-grained types. We show the resulting effective at combining these sources, performs competitively, sometimes out-performing current state-of-the-art systems across datasets, any also our model can effectively “embed” entities are new able link mentions accurately."
https://openalex.org/W2785787385,https://doi.org/10.18653/v1/w17-2630,Deep Active Learning for Named Entity Recognition,2017,"Deep learning has yielded state-of-the-art performance on many natural language processing tasks including named entity recognition (NER). However, this typically requires large amounts of labeled data. In work, we demonstrate that the amount training data can be drastically reduced when deep is combined with active learning. While sample-efficient, it computationally expensive since iterative retraining. To speed up, introduce a lightweight architecture for NER, viz., CNN-CNN-LSTM model consisting convolutional character and word encoders long short term memory (LSTM) tag decoder. The achieves nearly standard datasets task while being much more efficient than best performing models. We carry out incremental learning, during process, are able to match just 25\% original"
https://openalex.org/W2963592583,https://doi.org/10.1609/aaai.v33i01.33016908,Data-to-Text Generation with Content Selection and Planning,2019,"Recent advances in data-to-text generation have led to the use of large-scale datasets and neural network models which are trained end-to-end, without explicitly modeling what say order. In this work, we present a architecture incorporates content selection planning sacrificing end-to-end training. We decompose task into two stages. Given corpus data records (paired with descriptive documents), first generate plan highlighting information should be mentioned order then document while taking account. Automatic human-based evaluation experiments show that our model1 outperforms strong baselines improving state-of-the-art on recently released RotoWIRE dataset."
https://openalex.org/W3105663928,https://doi.org/10.1162/coli_a_00276,Argumentation Mining in User-Generated Web Discourse,2017,"The goal of argumentation mining, an evolving research field in computational linguistics, is to design methods capable analyzing people's argumentation. In this article, we go beyond the state art several ways. (i) We deal with actual Web data and take up challenges given by variety registers, multiple domains, unrestricted noisy user-generated discourse. (ii) bridge gap between normative theories phenomena encountered adapting model tested extensive annotation study. (iii) create a new gold standard corpus (90k tokens 340 documents) experiment machine learning identify argument components. offer data, source codes, guidelines community under free licenses. Our findings show that mining discourse feasible but challenging task."
https://openalex.org/W2251199578,https://doi.org/10.18653/v1/d15-1076,Question-Answer Driven Semantic Role Labeling: Using Natural Language to Annotate Natural Language,2015,"This paper introduces the task of questionanswer driven semantic role labeling (QA-SRL), where question-answer pairs are used to represent predicate-argument structure. For example, verb “introduce” in previous sentence would be labeled with questions “What is introduced?”, and something?”, each paired phrase from that gives correct answer. Posing problem this way allows themselves define set possible roles, without need for predefined frame or thematic ontologies. It also scalable data collection by annotators very little training no linguistic expertise. We gather two domains, newswire text Wikipedia articles, introduce simple classifierbased models predicting which ask what their answers should be. Our results show non-expert can produce high quality QA-SRL data, establish baseline performance levels future work on task."
https://openalex.org/W2744813330,https://doi.org/10.18653/v1/p17-2061,An Empirical Comparison of Domain Adaptation Methods for Neural Machine Translation,2017,"In this paper, we propose a novel domain adaptation method named “mixed fine tuning” for neural machine translation (NMT). We combine two existing approaches namely tuning and multi NMT. first train an NMT model on out-of-domain parallel corpus, then tune it corpus which is mix of the in-domain corpora. All corpora are augmented with artificial tags to indicate specific domains. empirically compare our proposed against methods discuss its benefits shortcomings."
https://openalex.org/W2963991316,https://doi.org/10.1162/tacl_a_00105,Deep Recurrent Models with Fast-Forward Connections for Neural Machine Translation,2016,"Neural machine translation (NMT) aims at solving (MT) problems using neural networks and has exhibited promising results in recent years. However, most of the existing NMT models are shallow there is still a performance gap between single model best conventional MT system. In this work, we introduce new type linear connections, named fast-forward based on deep Long Short-Term Memory (LSTM) networks, an interleaved bi-directional architecture for stacking LSTM layers. Fast-forward connections play essential role propagating gradients building topology depth 16. On WMT’14 English-to-French task, achieve BLEU=37.7 with attention model, which outperforms corresponding by 6.2 BLEU points. This first time that achieves state-of-the-art 0.7 We can BLEU=36.3 even without mechanism. After special handling unknown words ensembling, obtain score reported to date task BLEU=40.4. Our also validated more difficult English-to-German task."
https://openalex.org/W3036120435,https://doi.org/10.1162/tacl_a_00313,Leveraging Pre-trained Checkpoints for Sequence Generation Tasks,2020,"Unsupervised pre-training of large neural models has recently revolutionized Natural Language Processing. By warm-starting from the publicly released checkpoints, NLP practitioners have pushed state-of-the-art on multiple benchmarks while saving significant amounts compute time. So far focus been mainly Understanding tasks. In this paper, we demonstrate efficacy pre-trained checkpoints for Sequence Generation. We developed a Transformer-based sequence-to-sequence model that is compatible with available BERT, GPT-2, and RoBERTa conducted an extensive empirical study utility initializing our model, both encoder decoder, these checkpoints. Our result in new results Machine Translation, Text Summarization, Sentence Splitting, Fusion."
https://openalex.org/W3099655892,https://doi.org/10.18653/v1/2020.findings-emnlp.171,UNIFIEDQA: Crossing Format Boundaries with a Single QA System,2020,"Question answering (QA) tasks have been posed using a variety of formats, such as extractive span selection, multiple choice, etc. This has led to format-specialized models, and even an implicit division in the QA community. We argue that boundaries are artificial perhaps unnecessary, given reasoning abilities we seek teach not governed by format. As evidence, use latest advances language modeling build single pre-trained model, UNIFIEDQA, performs well across 19 datasets spanning 4 diverse formats. UNIFIEDQA on par with 8 different models were trained individual themselves. Even when faced 12 unseen observed surprisingly well, showing strong generalization from its outof-format training data. Finally, simply finetuning this pre model into specialized results new state art 10 factoid commonsense question datasets, establishing starting point for building systems."
https://openalex.org/W2058990119,https://doi.org/10.1016/j.eswa.2014.10.023,A semantic approach for text clustering using WordNet and lexical chains,2015,"A modified WordNet based similarity measure for word sense disambiguation.Lexical chains as text representation ideally cover the theme of texts.Extracted core semantics are sufficient to reduce dimensionality feature set.The proposed scheme is able correctly estimate true number clusters.The topic labels have good indicator recognizing and understanding clusters. Traditional clustering algorithms do not consider semantic relationships among words so that cannot accurately represent meaning documents. To overcome this problem, introducing information from ontology such has been widely used improve quality clustering. However, there still exist several challenges, synonym polysemy, high dimensionality, extracting texts, assigning appropriate description generated In paper, we report our attempt towards integrating with lexical alleviate these problems. The approach exploits hierarchical structure relations provide a more accurate assessment between terms disambiguation. Furthermore, introduce extract set semantically related which can content texts. Although extensively in summarization, their potential impact on problem fully investigated. Our integrated way identify documents disambiguated features extracted, parallel downsize dimensions space. experimental results using framework reuters-21578 show performance improves significantly compared classical methods."
https://openalex.org/W2096951189,https://doi.org/10.1093/bib/bbv024,"Community challenges in biomedical text mining over 10 years: success, failure and the future",2016,"One effective way to improve the state of art is through competitions. Following success Critical Assessment protein Structure Prediction (CASP) in bioinformatics research, a number challenge evaluations have been organized by text-mining research community assess and advance natural language processing (NLP) for biomedicine. In this article, we review different held from 2002 2014 their respective tasks. Furthermore, examine these tasks targeted problems NLP biomedical applications, respectively. Next, describe general workflow organizing Biomedical (BioNLP) involved stakeholders (task organizers, task data producers, participants end users). Finally, summarize impact contributions taking into account BioNLP challenges as whole, followed discussion limitations difficulties. We conclude with future trends evaluations."
https://openalex.org/W2206052695,https://doi.org/10.20472/te.2015.3.3.002,The importance of vocabulary in language learning and how to be taught,2015,"Vocabulary learning is an essential part in foreign language as the meanings of new words are very often emphasized, whether books or classrooms. It also central to teaching and paramount importance a learner. Recent research indicate that vocabulary may be problematic because many teachers not confident about best practice at times don?t know where begin form instructional emphasis on word (Berne & Blachowicz, 2008)In this article, I summarizes important impotence explaining techniques used by English when English, well my own personal view these issues."
https://openalex.org/W2887920589,https://doi.org/10.18653/v1/d18-1103,Rapid Adaptation of Neural Machine Translation to New Languages,2018,"This paper examines the problem of adapting neural machine translation systems to new, low-resourced languages (LRLs) as effectively and rapidly possible. We propose methods based on starting with massively multilingual ""seed models"", which can be trained ahead-of-time, then continuing training data related LRL. contrast a number strategies, leading novel, simple, yet effective method ""similar-language regularization"", where we jointly train both LRL interest similar high-resourced language prevent over-fitting small data. Experiments demonstrate that models, even without any explicit adaptation, are surprisingly effective, achieving BLEU scores up 15.5 no from LRL, proposed similar-language regularization improves over other adaptation by 1.7 points average 4 settings. Code reproduce experiments at https://github.com/neubig/rapid-adaptation"
https://openalex.org/W2958953787,https://doi.org/10.48550/arxiv.1907.05019,"Massively Multilingual Neural Machine Translation in the Wild: Findings
  and Challenges",2019,"We introduce our efforts towards building a universal neural machine translation (NMT) system capable of translating between any language pair. set milestone this goal by single massively multilingual NMT model handling 103 languages trained on over 25 billion examples. Our demonstrates effective transfer learning ability, significantly improving quality low-resource languages, while keeping high-resource on-par with competitive bilingual baselines. provide in-depth analysis various aspects that are crucial to achieving and practicality in NMT. While we prototype high-quality system, extensive empirical exposes issues need be further addressed, suggest directions for future research."
https://openalex.org/W2963702064,https://doi.org/10.1016/j.knosys.2018.07.041,Multimodal sentiment analysis using hierarchical fusion with context modeling,2018,"Multimodal sentiment analysis is a very actively growing field of research. A promising area opportunity in this to improve the multimodal fusion mechanism. We present novel feature strategy that proceeds hierarchical fashion, first fusing modalities two and only then all three modalities. On individual utterances, our outperforms conventional concatenation features by 1%, which amounts 5% reduction error rate. utterance-level multi-utterance video clips, for current state-of-the-art techniques incorporate contextual information from other utterances same clip, gives up 2.4% (almost 10% rate reduction) over currently used concatenation. The implementation method publicly available form open-source code."
https://openalex.org/W2964068236,https://doi.org/10.1609/aaai.v33i01.33016859,Combining Fact Extraction and Verification with Neural Semantic Matching Networks,2019,"The increasing concern with misinformation has stimulated research efforts on automatic fact checking. recentlyreleased FEVER dataset introduced a benchmark factverification task in which system is asked to verify claim using evidential sentences from Wikipedia documents. In this paper, we present connected consisting of three homogeneous neural semantic matching models that conduct document retrieval, sentence selection, and verification jointly for extraction verification. For evidence retrieval (document selection), unlike traditional vector space IR queries sources are matched some pre-designed term space, develop perform deep raw textual input, assuming no intermediate representation access structured external knowledge bases. We also show Pageview frequency can help improve the performance results, later be by our network. verification, previous approaches simply feed upstream retrieved natural language inference (NLI) model, further enhance NLI model providing it internal relatedness scores (hence integrating modules) ontological WordNet features. Experiments indicate (1) method outperforms popular TF-IDF encoder models, significant margins all metrics, (2) additional score features via better awareness, (3) formalizing subtasks as similar problem improving stages, complete able achieve state-of-the-art results test set (two times greater than baseline results).1"
https://openalex.org/W2973154008,https://doi.org/10.18653/v1/w19-4825,Open Sesame: Getting inside BERT’s Linguistic Knowledge,2019,"How and to what extent does BERT encode syntactically-sensitive hierarchical information or positionally-sensitive linear information? Recent work has shown that contextual representations like perform well on tasks require sensitivity linguistic structure. We present here two studies which aim provide a better understanding of the nature BERT's representations. The first these focuses identification structurally-defined elements using diagnostic classifiers, while second explores representation subject-verb agreement anaphor-antecedent dependencies through quantitative assessment self-attention vectors. In both cases, we find encodes positional about word tokens its lower layers, but switches hierarchically-oriented encoding higher layers. conclude then do indeed model linguistically relevant aspects structure, though they not appear show sharp structure is found in human processing reflexive anaphora."
https://openalex.org/W2161222299,https://doi.org/10.3115/v1/p15-1032,Structured Training for Neural Network Transition-Based Parsing,2015,"David Weiss, Chris Alberti, Michael Collins, Slav Petrov. Proceedings of the 53rd Annual Meeting Association for Computational Linguistics and 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2015."
https://openalex.org/W2518510348,https://doi.org/10.18653/v1/p16-1150,Which argument is more convincing? Analyzing and predicting convincingness of Web arguments using bidirectional LSTM,2016,"We propose a new task in the field of computational argumentation which we investigate qualitative properties Web arguments, namely their convincingness. cast problem as relation classification, where pair arguments having same stance to prompt is judged. annotate large datasets 16k pairs over 32 topics and whether A more convincing than B exhibits total ordering; these findings are used global constraints for cleaning crowdsourced data. two tasks: (1) predicting argument from an (2) ranking all topic based on convincingness.
We experiment with feature-rich SVM bidirectional LSTM obtain 0.76-0.78 accuracy 0.35-0.40 Spearman's correlation cross-topic evaluation. release newly created corpus UKPConvArg1 experimental software under open licenses."
https://openalex.org/W2601686579,https://doi.org/10.1109/iccv.2017.77,Predicting Deeper into the Future of Semantic Segmentation,2017,"The ability to predict and therefore anticipate the future is an important attribute of intelligence. It also utmost importance in real-time systems, e.g. robotics or autonomous driving, which depend on visual scene understanding for decision making. While prediction raw RGB pixel values video frames has been studied previous work, here we introduce novel task predicting semantic segmentations frames. Given a sequence frames, our goal segmentation maps not yet observed that lie up second further future. We develop autoregressive convolutional neural network learns iteratively generate multiple Our results Cityscapes dataset show directly substantially better than then segmenting Prediction half are visually convincing much more accurate those baseline based warping using optical flow."
https://openalex.org/W2606837722,https://doi.org/10.1073/pnas.1701590114,Neurophysiological dynamics of phrase-structure building during sentence processing,2017,"Significance According to most linguists, the syntactic structure of sentences involves a tree-like hierarchy nested phrases, as in sentence [happy linguists] [draw [a diagram]]. Here, we searched for neural implementation this hypothetical construct. Epileptic patients volunteered perform language task while implanted with intracranial electrodes clinical purposes. While read one word at time, activation left-hemisphere areas increased each successive but decreased suddenly whenever words could be merged into phrase. This may footprint “merge,” fundamental tree-building operation that has been hypothesized allow recursive properties human language."
https://openalex.org/W2609368435,https://doi.org/10.24963/ijcai.2018/585,Deep Text Classification Can be Fooled,2017,"In this paper, we present an effective method to craft text adversarial samples, revealing one important yet underestimated fact that DNN-based classifiers are also prone sample attack. Specifically, confronted with different scenarios, the items for classification identified by computing cost gradients of input (white-box attack) or generating a series occluded test samples (black-box attack). Based on these items, design three perturbation strategies, namely insertion, modification, and removal, generate samples. The experiment results show generated our can successfully fool both state-of-the-art character-level word-level classifiers. be perturbed any desirable classes without compromising their utilities. At same time, introduced is difficult perceived."
https://openalex.org/W2762513422,https://doi.org/10.1145/3133887,SQLizer: query synthesis from natural language,2017,"This paper presents a new technique for automatically synthesizing SQL queries from natural language (NL). At the core of our is NL-based program synthesis methodology that combines semantic parsing techniques NLP community with type-directed and automated repair. Starting sketch obtained using standard techniques, approach involves an iterative refinement loop alternates between probabilistic type inhabitation We use proposed idea to build end-to-end system called SQLIZER can synthesize language. Our method fully automated, works any database without requiring additional customization, does not require users know underlying schema. evaluate on over 450 concerning three different databases, namely MAS, IMDB, YELP. experiments show desired query ranked within top 5 candidates in close 90% cases outperforms NALIR, state-of-the-art tool won best award at VLDB'14."
https://openalex.org/W2962869524,https://doi.org/10.1109/cvpr.2019.00134,MAN: Moment Alignment Network for Natural Language Moment Retrieval via Iterative Graph Adjustment,2019,"This research strives for natural language moment retrieval in long, untrimmed video streams. The problem is not trivial especially when a contains multiple moments of interests and the describes complex temporal dependencies, which often happens real scenarios. We identify two crucial challenges: semantic misalignment structural misalignment. However, existing approaches treat different separately do explicitly model moment-wise relations. In this paper, we present Moment Alignment Network (MAN), novel framework that unifies candidate encoding reasoning single-shot feed-forward network. MAN naturally assigns representations aligned with semantics over locations scales. Most importantly, propose to relations as structured graph devise an iterative adjustment network jointly learn best structure end-to-end manner. evaluate proposed approach on challenging public benchmarks DiDeMo Charades-STA, where our significantly outperforms state-of-the-art by large margin."
https://openalex.org/W2963177403,https://doi.org/10.1109/cvpr.2018.00443,Video Captioning via Hierarchical Reinforcement Learning,2018,"Video captioning is the task of automatically generating a textual description actions in video. Although previous work (e.g. sequence-to-sequence model) has shown promising results abstracting coarse short video, it still very challenging to caption video containing multiple fine-grained with detailed description. This paper aims address challenge by proposing novel hierarchical reinforcement learning framework for captioning, where high-level Manager module learns design sub-goals and low-level Worker recognizes primitive fulfill sub-goal. With this compositional reinforce at different levels, our approach significantly outperforms all baseline methods on newly introduced large-scale dataset captioning. Furthermore, non-ensemble model already achieved state-of-the-art widely-used MSR-VTT dataset."
https://openalex.org/W2963472233,https://doi.org/10.18653/v1/d18-1214,Gromov-Wasserstein Alignment of Word Embedding Spaces,2018,"Cross-lingual or cross-domain correspondences play key roles in tasks ranging from machine translation to transfer learning. Recently, purely unsupervised methods operating on monolingual embeddings have become effective alignment tools. Current state-of-the-art methods, however, involve multiple steps, including heuristic post-hoc refinement strategies. In this paper, we cast the correspondence problem directly as an optimal transport (OT) problem, building idea that word arise metric recovery algorithms. Indeed, exploit Gromov-Wasserstein distance measures how similarities between pairs of words relate across languages. We show our OT objective can be estimated efficiently, requires little no tuning, and results performance comparable with various tasks."
https://openalex.org/W2964301648,https://doi.org/10.18653/v1/p18-1041,Baseline Needs More Love: On Simple Word-Embedding-Based Models and Associated Pooling Mechanisms,2018,"Many deep learning architectures have been proposed to model the compositionality in text sequences, requiring substantial number of parameters and expensive computations. However, there has not a rigorous evaluation regarding added value sophisticated compositional functions. In this paper, we conduct point-by-point comparative study between Simple Word-Embedding-based Models (SWEMs), consisting parameter-free pooling operations, relative word-embedding-based RNN/CNN models. Surprisingly, SWEMs exhibit comparable or even superior performance majority cases considered. Based upon understanding, propose two additional strategies over learned word embeddings: (i) max-pooling operation for improved interpretability; (ii) hierarchical operation, which preserves spatial (n-gram) information within sequences. We present experiments on 17 datasets encompassing three tasks: (long) document classification; sequence matching; (iii) short tasks, including classification tagging."
https://openalex.org/W3105951585,https://doi.org/10.3390/info11060314,COVID-19 Public Sentiment Insights and Machine Learning for Tweets Classification,2020,"Along with the Coronavirus pandemic, another crisis has manifested itself in form of mass fear and panic phenomena, fueled by incomplete often inaccurate information. There is therefore a tremendous need to address better understand COVID-19's informational gauge public sentiment, so that appropriate messaging policy decisions can be implemented. In this research article, we identify sentiment associated pandemic using specific Tweets R statistical software, along its analysis packages. We demonstrate insights into progress fear-sentiment over time as COVID-19 approached peak levels United States, descriptive textual analytics supported necessary data visualizations. Furthermore, provide methodological overview two essential machine learning (ML) classification methods, context analytics, compare their effectiveness classifying varying lengths. observe strong accuracy 91% for short Tweets, Naive Bayes method. also logistic regression method provides reasonable 74% shorter both methods showed relatively weaker performance longer Tweets. This progression, outlines implications, limitations opportunities."
https://openalex.org/W3111372685,https://doi.org/10.1162/tacl_a_00342,oLMpics-On What Language Model Pre-training Captures,2020,"Recent success of pre-trained language models (LMs) has spurred widespread interest in the capabilities that they possess. However, efforts to understand whether LM representations are useful for symbolic reasoning tasks have been limited and scattered. In this work, we propose eight tasks, which conceptually require operations such as comparison, conjunction, composition. A fundamental challenge is performance a on task should be attributed or process fine-tuning data. To address this, an evaluation protocol includes both zero-shot (no fine-tuning), well comparing learning curve fine-tuned multiple controls, paints rich picture capabilities. Our main findings that: (a) different LMs exhibit qualitatively abilities, e.g., RoBERTa succeeds where BERT fails completely; (b) do not reason abstract manner context-dependent, while can compare ages, it so only when ages typical range human ages; (c) On half our all fail completely. infrastructure help future work designing new datasets, models, objective functions pre-training."
https://openalex.org/W2239285313,https://doi.org/10.1109/iccv.2015.528,Text Flow: A Unified Text Detection System in Natural Scene Images,2015,"The prevalent scene text detection approach follows four sequential steps comprising character candidate detection, false removal, line extraction, and verification. However, errors occur accumulate throughout each of these which often lead to low performance. To address issues, we propose a unified system, namely Text Flow, by utilizing the minimum cost (min-cost) flow network model. With candidates detected cascade boosting, min-cost model integrates last three into single process solves error accumulation problem at both level effectively. proposed technique has been tested on public datasets, i.e, ICDAR2011 dataset, ICDAR2013 dataset multilingual it outperforms state-of-the-art methods all datasets with much higher recall F-score. good performance shows that can be used for texts in different languages."
https://openalex.org/W2561529111,https://doi.org/10.1609/aaai.v31i1.11164,ConceptNet 5.5: An Open Multilingual Graph of General Knowledge,2016,"Machine learning about language can be improved by supplying it with specific knowledge and sources of external information. We present here a new version the linked open data resource ConceptNet that is particularly well suited to used modern NLP techniques such as word embeddings. graph connects words phrases natural labeled edges. Its collected from many include expert-created resources, crowd-sourcing, games purpose. It designed represent general involved in understanding language, improving applications allowing application better understand meanings behind people use. When combined embeddings acquired distributional semantics (such word2vec), provides they would not acquire alone, nor narrower resources WordNet or DBPedia. demonstrate this state-of-the-art results on intrinsic evaluations relatedness translate into improvements vectors, including solving SAT-style analogies."
https://openalex.org/W2726585279,https://doi.org/10.1007/978-1-4899-7687-1_187,Cross-Language Information Retrieval,2017,"This article addresses a timely problem, especially within the context of Internet, which is accessible by anyone in any discipline from virtually anywhere world. The problem cross-language retrieval not new. Then at now, information was seen to be function that would facilitate effective search for, exchange of, and information. chapter reviews research practice CLIR allows users state queries their native language retrieve documents other supported system. can simplify searching multilingual and, if translation resources are limited, allow monolingual allocate those more promising documents. review begins with an examination literature on user needs for CLIR. largely follows system model (document preprocessing, query formulation, matching, selection, delivery). Each section highlights unique requirements imposed one stages applications. authors describe evaluation techniques conclude observations regarding future directions research."
https://openalex.org/W2884797218,https://doi.org/10.21437/interspeech.2018-1768,"The Fifth 'CHiME' Speech Separation and Recognition Challenge: Dataset, Task and Baselines",2018,"The CHiME challenge series aims to advance robust automatic speech recognition (ASR) technology by promoting research at the interface of and language processing, signal processing , machine learning. This paper introduces 5th Challenge, which considers task distant multi-microphone conversational ASR in real home environments. Speech material was elicited using a dinner party scenario with efforts taken capture data that is representative natural recorded 6 Kinect microphone arrays 4 binaural pairs. features single-array track multiple-array and, for each track, distinct rankings will be produced systems focusing on robustness respect distant-microphone vs. attempting address all aspects including modeling. We discuss rationale provide detailed description collection procedure, task, baseline array synchronization, enhancement, conventional end-to-end ASR."
https://openalex.org/W2915128308,https://doi.org/10.18653/v1/n19-1162,"Cross-Lingual Alignment of Contextual Word Embeddings, with Applications to Zero-shot Dependency Parsing",2019,"We introduce a novel method for multilingual transfer that utilizes deep contextual embeddings, pretrained in an unsupervised fashion. While embeddings have been shown to yield richer representations of meaning compared their static counterparts, aligning them poses challenge due dynamic nature. To this end, we construct context-independent variants the original monolingual spaces and utilize mapping derive alignment context-dependent spaces. This readily supports processing target language, improving by context-aware embeddings. Our experimental results demonstrate effectiveness approach zero-shot few-shot learning dependency parsing. Specifically, our consistently outperforms previous state-of-the-art on 6 tested languages, yielding improvement 6.8 LAS points average."
https://openalex.org/W2952328691,https://doi.org/10.18653/v1/p19-1164,Evaluating Gender Bias in Machine Translation,2019,"We present the first challenge set and evaluation protocol for analysis of gender bias in machine translation (MT). Our approach uses two recent coreference resolution datasets composed English sentences which cast participants into non-stereotypical roles (e.g., “The doctor asked nurse to help her operation”). devise an automatic method eight target languages with grammatical gender, based on morphological use female inflection word “doctor”). analyses show that four popular industrial MT systems state-of-the-art academic models are significantly prone gender-biased errors all tested languages. data code publicly available at https://github.com/gabrielStanovsky/mt_gender."
https://openalex.org/W2963035245,https://doi.org/10.21437/odyssey.2018-28,The Voice Conversion Challenge 2018: Promoting Development of Parallel and Nonparallel Methods,2018,"We present the Voice Conversion Challenge 2018, designed as a follow up to 2016 edition with aim of providing common framework for evaluating and comparing different state-of-the-art voice conversion (VC) systems. The objective challenge was perform speaker (i.e. transform vocal identity) source target while maintaining linguistic information. As an update previous challenge, we considered both parallel non-parallel data form Hub Spoke tasks, respectively. A total 23 teams from around world submitted their systems, 11 them additionally participated in optional task. large-scale crowdsourced perceptual evaluation then carried out rate converted speech terms naturalness similarity identity. In this paper, brief summary techniques VC, followed by detailed explanation tasks results that were obtained."
https://openalex.org/W2963165489,https://doi.org/10.18653/v1/p19-1070,"How to (Properly) Evaluate Cross-Lingual Word Embeddings: On Strong Baselines, Comparative Analyses, and Some Misconceptions",2019,"Cross-lingual word embeddings (CLEs) facilitate cross-lingual transfer of NLP models. Despite their ubiquitous downstream usage, increasingly popular projection-based CLE models are almost exclusively evaluated on bilingual lexicon induction (BLI). Even the BLI evaluations vary greatly, hindering our ability to correctly interpret performance and properties different In this work, we take first step towards a comprehensive evaluation models: thoroughly evaluate both supervised unsupervised models, for large number language pairs, three tasks, providing new insights concerning cutting-edge support NLP. We empirically demonstrate that largely depends task at hand optimizing may hurt performance. indicate most robust emphasize need reassess simple baselines, which still display competitive across board. hope work catalyzes further research model analysis."
https://openalex.org/W2964006684,https://doi.org/10.18653/v1/d18-1547,MultiWOZ - A Large-Scale Multi-Domain Wizard-of-Oz Dataset for Task-Oriented Dialogue Modelling,2018,"Even though machine learning has become the major scene in dialogue research community, real breakthrough been blocked by scale of data available.To address this fundamental obstacle, we introduce Multi-Domain Wizard-of-Oz dataset (MultiWOZ), a fully-labeled collection human-human written conversations spanning over multiple domains and topics.At size 10k dialogues, it is at least one order magnitude larger than all previous annotated task-oriented corpora.The contribution work apart from open-sourced two-fold:firstly, detailed description procedure along with summary structure analysis provided. The proposed data-collection pipeline entirely based on crowd-sourcing without need hiring professional annotators;secondly, set benchmark results belief tracking, act response generation reported, which shows usability sets baseline for future studies."
https://openalex.org/W2471094925,https://doi.org/10.1007/978-3-319-46484-8_44,Revisiting Visual Question Answering Baselines,2016,"Visual question answering (VQA) is an interesting learning setting for evaluating the abilities and shortcomings of current systems image understanding. Many recently proposed VQA include attention or memory mechanisms designed to perform “reasoning”. Furthermore, task multiple-choice VQA, nearly all these train a multi-class classifier on features predict answer. This paper questions value common practices develops simple alternative model based binary classification. Instead treating answers as competing choices, our receives answer input predicts whether not image-question-answer triplet correct. We evaluate Visual7W Telling Real Multiple Choice tasks, find that even versions competitively. Our best achieves state-of-the-art performance \(65.8\,\%\) accuracy compares surprisingly well with most complex task. Additionally, we explore variants study transferability between both datasets. also present error analysis model, results which suggest key problem lies in lack visual grounding localization concepts occur answers."
https://openalex.org/W2916104401,https://doi.org/10.1109/icassp.2019.8683120,Utterance-level Aggregation for Speaker Recognition in the Wild,2019,"The objective of this paper is speaker recognition ""in the wild""-where utterances may be variable length and also contain irrelevant signals. Crucial elements in design deep networks for task are type trunk (frame level) network, method temporal aggregation. We propose a powerful using ""thin-ResNet"" architecture, dictionary-based NetVLAD or GhostVLAD layer to aggregate features across time, that can trained end-to-end. show our network achieves state art performance by significant margin on VoxCeleb1 test set recognition, whilst requiring fewer parameters than previous methods. investigate effect utterance performance, conclude wild"" data, longer beneficial."
https://openalex.org/W3035017890,https://doi.org/10.1109/cvpr42600.2020.00377,Unbiased Scene Graph Generation From Biased Training,2020,"Today’s scene graph generation (SGG) task is still far from practical, mainly due to the severe training bias, e.g., collapsing diverse human walk on / sit lay beach into beach. Given such SGG, down-stream tasks as VQA can hardly infer better structures than merely a bag of objects. However, debiasing in SGG not trivial because traditional methods cannot distinguish between good and bad context prior (e.g., person read book rather eat) long-tailed bias near dominating behind front of). In this paper, we present novel framework based causal inference but conventional likelihood. We first build for perform biased with graph. Then, propose draw counterfactual causality trained effect which should be removed. particular, use Total Direct Effect (TDE) proposed final predicate score unbiased SGG. Note that our agnostic any model thus widely applied community who seeks predictions. By using Scene Graph Diagnosis toolkit benchmark Visual Genome several prevailing models, observed significant improvements over previous state-of-the-art methods."
https://openalex.org/W3035625205,https://doi.org/10.18653/v1/2020.acl-main.519,A Unified MRC Framework for Named Entity Recognition,2020,"The task of named entity recognition (NER) is normally divided into nested NER and flat depending on whether entities are or not. Models usually separately developed for the two tasks, since sequence labeling models, most widely used backbone NER, only able to assign a single label particular token, which unsuitable where token may be assigned several labels. In this paper, we propose unified framework that capable handling both tasks. Instead treating as problem, formulate it machine reading comprehension (MRC) task. For example, extracting with \textsc{per} formalized answer spans question ""{\it person mentioned in text?}"". This formulation naturally tackles overlapping issue NER: extraction different categories requires answering independent questions. Additionally, query encodes informative prior knowledge, strategy facilitates process extraction, leading better performances not but NER. We conduct experiments {\em nested} flat} datasets. Experimental results demonstrate effectiveness proposed formulation. achieve vast amount performance boost over current SOTA models datasets, i.e., +1.28, +2.55, +5.44, +6.37, respectively ACE04, ACE05, GENIA KBP17, along i.e.,+0.24, +1.95, +0.21, +1.49 English CoNLL 2003, OntoNotes 5.0, Chinese MSRA, 4.0."
https://openalex.org/W594625170,https://doi.org/10.1080/18756891.2015.1061394,An approach to multiple attribute group decision making based on linguistic intuitionistic fuzzy numbers,2015,"AbstractMotivated by intuitionistic fuzzy sets and fzzy linguistic approach, this article proposes the concept of numbers (LIFNs) where membership nonmembership are represented as terms. In order to process multiple attribute decision making (MADM) with LIFNs, we introduce score index accuracy LIFN. Simultaneously, operation laws for LIFNs defined related properties studied. Further, some aggregation operators developed, involving weighted averaging (LIFWA) operator, ordered (LIFOWA) operator hybrid (LIFHA) etc., which can be utilized aggregate preference information taking form LIFNs. Based on LIFWA LIFHA operators, propose an approach handle MADM under environment. Finally, illustrativ..."
https://openalex.org/W2552579943,https://doi.org/10.1109/cvpr.2017.767,Instance-Aware Image and Sentence Matching with Selective Multimodal LSTM,2017,"Effective image and sentence matching depends on how to well measure their global visual-semantic similarity. Based the observation that such a similarity arises from complex aggregation of multiple local similarities between pairwise instances (objects) (words), we propose selective multimodal Long Short-Term Memory network (sm-LSTM) for instance-aware matching. The sm-LSTM includes context-modulated attention scheme at each timestep can selectively attend pair sentence, by predicting saliency maps sentence. For selected instances, representations are obtained based predicted maps, then compared By similarly measuring within few timesteps, sequentially aggregates them with hidden states obtain final score as desired Extensive experiments show our model match content, achieve state-of-the-art results two public benchmark datasets."
https://openalex.org/W2916723116,https://doi.org/10.1109/cvpr.2019.00209,MUREL: Multimodal Relational Reasoning for Visual Question Answering,2019,"Multimodal attentional networks are currently state-of-the-art models for Visual Question Answering (VQA) tasks involving real images. Although attention allows to focus on the visual content relevant question, this simple mechanism is arguably insufficient model complex reasoning features required VQA or other high-level tasks. In paper, we propose MuRel, a multimodal relational network which learned end-to-end reason over Our first contribution introduction of MuRel cell, an atomic primitive representing interactions between question and image regions by rich vectorial representation, modeling region relations with pairwise combinations. Secondly, incorporate cell into full network, progressively refines interactions, can be leveraged define visualization schemes finer than mere maps. We validate relevance our approach various ablation studies, show its superiority attention-based methods three datasets: 2.0, VQA-CP v2 TDIUC. final competitive outperforms results in challenging context. code available: github.com/Cadene/murel.bootstrap.pytorch"
https://openalex.org/W2950888501,https://doi.org/10.18653/v1/p19-1159,Mitigating Gender Bias in Natural Language Processing: Literature Review,2019,"As Natural Language Processing (NLP) and Machine Learning (ML) tools rise in popularity, it becomes increasingly vital to recognize the role they play shaping societal biases stereotypes. Although NLP models have shown success modeling various applications, propagate may even amplify gender bias found text corpora. While study of artificial intelligence is not new, methods mitigate are relatively nascent. In this paper, we review contemporary studies on recognizing mitigating NLP. We discuss based four forms representation analyze bias. Furthermore, advantages drawbacks existing debiasing methods. Finally, future for"
https://openalex.org/W2963246595,https://doi.org/10.18653/v1/p18-2058,Jointly Predicting Predicates and Arguments in Neural Semantic Role Labeling,2018,"Recent BIO-tagging-based neural semantic role labeling models are very high performing, but assume gold predicates as part of the input and cannot incorporate span-level features. We propose an end-to-end approach for jointly predicting all predicates, arguments spans, relations between them. The model makes independent decisions about what relationship, if any, holds every possible word-span pair, learns contextualized span representations that provide rich, shared features each decision. Experiments demonstrate this sets a new state art on PropBank SRL without predicates."
https://openalex.org/W2965510113,https://doi.org/10.1609/aaai.v33i01.33016714,A Unified Model for Opinion Target Extraction and Target Sentiment Prediction,2019,"Target-based sentiment analysis involves opinion target extraction and classification. However, most of the existing works usually studied one these two sub-tasks alone, which hinders their practical use. This paper aims to solve complete task target-based in an end-to-end fashion, presents a novel unified model applies tagging scheme. Our framework stacked recurrent neural networks: The upper predicts tags produce final output results primary analysis; lower performs auxiliary boundary prediction aiming at guiding network improve performance task. To explore inter-task dependency, we propose explicitly constrained transitions from boundaries polarities. We also maintain consistency within via gate mechanism models relation between features for current word previous word. conduct extensive experiments on three benchmark datasets our achieves consistently superior results."
https://openalex.org/W3108316907,https://doi.org/10.1007/978-3-030-58545-7_19,Contrastive Learning for Unpaired Image-to-Image Translation,2020,"Abstract In image-to-image translation, each patch in the output should reflect content of corresponding input, independent domain. We propose a straightforward method for doing so – maximizing mutual information between two, using framework based on contrastive learning. The encourages two elements (corresponding patches) to map similar point learned feature space, relative other (other dataset, referred as negatives. explore several critical design choices making learning effective image synthesis setting. Notably, we use multilayer, patch-based approach, rather than operate entire images. Furthermore, draw negatives from within input itself, rest dataset. demonstrate that our enables one-sided translation unpaired setting, while improving quality and reducing training time. addition, can even be extended setting where “domain” is only single image.KeywordsContrastive learningNoise estimationMutual informationImage generation"
https://openalex.org/W2592170186,https://doi.org/10.18653/v1/e17-2026,Identifying beneficial task relations for multi-task learning in deep neural networks,2017,"Multi-task learning (MTL) in deep neural networks for NLP has recently received increasing interest due to some compelling benefits, including its potential efficiently regularize models and reduce the need labeled data. While it brought significant improvements a number of tasks, mixed results have been reported, little is known about conditions under which MTL leads gains NLP. This paper sheds light on specific task relations that can lead from over single-task setups."
https://openalex.org/W2760505947,https://doi.org/10.18653/v1/w17-4418,Results of the WNUT2017 Shared Task on Novel and Emerging Entity Recognition,2017,"This shared task focuses on identifying unusual, previously-unseen entities in the context of emerging discussions. Named form basis many modern approaches to other tasks (like event clustering and summarization), but recall them is a real problem noisy text - even among annotators. drop tends be due novel surface forms. Take for example tweet so.. kktny 30 mins?! -- human experts find entity 'kktny' hard detect resolve. The goal this provide definition rare entities, based that, also datasets detecting these entities. as described paper evaluated ability participating entries classify named text."
https://openalex.org/W2965066169,https://doi.org/10.1609/aaai.v33i01.33018610,"Show, Attend and Read: A Simple and Strong Baseline for Irregular Text Recognition",2019,"Recognizing irregular text in natural scene images is challenging due to the large variance appearance, such as curvature, orientation and distortion. Most existing approaches rely heavily on sophisticated model designs and/or extra fine-grained annotations, which, some extent, increase difficulty algorithm implementation data collection. In this work, we propose an easy-to-implement strong baseline for recognition, using offthe-shelf neural network components only word-level annotations. It composed of a 31-layer ResNet, LSTMbased encoder-decoder framework 2-dimensional attention module. Despite its simplicity, proposed method robust. achieves state-of-the-art performance recognition benchmarks comparable results regular datasets. The code will be released."
https://openalex.org/W2207774637,https://doi.org/10.1016/j.autcon.2015.11.001,Automated content analysis for construction safety: A natural language processing system to extract precursors and outcomes from unstructured injury reports,2016,"Abstract In the United States like in many other countries throughout globe, construction workers are more likely to be injured on job than any industry. This poor safety performance is responsible for huge human and financial losses has motivated extensive research. Unfortunately, improvement decelerated last decade traditional programs have reached saturation. Yet major companies federal agencies possess a wealth of empirical knowledge form databases digital injury reports. could used better understand, predict, prevent occurrence accidents. due lack clear methodology high costs manual large-scale content analysis, these valuable data yet extracted leveraged. Recently, researchers proposed framework allowing meaningful from accident However, resource limitations inherent analysis still remain. The present study tested proposition that reports can eliminated using natural language processing (NLP). paper describes (1) overall strategy developing system, specifically how key challenges with decoding unstructured were overcome; (2) system was built through an iterative process coding testing against results team seven independent analysts; (3) implications potential uses extracted. indicate NLP capable quickly automatically scanning 101 attributes outcomes over 95% accuracy. main contribution this research empower organization obtain large highly reliable structured attribute outcome set their Such necessary prerequisite application statistical modeling techniques, extraction new finally amelioration management."
https://openalex.org/W2604178507,https://doi.org/10.1109/iccv.2017.445,Speaking the Same Language: Matching Machine to Human Captions by Adversarial Training,2017,"While strong progress has been made in image captioning recently, machine and human captions are still quite distinct. This is primarily due to the deficiencies generated word distribution, vocabulary size, bias generators towards frequent captions. Furthermore, humans – rightfully so generate multiple, diverse captions, inherent ambiguity task which not explicitly considered today's systems. To address these challenges, we change training objective of caption generator from reproducing ground-truth generating a set that indistinguishable written Instead handcrafting such learning target, employ adversarial combination with an approximate Gumbel sampler implicitly match distribution one. our method achieves comparable performance state-of-the-art terms correctness significantly less biased better global uni-, bi- tri-gram distributions"
https://openalex.org/W2607719644,https://doi.org/10.18653/v1/w17-1606,Gender and Dialect Bias in YouTube's Automatic Captions,2017,"This project evaluates the accuracy of YouTube’s automatically-generated captions across two genders and five dialect groups. Speakers’ gender was controlled for by using videos uploaded as part “accent tag challenge”, where speakers explicitly identify their language background. The results show robust differences in both dialect, with lower 1) women 2) from Scotland. finding builds on earlier research that speaker’s sociolinguistic identity may negatively impact ability to use automatic speech recognition, demonstrates need sociolinguistically-stratified validation systems."
https://openalex.org/W2735159761,https://doi.org/10.1007/s11263-017-1033-7,Uncovering the Temporal Context for Video Question Answering,2017,"In this work, we introduce Video Question Answering in the temporal domain to infer past, describe present and predict future. We an encoder---decoder approach using Recurrent Neural Networks learn structures of videos a dual-channel ranking loss answer multiple-choice questions. explore approaches for finer understanding video content question form fill-in-the-blank, collect our Context QA dataset consisting 109,895 clips with total duration more than 1000 h from existing TACoS, MPII-MD MEDTest 14 datasets. addition, 390,744 corresponding questions are generated annotations. Extensive experiments demonstrate that significantly outperforms compared baselines."
https://openalex.org/W2756816896,https://doi.org/10.18653/v1/d17-1310,Deep Multi-Task Learning for Aspect Term Extraction with Memory Interaction,2017,We propose a novel LSTM-based deep multi-task learning framework for aspect term extraction from user review sentences. Two LSTMs equipped with extended memories and neural memory operations are designed jointly handling the tasks of aspects opinions via interactions. Sentimental sentence constraint is also added more accurate prediction another LSTM. Experiment results over two benchmark datasets demonstrate effectiveness our framework.
https://openalex.org/W2806718802,https://doi.org/10.1145/3276517,DeepBugs: a learning approach to name-based bug detection,2018,"Natural language elements in source code, e.g., the names of variables and functions, convey useful information. However, most existing bug detection tools ignore this information therefore miss some classes bugs. The few name-based approaches reason about on a syntactic level rely manually designed tuned algorithms to detect This paper presents DeepBugs, learning approach detection, which reasons based semantic representation automatically learns detectors instead writing them. We formulate as binary classification problem train classifier that distinguishes correct from incorrect code. To address challenge effectively detector requires examples both we create likely code an corpus through simple transformations. A novel insight learned our work is artificially seeded bugs yields are effective at finding real-world implement idea into framework for learning-based detection. Three built top accidentally swapped function arguments, operators, operands operations. Applying 150,000 JavaScript files have high accuracy (between 89% 95%), very efficient (less than 20 milliseconds per analyzed file), reveal 102 programming mistakes (with 68% true positive rate)"
https://openalex.org/W2963283805,https://doi.org/10.18653/v1/p18-1152,Learning to Write with Cooperative Discriminators,2018,"Despite their local fluency, long-form text generated from RNNs is often generic, repetitive, and even self-contradictory. We propose a unified learning framework that collectively addresses all the above issues by composing committee of discriminators can guide base RNN generator towards more globally coherent generations. More concretely, each specialize in different principle communication, such as Grice’s maxims, are combined with through composite decoding objective. Human evaluation demonstrates our model preferred over baselines large margin, significantly enhancing overall coherence, style, information"
https://openalex.org/W2998230451,https://doi.org/10.1609/aaai.v34i05.6510,Semantics-Aware BERT for Language Understanding,2020,"The latest work on language representations carefully integrates contextualized features into model training, which enables a series of success especially in various machine reading comprehension and natural inference tasks. However, the existing representation models including ELMo, GPT BERT only exploit plain context-sensitive such as character or word embeddings. They rarely consider incorporating structured semantic information can provide rich semantics for representation. To promote understanding, we propose to incorporate explicit contextual from pre-trained role labeling, introduce an improved model, Semantics-aware (SemBERT), is capable explicitly absorbing over backbone. SemBERT keeps convenient usability its precursor light fine-tuning way without substantial task-specific modifications. Compared with BERT, semantics-aware simple concept but more powerful. It obtains new state-of-the-art substantially improves results ten"
https://openalex.org/W3034383590,https://doi.org/10.18653/v1/2020.acl-main.173,On Faithfulness and Factuality in Abstractive Summarization,2020,"It is well known that the standard likelihood training and approximate decoding objectives in neural text generation models lead to less human-like responses for open-ended tasks such as language modeling story generation. In this paper we have analyzed limitations of these abstractive document summarization found are highly prone hallucinate content unfaithful input document. We conducted a large scale human evaluation several systems better understand types hallucinations they produce. Our annotators substantial amounts hallucinated all model generated summaries. However, our analysis does show pretrained summarizers not only terms raw metrics, i.e., ROUGE, but also generating faithful factual summaries evaluated by humans. Furthermore, textual entailment measures correlate with faithfulness than potentially leading way automatic metrics criteria."
https://openalex.org/W3100806282,https://doi.org/10.18653/v1/2020.emnlp-main.365,Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation,2020,"We present an easy and efficient method to extend existing sentence embedding models new languages. This allows create multilingual versions from previously monolingual models. The training is based on the idea that a translated should be mapped same location in vector space as original sentence. use (monolingual) model generate embeddings for source language then train system sentences mimic model. Compared other methods embeddings, this approach has several advantages: It with relatively few samples languages, it easier ensure desired properties space, hardware requirements are lower. demonstrate effectiveness of our 50+ languages various families. Code more than 400 publicly available."
https://openalex.org/W1488659701,https://doi.org/10.3389/fnins.2015.00217,Brain-to-text: decoding spoken phrases from phone representations in the brain,2015,"It has long been speculated whether communication between humans and machines based on natural speech related cortical activity is possible. Over the past decade, studies have suggested that it feasible to recognize isolated aspects of from neural signals, such as auditory features, phones or one a few words. However, until now remained an unsolved challenge decode continuously spoken substrate associated with language processing. Here, we show for first time can be decoded into expressed words intracranial electrocorticographic (ECoG) recordings.Specifically, implemented system, which call Brain-To-Text models single phones, employs techniques automatic recognition (ASR), thereby transforms brain while speaking corresponding textual representation. Our results demonstrate our system achieve word error rates low 25% phone below 50%. Additionally, approach contributes current understanding basis continuous production by identifying those regions hold substantial information about individual phones. In conclusion, described in this paper represents important step toward human-machine imagined speech."
https://openalex.org/W2041532239,https://doi.org/10.1007/s10579-014-9287-y,A massively parallel corpus: the Bible in 100 languages,2015,We describe the creation of a massively parallel corpus based on 100 translations Bible. discuss some difficulties in acquiring and processing raw material as well potential Bible for natural language processing. Finally we present statistical analysis corpora collected detailed comparison between English translation other corpora.
https://openalex.org/W2150815390,https://doi.org/10.1145/2723372.2751523,Mining Quality Phrases from Massive Text Corpora,2015,"Text data are ubiquitous and play an essential role in big applications. However, text mostly unstructured. Transforming unstructured into structured units (e.g., semantically meaningful phrases) will substantially reduce semantic ambiguity enhance the power efficiency at manipulating such using database technology. Thus mining quality phrases is a critical research problem field of databases. In this paper, we propose new framework that extracts from corpora integrated with phrasal segmentation. The requires only limited training but so generated close to human judgment. Moreover, method scalable: both computation time required space grow linearly as corpus size increases. Our experiments on large demonstrate method."
https://openalex.org/W2339652278,https://doi.org/10.1145/2964284.2964299,Image Captioning with Deep Bidirectional LSTMs,2016,"This work presents an end-to-end trainable deep bidirectional LSTM (Long-Short Term Memory) model for image captioning. Our builds on a convolutional neural network (CNN) and two separate networks. It is capable of learning long term visual-language interactions by making use history future context information at high level semantic space. Two novel variant models, in which we increase the depth nonlinearity transition different way, are proposed to learn hierarchical embeddings. Data augmentation techniques such as multi-crop, multi-scale vertical mirror prevent overfitting training models. We visualize evolution internal states over time qualitatively analyze how our models translate sentence. evaluated caption generation image-sentence retrieval tasks with three benchmark datasets: Flickr8K, Flickr30K MSCOCO datasets. demonstrate that achieve highly competitive performance state-of-the-art results even without integrating additional mechanism (e.g. object detection, attention etc.) significantly outperform recent methods task"
https://openalex.org/W2741494657,https://doi.org/10.18653/v1/p17-1074,Automatic Annotation and Evaluation of Error Types for Grammatical Error Correction,2017,"Until now, error type performance for Grammatical Error Correction (GEC) systems could only be measured in terms of recall because system output is not annotated. To overcome this problem, we introduce ERRANT, a grammatical ERRor ANnotation Toolkit designed to automatically extract edits from parallel original and corrected sentences classify them according new, dataset-agnostic, rule-based framework. This facilitates evaluation at different levels granularity, but can also used reduce annotator workload standardise existing GEC datasets. Human experts rated the automatic as “Good” or “Acceptable” least 95% cases, so applied ERRANT CoNLL-2014 shared task carry out detailed analysis first time."
https://openalex.org/W2782213998,https://doi.org/10.1038/s41467-018-03068-4,Toward a universal decoder of linguistic meaning from brain activation,2018,"Prior work decoding linguistic meaning from imaging data has been largely limited to concrete nouns, using similar stimuli for training and testing, a relatively small number of semantic categories. Here we present new approach building brain system in which words sentences are represented as vectors space constructed massive text corpora. By efficiently sampling this select shown subjects, maximize the ability generalize meanings data. To validate approach, train on individual concepts, show it can decode vector representations about wide variety both abstract topics two separate datasets. These decoded sufficiently detailed distinguish even semantically sentences, capture similarity structure relationships between sentences."
https://openalex.org/W2915240437,https://doi.org/10.18653/v1/s17-2003,SemEval-2017 Task 3: Community Question Answering,2017,"We describe SemEval–2017 Task 3 on Community Question Answering. This year, we reran the four subtasks from SemEval-2016: (A) Question–Comment Similarity, (B) Question–Question (C) Question–External Comment and (D) Rerank correct answers for a new question in Arabic, providing all data 2015 2016 training, fresh testing. Additionally, added subtask E order to enable experimentation with Multi-domain Duplicate Detection larger-scale scenario, using StackExchange subforums. A total of 23 teams participated task, submitted 85 runs (36 primary 49 contrastive) A–D. Unfortunately, no E. variety approaches features were used by participating systems address different subtasks. The best achieved an official score (MAP) 88.43, 47.22, 15.46, 61.16 A, B, C, D, respectively. These scores are better than baselines, especially A–C."
https://openalex.org/W2963333747,https://doi.org/10.18653/v1/n16-1102,Incorporating Structural Alignment Biases into an Attentional Neural Translation Model,2016,"Neural encoder-decoder models of machine translation have achieved impressive results, rivalling traditional models. However their modelling formulation is overly simplistic, and omits several key inductive biases built into In this paper we extend the attentional neural model to include structural from word based alignment models, including positional bias, Markov conditioning, fertility agreement over directions. We show improvements a baseline standard phrase-based language pairs, evaluating on difficult languages in low resource setting."
https://openalex.org/W3045462440,https://doi.org/10.1162/tacl_a_00317,T<scp>y</scp>D<scp>i</scp> QA: A Benchmark for Information-Seeking Question Answering in <i>Ty</i>pologically <i>Di</i>verse Languages,2020,"Confidently making progress on multilingual modeling requires challenging, trustworthy evaluations. We present TyDi QA—a question answering dataset covering 11 typologically diverse languages with 204K question-answer pairs. The of QA are regard to their typology—the set linguistic features each language expresses—such that we expect models performing well this generalize across a large number the world’s languages. quantitative analysis data quality and example-level qualitative analyses observed phenomena would not be found in English-only corpora. To provide realistic information-seeking task avoid priming effects, questions written by people who want know answer, but don’t answer yet, is collected directly without use translation."
https://openalex.org/W3104486441,https://doi.org/10.18653/v1/p17-1055,Attention-over-Attention Neural Networks for Reading Comprehension,2017,"Cloze-style queries are representative problems in reading comprehension. Over the past few months, we have seen much progress that utilizing neural network approach to solve questions. In this paper, present a novel model called attention-over-attention reader for comprehension task. Our aims place another attention mechanism over document-level attention, and induces ""attended attention"" final predictions. Unlike previous works, our requires less pre-defined hyper-parameters uses an elegant architecture modeling. Experimental results show proposed significantly outperforms various state-of-the-art systems by large margin public datasets, such as CNN Children's Book Test datasets."
https://openalex.org/W2468484304,https://doi.org/10.18653/v1/s16-1083,SemEval-2016 Task 3: Community Question Answering,2016,"This paper describes the SemEval–2016 Task 3 on Community Question Answering, which we offered in English and Arabic. For English, had three subtasks: Question–Comment Similarity (subtask A), Question–Question (B), Question–External Comment (C). Arabic, another subtask: Rerank correct answers for a new question (D). Eighteen teams participated task, submitting total of 95 runs (38 primary 57 contrastive) four subtasks. A variety approaches features were used by participating systems to address different subtasks, are summarized this paper. The best achieved an official score (MAP) 79.19, 76.70, 55.41, 45.83 subtasks A, B, C, D, respectively. These scores significantly better than those baselines that provided. subtask system improved over 2015 winner points absolute terms Accuracy."
https://openalex.org/W2565656701,https://doi.org/10.1109/cvpr.2017.347,"End-to-End Concept Word Detection for Video Captioning, Retrieval, and Question Answering",2017,"We propose a high-level concept word detector that can be integrated with any video-to-language models. It takes video as input and generates list of words useful semantic priors for language generation The proposed has two important properties. First, it does not require external knowledge sources training. Second, the is trainable in an end-to-end manner jointly To maximize values detected words, we also develop attention mechanism selectively focuses on fuse them encoding decoding model. In order to demonstrate approach indeed improves performance multiple tasks, participate four tasks LSMDC 2016. Our achieves best accuracies three them, including fill-in-the-blank, multiple-choice test, movie retrieval. attain comparable other task, description."
https://openalex.org/W2963419157,https://doi.org/10.18653/v1/w16-2506,Problems With Evaluation of Word Embeddings Using Word Similarity Tasks,2016,"Lacking standardized extrinsic evaluation methods for vector representations of words, the NLP community has relied heavily on word similarity tasks as a proxy intrinsic vectors. Word evaluation, which correlates distance between vectors and human judgments “semantic similarity” is attractive, because it computationally inexpensive fast. In this paper we present several problems associated with datasets, summarize existing solutions. Our study suggests that use not sustainable calls further research methods."
https://openalex.org/W2964067226,https://doi.org/10.1109/cvpr.2019.00680,Dynamic Fusion With Intra- and Inter-Modality Attention Flow for Visual Question Answering,2019,"Learning effective fusion of multi-modality features is at the heart visual question answering. We propose a novel method dynamically fusing multi-modal with intra- and inter-modality information flow, which alternatively pass dynamic between across language modalities. It can robustly capture high-level interactions vision domains, thus significantly improves performance also show that proposed intra-modality attention flow conditioned on other modality modulate target modality, vital for multimodality feature fusion. Experimental evaluations VQA 2.0 dataset achieves state-of-the-art performance. Extensive ablation studies are carried out comprehensive analysis method."
https://openalex.org/W2964152081,https://doi.org/10.18653/v1/e17-1013,"Chains of Reasoning over Entities, Relations, and Text using Recurrent Neural Networks",2017,"Our goal is to combine the rich multi-step inference of symbolic logical reasoning with generalization capabilities neural networks. We are particularly interested in complex about entities and relations text large-scale knowledge bases (KBs). Neelakantan et al. (2015) use RNNs compose distributed semantics multi-hop paths KBs; however for multiple reasons, approach lacks accuracy practicality. This paper proposes three significant modeling advances: (1) we learn jointly reason relations, entities, entity-types; (2) attention incorporate paths; (3) share strength a single RNN that represents composition across all relations. On Freebase+ClueWeb prediction task, achieve 25% error reduction, 53% reduction on sparse due shared strength. chains WordNet reduce mean quantile by 84% versus previous state-of-the-art."
https://openalex.org/W2972413484,https://doi.org/10.18653/v1/w19-3823,Measuring Bias in Contextualized Word Representations,2019,"Contextual word embeddings such as BERT have achieved state of the art performance in numerous NLP tasks. Since they are optimized to capture statistical properties training data, tend pick up on and amplify social stereotypes present data well. In this study, we (1)~propose a template-based method quantify bias BERT; (2)~show that obtains more consistent results capturing biases than traditional cosine based method; (3)~conduct case evaluating gender downstream task Gender Pronoun Resolution. Although our study focuses bias, proposed technique is generalizable unveiling other biases, including multiclass settings, racial religious biases."
https://openalex.org/W3153427360,https://doi.org/10.18653/v1/2021.eacl-main.20,Exploiting Cloze-Questions for Few-Shot Text Classification and Natural Language Inference,2021,"Some NLP tasks can be solved in a fully unsupervised fashion by providing pretrained language model with task descriptions natural (e.g., Radford et al., 2019). While this approach underperforms its supervised counterpart, we show work that the two ideas combined: We introduce Pattern-Exploiting Training (PET), semi-supervised training procedure reformulates input examples as cloze-style phrases to help models understand given task. These are then used assign soft labels large set of unlabeled examples. Finally, standard is performed on resulting set. For several and languages, PET outperforms strong approaches low-resource settings margin."
https://openalex.org/W3177265267,https://doi.org/10.1109/emc2-nips53020.2019.00016,Q8BERT: Quantized 8Bit BERT,2019,"Recently, pre-trained Transformer based language models such as BERT and GPT, have shown great improvement in many Natural Language Processing (NLP) tasks. However, these contain a large amount of parameters. The emergence even larger more accurate GPT2 Megatron, suggest trend models. using production environments is complex task requiring compute, memory power resources. In this work we show how to perform quantization-aware training during the fine-tuning phase order compress by $4\times$ with minimal accuracy loss. Furthermore, produced quantized model can accelerate inference speed if it optimized for 8bit Integer supporting hardware."
https://openalex.org/W2251792193,https://doi.org/10.18653/v1/d15-1298,PhraseRNN: Phrase Recursive Neural Network for Aspect-based Sentiment Analysis,2015,This paper presents a new method to identify sentiment of an aspect entity. It is extension RNN (Recursive Neural Network) that takes both dependency and constituent trees sentence into account. Results experiment show our significantly outperforms previous methods.
https://openalex.org/W2470324779,https://doi.org/10.18653/v1/n16-1042,Grammatical error correction using neural machine translation,2016,"This paper presents the first study using neural machine translation (NMT) for grammatical error correction (GEC). We propose a twostep approach to handle rare word problem in NMT, which has been proved be useful and effective GEC task. Our best NMTbased system trained on CLC outperforms our SMT-based when testing publicly available FCE test set. The same achieves an F0.5 score of 39.90% CoNLL-2014 shared task set, outperforming state-of-the-art demonstrating that NMT-based generalises effectively."
https://openalex.org/W2605202003,https://doi.org/10.1609/aaai.v31i1.10742,DeepFix: Fixing Common C Language Errors by Deep Learning,2017,"The problem of automatically fixing programming errors is a very active research topic in software engineering. This challenging as even single error may require analysis the entire program. In practice, number arise due to programmer's inexperience with language or lack attention detail. We call these common errors. These are analogous grammatical natural languages. Compilers detect such errors, but their messages usually inaccurate. this work, we present an end-to-end solution, called DeepFix, that can fix multiple program without relying on any external tool locate them. At heart DeepFix multi-layered sequence-to-sequence neural network which trained predict erroneous locations along required correct statements. On set 6971 C programs written by students for 93 tasks, could 1881 (27%) completely and 1338 (19%) partially."
https://openalex.org/W2741956709,https://doi.org/10.18653/v1/p17-1085,Going out on a limb: Joint Extraction of Entity Mentions and Relations without Dependency Trees,2017,"We present a novel attention-based recurrent neural network for joint extraction of entity mentions and relations. show that attention along with long short term memory (LSTM) can extract semantic relations between without having access to dependency trees. Experiments on Automatic Content Extraction (ACE) corpora our model significantly outperforms feature-based by Li Ji (2014). also compare an end-to-end tree-based LSTM (SPTree) Miwa Bansal (2016) performs within 1% 2% Our fine-grained analysis shows better Agent-Artifact relations, while SPTree Physical Part-Whole"
https://openalex.org/W2802787326,https://doi.org/10.1016/j.eswa.2018.03.058,A recent overview of the state-of-the-art elements of text classification,2018,"Abstract The aim of this study is to provide an overview the state-of-the-art elements text classification. For purpose, we first select and investigate primary recent studies objectives in field. Next, examine In following steps, qualitatively quantitatively analyse related works. Herein, describe six baseline classification including data collection, analysis for labelling, feature construction weighing, selection projection, training a model, solution evaluation. This will help readers acquire necessary information about these their associated techniques. Thus, believe that assist other researchers professionals propose new field"
https://openalex.org/W2963088785,https://doi.org/10.1609/aaai.v33i01.33013159,Character-Level Language Modeling with Deeper Self-Attention,2019,"LSTMs and other RNN variants have shown strong performance on character-level language modeling. These models are typically trained using truncated backpropagation through time, it is common to assume that their success stems from ability remember long-term contexts. In this paper, we show a deep (64-layer) transformer model (Vaswani et al. 2017) with fixed context outperforms by large margin, achieving state of the art two popular benchmarks: 1.13 bits per character text8 1.06 enwik8. To get good results at depth, important add auxiliary losses, both intermediate network layers sequence positions."
https://openalex.org/W2963339489,https://doi.org/10.1093/bioinformatics/bty869,Cross-type biomedical named entity recognition with deep multi-task learning,2019,"State-of-the-art biomedical named entity recognition (BioNER) systems often require handcrafted features specific to each type, such as genes, chemicals and diseases. Although recent studies explored using neural network models for BioNER free experts from manual feature engineering, the performance remains limited by available training data type.We propose a multi-task learning framework collectively use of different types entities improve on them. In experiments 15 benchmark datasets, our model achieves substantially better compared with state-of-the-art baseline sequence labeling models. Further analysis shows that large gains come sharing character- word-level information among relevant across differently labeled corpora.Our source code is at https://github.com/yuzhimanhua/lm-lstm-crf.Supplementary are Bioinformatics online."
https://openalex.org/W833103999,https://doi.org/10.1016/j.dr.2015.05.002,"Statistical learning of language: Theory, validity, and predictions of a statistical learning account of language acquisition",2015,"• Assesses claim that statistical learning contributes to language acquisition. Evaluates ecological validity of experiments. Explores the explanatory power for developmental disorders. Considerable research indicates learners are sensitive probabilistic structure in laboratory studies artificial learning. However, and simplified nature stimuli used pioneering work on acquisition regularities has raised doubts about scalability such complexity natural input. In this review, we explore a central prediction accounts – sensitivity should be linked real processes via an examination of: (1) recent have increased stimuli; (2) suggest segmentation produces representations share properties with words; (3) correlations between individual variability ability outcomes; (4) atypicalities clinical populations characterized by delays or deficits."
https://openalex.org/W2131546905,https://doi.org/10.1016/j.jbi.2015.03.010,Cadec: A corpus of adverse drug event annotations,2015,"• Introduction of CADEC an annotated corpus consumer reviews in pharmacovigilance. A review and comparison available relevant resources. Challenges lessons from the process creating such CSIRO Adverse Drug Event Corpus (C adec ) is a new rich medical forum posts on patient-reported Events (ADEs). The sourced social media, contains text that largely written colloquial language often deviates formal English grammar punctuation rules. Annotations contain mentions concepts as drugs, adverse effects, symptoms, diseases linked to their corresponding controlled vocabularies, i.e., SNOMED Clinical Terms MedDRA. quality annotations ensured by annotation guidelines, multi-stage annotations, measuring inter-annotator agreement, final clinical terminologist. This useful for studies area information extraction, or more generally mining, media detect possible drug reactions direct patient reports. publicly at https://data.csiro.au . 1 data can be used research purposes only, under licence."
https://openalex.org/W2187445524,https://doi.org/10.1073/pnas.1509321112,Universal brain signature of proficient reading: Evidence from four contrasting languages,2015,"Significance Using functional MRI, we examined reading and speech perception in four highly contrasting languages: Spanish, English, Hebrew, Chinese. With three complementary analytic approaches, demonstrate that spite of striking dissimilarities among writing systems, successful literacy acquisition results a convergence the orthographic processing systems onto common network neural structures. These findings have major theoretical implication has evolved to be universally constrained by organization brain underlying speech."
https://openalex.org/W2600659824,https://doi.org/10.1186/s12859-017-1609-9,A neural joint model for entity and relation extraction from biomedical text,2017,"Extracting biomedical entities and their relations from text has important applications on research. Previous work primarily utilized feature-based pipeline models to process this task. Many efforts need be made feature engineering when are employed. Moreover, may suffer error propagation not able utilize the interactions between subtasks. Therefore, we propose a neural joint model extract as well simultaneously, it can alleviate problems above.Our was evaluated two tasks, i.e., task of extracting adverse drug events disease entities, resident bacteria location entities. Compared with state-of-the-art systems in these our improved F1 scores first by 5.1% entity recognition 8.0% relation extraction, that second 9.2% extraction.The proposed achieves competitive performances less engineering. We demonstrate based networks is effective for extraction. In addition, parameter sharing an alternative method jointly Our facilitate research mining."
https://openalex.org/W2920114910,https://doi.org/10.1162/tacl_a_00240,Mind the GAP: A Balanced Corpus of Gendered Ambiguous Pronouns,2018,"Coreference resolution is an important task for natural language understanding, and the of ambiguous pronouns a longstanding challenge. Nonetheless, existing corpora do not capture in sufficient volume or diversity to accurately indicate practical utility models. Furthermore, we find gender bias systems favoring masculine entities. To address this, present release GAP, gender-balanced labeled corpus 8,908 pronoun–name pairs sampled provide diverse coverage challenges posed by real-world text. We explore range baselines that demonstrate complexity challenge, best achieving just 66.9% F1. show syntactic structure continuous neural models promising, complementary cues approaching"
https://openalex.org/W2963951265,https://doi.org/10.18653/v1/p18-1027,"Sharp Nearby, Fuzzy Far Away: How Neural Language Models Use Context",2018,"We know very little about how neural language models (LM) use prior linguistic context. In this paper, we investigate the role of context in an LSTM LM, through ablation studies. Specifically, analyze increase perplexity when words are shuffled, replaced, or dropped. On two standard datasets, Penn Treebank and WikiText-2, find that model is capable using 200 tokens on average, but sharply distinguishes nearby (recent 50 tokens) from distant history. The highly sensitive to order within most recent sentence, ignores word long-range (beyond tokens), suggesting past modeled only as a rough semantic field topic. further caching (Grave et al., 2017b) especially helps copy Overall, our analysis not provides better understanding LMs their context, also sheds light success cache-based models."
https://openalex.org/W2964222268,https://doi.org/10.18653/v1/w18-5426,Under the Hood: Using Diagnostic Classifiers to Investigate and Improve how Language Models Track Agreement Information,2018,"How do neural language models keep track of number agreement between subject and verb? We show that ‘diagnostic classifiers’, trained to predict from the internal states a model, provide detailed understanding how, when, where this information is represented. Moreover, they give us insight into when corrupted in cases model ends up making errors. To demonstrate causal role played by representations we find, then use influence course LSTM during processing difficult sentences. Results such an intervention reveal large increase model’s accuracy. Together, these results diagnostic classifiers unrivalled look representation linguistic models, knowledge can be used improve their performance."
https://openalex.org/W2964288660,https://doi.org/10.18653/v1/d16-1103,A Hierarchical Model of Reviews for Aspect-based Sentiment Analysis,2016,"Opinion mining from customer reviews has become pervasive in recent years. Sentences reviews, however, are usually classified independently, even though they form part of a review's argumentative structure. Intuitively, sentences review build and elaborate upon each other; knowledge the structure sentential context should thus inform classification sentence. We demonstrate this hypothesis for task aspect-based sentiment analysis by modeling interdependencies with hierarchical bidirectional LSTM. show that model outperforms two non-hierarchical baselines, obtains results competitive state-of-the-art, state-of-the-art on five multilingual, multi-domain datasets without any hand-engineered features or external resources."
https://openalex.org/W2970487286,https://doi.org/10.18653/v1/d19-1565,Fine-Grained Analysis of Propaganda in News Article,2019,"Propaganda aims at influencing people’s mindset with the purpose of advancing a specific agenda. Previous work has addressed propaganda detection document level, typically labelling all articles from propagandistic news outlet as propaganda. Such noisy gold labels inevitably affect quality any learning system trained on them. A further issue most existing systems is lack explainability. To overcome these limitations, we propose novel task: performing fine-grained analysis texts by detecting fragments that contain techniques well their type. In particular, create corpus manually annotated fragment level eighteen and suitable evaluation measure. We design multi-granularity neural network, show it outperforms several strong BERT-based baselines."
https://openalex.org/W2971307358,https://doi.org/10.18653/v1/d19-1339,The Woman Worked as a Babysitter: On Biases in Language Generation,2019,"We present a systematic study of biases in natural language generation (NLG) by analyzing text generated from prompts that contain mentions different demographic groups. In this work, we introduce the notion regard towards demographic, use varying levels demographics as defining metric for bias NLG, and analyze extent to which sentiment scores are relevant proxy regard. To end, collect strategically-generated models manually annotate with both scores. Additionally, build an automatic classifier through transfer learning, so can unseen text. Together, these methods reveal biased nature model generations. Our analysis provides metrics correlated human judgments, empirical evidence on usefulness our annotated dataset."
https://openalex.org/W3035032094,https://doi.org/10.18653/v1/2020.acl-main.560,The State and Fate of Linguistic Diversity and Inclusion in the NLP World,2020,"Language technologies contribute to promoting multilingualism and linguistic diversity around the world. However, only a very small number of over 7000 languages world are represented in rapidly evolving language applications. In this paper we look at relation between types languages, resources, their representation NLP conferences understand trajectory that different have followed time. Our quantitative investigation underlines disparity especially terms calls into question “language agnostic” status current models systems. Through paper, attempt convince ACL community prioritise resolution predicaments highlighted here, so no is left behind."
https://openalex.org/W3098325931,https://doi.org/10.18653/v1/p18-1240,On the Automatic Generation of Medical Imaging Reports,2018,"Medical imaging is widely used in clinical practice for diagnosis and treatment. Report-writing can be error-prone unexperienced physicians, time- consuming tedious experienced physicians. To address these issues, we study the automatic generation of medical reports. This task presents several challenges. First, a complete report contains multiple heterogeneous forms information, including findings tags. Second, abnormal regions images are difficult to identify. Third, re- ports typically long, containing sentences. cope with challenges, (1) build multi-task learning framework which jointly performs pre- diction tags para- graphs, (2) propose co-attention mechanism localize abnormalities generate narrations them, (3) develop hierarchical LSTM model long paragraphs. We demonstrate effectiveness proposed methods on two publicly available datasets."
https://openalex.org/W2291962372,https://doi.org/10.1007/s41066-015-0006-x,Managing multi-granularity linguistic information in qualitative group decision making: an overview,2016,"Linguistic term sets are usually preferred to represent evaluations and preferences in qualitative group decision making (QGDM). Due the different backgrounds levels of knowledge experts, linguistic with cardinality and/or semantics used. Hence, it is vital manage these terms from distinct information sources QGDM. In this paper, we present a comprehensive review on relative developments multi-granularity by several perspectives, such as transformation techniques, aggregation functions, processes applications. Finally, some possible directions for future research pointed out well."
https://openalex.org/W2801930304,https://doi.org/10.18653/v1/n18-3011,Construction of the Literature Graph in Semantic Scholar,2018,"We describe a deployed scalable system for organizing published scientific literature into heterogeneous graph to facilitate algorithmic manipulation and discovery. The resulting consists of more than 280M nodes, representing papers, authors, entities various interactions between them (e.g., authorships, citations, entity mentions). reduce construction familiar NLP tasks extraction linking), point out research challenges due differences from standard formulations these tasks, report empirical results each task. methods described in this paper are used enable semantic features www.semanticscholar.org."
https://openalex.org/W2962816513,https://doi.org/10.18653/v1/d18-1407,Pathologies of Neural Models Make Interpretations Difficult,2018,"One way to interpret neural model predictions is highlight the most important input features---for example, a heatmap visualization over words in an sentence. In existing interpretation methods for NLP, word's importance determined by either perturbation---measuring decrease confidence when that word removed---or gradient with respect word. To understand limitations of these methods, we use reduction, which iteratively removes least from input. This exposes pathological behaviors models: remaining appear nonsensical humans and are not ones as methods. As confirm human experiments, reduced examples lack information support prediction any label, but models still make same high confidence. explain counterintuitive results, draw connections adversarial calibration: reveal difficulties interpreting trained maximum likelihood. mitigate their deficiencies, fine-tune encouraging entropy outputs on examples. Fine-tuned become more interpretable under reduction without accuracy loss regular"
https://openalex.org/W2963393391,https://doi.org/10.1609/aaai.v33i01.33019159,To Find Where You Talk: Temporal Sentence Localization in Video with Attention Based Location Regression,2019,"We have witnessed the tremendous growth of videos over Internet, where most these are typically paired with abundant sentence descriptions, such as video titles, captions and comments. Therefore, it has been increasingly crucial to associate specific segments corresponding informative text for a deeper understanding content. This motivates us explore an overlooked problem in research community — temporal localization video, which aims automatically determine start end points given within video. For solving this problem, we face three critical challenges: (1) preserving intrinsic structure global context locate accurate positions entire sequence; (2) fully exploring semantics give clear guidance localization; (3) ensuring efficiency method adapt long videos. To address issues, propose novel Attention Based Location Regression (ABLR) approach localize descriptions efficient end-to-end manner. Specifically, preserve information, ABLR first encodes both via Bi-directional LSTM networks. Then, multi-modal co-attention mechanism is presented generate attentions. The former reflects structure, while latter highlights details localization. Finally, attention based location prediction network designed regress coordinates from previous evaluate proposed on two public datasets ActivityNet Captions TACoS. Experimental results show that significantly outperforms existing approaches effectiveness efficiency."
https://openalex.org/W2964010755,https://doi.org/10.1109/cvpr.2018.00609,Single-Shot Object Detection with Enriched Semantics,2018,"We propose a novel single shot object detection network named Detection with Enriched Semantics (DES). Our motivation is to enrich the semantics of features within typical deep detector, by semantic segmentation branch and global activation module. The supervised weak ground-truth, i.e., no extra annotation required. In conjunction that, we employ module which learns relationship between channels classes in self-supervised manner. Comprehensive experimental results on both PASCAL VOC MS COCO datasets demonstrate effectiveness proposed method. particular, VGG16 based DES, achieve an mAP 81.7 VOC2007 test 32.8 test-dev inference speed 31.5 milliseconds per image Titan Xp GPU. With lower resolution version, 79.7 13.0 image."
https://openalex.org/W2993873509,https://doi.org/10.1093/jamia/ocz200,Deep learning in clinical natural language processing: a methodical review,2020,"Abstract Objective This article methodically reviews the literature on deep learning (DL) for natural language processing (NLP) in clinical domain, providing quantitative analysis to answer 3 research questions concerning methods, scope, and context of current research. Materials Methods We searched MEDLINE, EMBASE, Scopus, Association Computing Machinery Digital Library, Computational Linguistics Anthology articles using DL-based approaches NLP problems electronic health records. After screening 1,737 articles, we collected data 25 variables across 212 papers. Results DL publications more than doubled each year, through 2018. Recurrent neural networks (60.8%) word2vec embeddings (74.1%) were most popular methods; information extraction tasks text classification, named entity recognition, relation dominant (89.2%). However, there was a “long tail” other methods specific tasks. Most contributions methodological variants or applications, but 20.8% new some kind. The earliest adopters community, medical informatics community prolific. Discussion Our shows growing acceptance as baseline research, community. A number common associations substantiated (eg, preference recurrent sequence-labeling recognition), while others surprisingly nuanced scarcity French with learning). Conclusion Deep has not yet fully penetrated is rapidly. review highlighted both unique trends this active field."
https://openalex.org/W2250623140,https://doi.org/10.18653/v1/d15-1198,Broad-coverage CCG Semantic Parsing with AMR,2015,"We propose a grammar induction technique for AMR semantic parsing. While previous techniques were designed to re-learn new parser each target application, the recently annotated Bank provides unique opportunity induce single model understanding broad-coverage newswire text and support wide range of applications. present that combines CCG parsing recover compositional aspects meaning factor graph non-compositional phenomena, such as anaphoric dependencies. Our approach achieves 66.2 Smatch F1 score on bank, significantly outperforming state art."
https://openalex.org/W2291723583,https://doi.org/10.18653/v1/n16-1015,Multi-domain Neural Network Language Generation for Spoken Dialogue Systems,2016,"Moving from limited-domain natural language generation (NLG) to open domain is difficult because the number of semantic input combinations grows exponentially with domains. Therefore, it important leverage existing resources and exploit similarities between domains facilitate adaptation. In this paper, we propose a procedure train multi-domain, Recurrent Neural Network-based (RNN) generators via multiple adaptation steps. procedure, model first trained on counterfeited data synthesised an out-of-domain dataset, then fine tuned small set in-domain utterances discriminative objective function. Corpus-based evaluation results show that proposed can achieve competitive performance in terms BLEU score slot error rate while significantly reducing needed new, unseen subjective testing, human judges confirm greatly improves generator when only amount available domain."
https://openalex.org/W2510317721,https://doi.org/10.1145/2959100.2959180,<i>Ask the GRU</i>,2016,"In a variety of application domains the content to be recommended users is associated with text. This includes research papers, movies plot summaries, news articles, blog posts, etc. Recommendation approaches based on latent factor models can extended naturally leverage text by employing an explicit mapping from factors. enables recommendations for new, unseen content, and may generalize better, since factors all items are produced compactly-parametrized model. Previous work has used topic or averages word embeddings this mapping. paper we present method leveraging deep recurrent neural networks encode sequence into vector, specifically gated units (GRUs) trained end-to-end collaborative filtering task. For task scientific recommendation, yields significantly higher accuracy. cold-start scenarios, beat previous state-of-the-art, which ignore order. Performance further improved multi-task learning, where encoder network combination recommendation item metadata prediction. regularizes model, ameliorating problem sparsity observed rating matrix."
https://openalex.org/W2526471240,https://doi.org/10.18653/v1/d16-1031,Language as a Latent Variable: Discrete Generative Models for Sentence Compression,2016,"In this work we explore deep generative models of text in which the latent representation a document is itself drawn from discrete language model distribution. We formulate variational auto-encoder for inference and apply it to task compressing sentences. application first draws summary sentence background model, then subsequently observed conditioned on summary. our empirical evaluation show that formulations both abstractive extractive compression yield state-of-the-art results when trained large amount supervised data. Further, semi-supervised scenarios where possible achieve performance competitive with previously proposed while training fraction"
https://openalex.org/W2792883466,https://doi.org/10.3390/mca23010011,Machine Learning-Based Sentiment Analysis for Twitter Accounts,2018,"Growth in the area of opinion mining and sentiment analysis has been rapid aims to explore opinions or text present on different platforms social media through machine-learning techniques with sentiment, subjectivity polarity calculations. Despite use various tools for during elections, there is a dire need state-of-the-art approach. To deal these challenges, contribution this paper includes adoption hybrid approach that involves analyzer machine learning. Moreover, also provides comparison political views by applying supervised algorithms such as Naive Bayes support vector machines (SVM)."
https://openalex.org/W2898339904,https://doi.org/10.1016/j.ijresmar.2018.09.009,Comparing automated text classification methods,2019,"Abstract Online social media drive the growth of unstructured text data. Many marketing applications require structuring this data at scales non-accessible to human coding, e.g., detect communication shifts in sentiment or other researcher-defined content categories. Several methods have been proposed automatically classify text. This paper compares performance ten such approaches (five lexicon-based, five machine learning algorithms) across 41 datasets covering major platforms, various sample sizes, and languages. So far, research relies predominantly on support vector machines (SVM) Linguistic Inquiry Word Count (LIWC). Across all tasks we study, either random forest (RF) naive Bayes (NB) performs best terms correctly uncovering intuition. In particular, RF exhibits consistently high for three-class sentiment, NB small samples sizes. SVM never outperform remaining methods. All lexicon-based approaches, LIWC perform poorly compared with learning. some applications, accuracies only slightly exceed chance. Since additional considerations classification choice are also favor RF, our results suggest that can benefit from considering these alternatives."
https://openalex.org/W2912971066,https://doi.org/10.1093/jamia/ocy173,Natural language processing of symptoms documented in free-text narratives of electronic health records: a systematic review,2019,"Abstract Objective Natural language processing (NLP) of symptoms from electronic health records (EHRs) could contribute to the advancement symptom science. We aim synthesize literature on use NLP process or analyze information documented in EHR free-text narratives. Materials and Methods Our search 1964 PubMed EMBASE was narrowed 27 eligible articles. Data related purpose, corpus, patients, symptoms, methodology, evaluation metrics, quality indicators were extracted for each study. Results Symptom-related presented as a primary outcome 14 studies. narratives represented various inpatient outpatient clinical specialties, with general, cardiology, mental occurring most frequently. Studies encompassed wide variety including shortness breath, pain, nausea, dizziness, disturbed sleep, constipation, depressed mood. approaches included previously developed tools, classification methods, manually curated rule-based processing. Only one-third (n = 9) studies reported patient demographic characteristics. Discussion is used extract written by healthcare providers an expansive range across diverse specialties. The current focus this field development methods disease tasks rather than examination themselves. Conclusion Future should concentrate investigation documentation Efforts be undertaken examine characteristics make symptom-related algorithms pipelines vocabularies openly available."
https://openalex.org/W2964193968,https://doi.org/10.18653/v1/e17-1110,Distant Supervision for Relation Extraction beyond the Sentence Boundary,2017,"The growing demand for structured knowledge has led to great interest in relation extraction, especially cases with limited supervision. However, existing distance supervision approaches only extract relations expressed single sentences. In general, cross-sentence extraction is under-explored, even the supervised-learning setting. this paper, we propose first approach applying distant extraction. At core of our a graph representation that can incorporate both standard dependencies and discourse relations, thus providing unifying way model within across We features from multiple paths graph, increasing accuracy robustness when confronted linguistic variation analysis error. Experiments on an important task precision medicine show learn accurate extractor, using small base unlabeled text biomedical research articles. Compared paradigm, extracted twice as many at similar precision, demonstrating prevalence promise approach."
https://openalex.org/W2964262738,https://doi.org/10.1162/tacl_a_00111,Large-scale Analysis of Counseling Conversations: An Application of Natural Language Processing to Mental Health,2016,"Mental illness is one of the most pressing public health issues our time. While counseling and psychotherapy can be effective treatments, knowledge about how to conduct successful conversations has been limited due lack large-scale data with labeled outcomes conversations. In this paper, we present a large-scale, quantitative study on discourse text-message-based We develop set novel computational analysis methods measure various linguistic aspects are correlated conversation outcomes. Applying techniques such as sequence-based models, language model comparisons, message clustering, psycholinguistics-inspired word frequency analyses, discover actionable strategies that associated better"
https://openalex.org/W3118062200,https://doi.org/10.48550/arxiv.2005.10242,"Understanding Contrastive Representation Learning through Alignment and
  Uniformity on the Hypersphere",2020,"Contrastive representation learning has been outstandingly successful in practice. In this work, we identify two key properties related to the contrastive loss: (1) alignment (closeness) of features from positive pairs, and (2) uniformity induced distribution (normalized) on hypersphere. We prove that, asymptotically, loss optimizes these properties, analyze their effects downstream tasks. Empirically, introduce an optimizable metric quantify each property. Extensive experiments standard vision language datasets confirm strong agreement between both metrics task performance. Remarkably, directly optimizing for leads representations with comparable or better performance at tasks than learning. Project Page: https://tongzhouwang.info/hypersphere Code: https://github.com/SsnL/align_uniform , https://github.com/SsnL/moco_align_uniform"
https://openalex.org/W2788474500,https://doi.org/10.1609/aaai.v32i1.12039,Graph Convolutional Networks With Argument-Aware Pooling for Event Detection,2018,"The current neural network models for event detection have only considered the sequential representation of sentences. Syntactic representations not been explored in this area although they provide an effective mechanism to directly link words their informative context In work, we investigate a convolutional based on dependency trees perform detection. We propose novel pooling method that relies entity mentions aggregate convolution vectors. extensive experiments demonstrate benefits dependency-based networks and mention-based achieve state-of-the-art performance widely used datasets with both perfect predicted mentions."
https://openalex.org/W2798542795,https://doi.org/10.18653/v1/p18-1015,"Retrieve, Rerank and Rewrite: Soft Template Based Neural Summarization",2018,"Most previous seq2seq summarization systems purely depend on the source text to generate summaries, which tends work unstably. Inspired by traditional template-based approaches, this paper proposes use existing summaries as soft templates guide model. To end, we a popular IR platform Retrieve proper candidate templates. Then, extend framework jointly conduct template Reranking and template-aware summary generation (Rewriting). Experiments show that, in terms of informativeness, our model significantly outperforms state-of-the-art methods, even themselves demonstrate high competitiveness. In addition, import high-quality external improves stability readability generated summaries."
https://openalex.org/W2915971115,https://doi.org/10.1016/j.isprsjprs.2019.02.006,Semantic segmentation of slums in satellite images using transfer learning on fully convolutional neural networks,2019,"Abstract Unprecedented urbanization in particular countries of the global south result informal urban development processes, especially mega cities. With an estimated 1 billion slum dwellers globally, United Nations have made fight against poverty number one sustainable goal. To provide better infrastructure and thus a life to dwellers, detailed information on spatial location size slums is crucial importance. In past, remote sensing has proven be extremely valuable effective tool for mapping slums. The nature used approaches by machine learning, however, it necessary invest lot effort training models. Recent advances deep learning allow transferring trained fully convolutional networks (FCN) from data set another. Thus, our study we aim at analyzing transfer capabilities FCNs various satellite images. A model very high resolution optical imagery QuickBird transferred Sentinel-2 TerraSAR-X data. While free-of-charge widely available, its comparably lower makes challenging task. other hand, higher considered powerful source intra-urban structure analysis. Due different image characteristics SAR compared data, could not improve performance semantic segmentation but observe accuracies mapped data: obtains 86–88% (positive prediction value sensitivity) significant increase applying can observed (from 38 55% 79 85% PPV sensitivity, respectively). Using proofs retrieving small-scaled structures such as patches even images decametric resolution."
https://openalex.org/W2964093087,https://doi.org/10.18653/v1/d17-1301,Exploiting Cross-Sentence Context for Neural Machine Translation,2017,"In translation, considering the document
as a whole can help to resolve ambiguities
and inconsistencies. this paper, we propose cross-sentence context-aware approach and investigate influence of historical contextual information on performance neural machine translation
(NMT). First, history is summarized
in hierarchical way. We then integrate
the representation into NMT in
two strategies: 1) warm-start encoder decoder states, 2) an auxiliary context source for updating decoder
states. Experimental results large
Chinese-English translation task show that
our significantly improves upon
a strong attention-based system by
up +2.1 BLEU points."
https://openalex.org/W2964164798,https://doi.org/10.18653/v1/e17-1051,An Incremental Parser for Abstract Meaning Representation,2017,"Abstract Meaning Representation (AMR) is a semantic representation for natural language that embeds annotations related to traditional tasks such as named entity recognition, role labeling, word sense disambiguation and co-reference resolution. We describe transition-based parser AMR parses sentences left-to-right, in linear time. further propose test-suite assesses specific subtasks are helpful comparing parsers, show our competitive with the state of art on LDC2015E86 dataset it outperforms state-of-the-art parsers recovering entities handling polarity."
https://openalex.org/W2366532918,https://doi.org/10.1145/2884781.2884800,Augmenting API documentation with insights from stack overflow,2016,"Software developers need access to different kinds of information which is often dispersed among documentation sources, such as API or Stack Overflow. We present an approach automatically augment with insight sentences from Overflow -- that are related a particular type and provide not contained in the type. Based on development set 1,574 sentences, we compare performance two state-of-the-art summarization techniques well pattern-based for sentence extraction. then SISE, novel machine learning based uses features themselves, their formatting, question, answer, authors part-of-speech tags similarity corresponding documentation. With were able achieve precision 0.64 coverage 0.7 set. In comparative study eight software developers, found SISE resulted highest number considered add useful These results indicate taking into account meta data available can significantly improve unsupervised extraction approaches when applied data."
https://openalex.org/W2468710617,https://doi.org/10.5087/dad.2016.301,The Dialog State Tracking Challenge Series: A Review,2016,"In a spoken dialog system, state tracking refers to the task of correctly inferring conversation -- such as user's goal given all history up that turn. Dialog is crucial success yet until recently there were no common resources, hampering progress. The State Tracking Challenge series 3 tasks introduced first shared testbed and evaluation metrics for tracking, has underpinned three key advances in tracking: move from generative discriminative models; adoption sequential techniques; incorporation speech recognition results directly into tracker. This paper reviews this research area, covering both challenge themselves summarizing work they have enabled."
https://openalex.org/W2538374209,https://doi.org/10.1145/2983323.2983818,aNMM,2016,"As an alternative to question answering methods based on feature engineering, deep learning approaches such as convolutional neural networks (CNNs) and Long Short-Term Memory Models (LSTMs) have recently been proposed for semantic matching of questions answers. To achieve good results, however, these models combined with additional features word overlap or BM25 scores. Without this combination, perform significantly worse than linguistic engineering. In paper, we propose attention model ranking short answer text. We adopt value-shared weighting scheme instead position-shared combining different signals incorporate term importance using network. Using the popular benchmark TREC QA data, show that relatively simple aNMM can outperform other network used task, is competitive are features. When features, it outperforms all baselines."
https://openalex.org/W2582614075,https://doi.org/10.1111/lang.12225,"Collocations in Corpus-Based Language Learning Research: Identifying, Comparing, and Interpreting the Evidence",2017,"This article focuses on the use of collocations in language learning research (LLR). Collocations, as units formulaic language, are becoming prominent our understanding and use; however, while number corpus-based LLR studies is growing, there still a need for deeper factors that play role establishing two words corpus can be considered to collocates. In this we critically review both application measures used identify collocability between nature relationship Particular attention paid comparison across different corpora representing genres, registers, or modalities. Several issues involved interpretation collocational patterns production first second users also considered. Reflecting current practices field, further directions collocation proposed."
https://openalex.org/W2609482285,https://doi.org/10.18653/v1/p17-1101,Selective Encoding for Abstractive Sentence Summarization,2017,"We propose a selective encoding model to extend the sequence-to-sequence framework for abstractive sentence summarization. It consists of encoder, gate network, and an attention equipped decoder. The encoder decoder are built with recurrent neural networks. network constructs second level representation by controlling information flow from is tailored summarization task, which leads better performance. evaluate our on English Gigaword, DUC 2004 MSR datasets. experimental results show that proposed outperforms state-of-the-art baseline models."
https://openalex.org/W2963240734,https://doi.org/10.1109/iccv.2017.391,Joint Detection and Recounting of Abnormal Events by Learning Deep Generic Knowledge,2017,"This paper addresses the problem of joint detection and recounting abnormal events in videos. Recounting events, i.e., explaining why they are judged to be abnormal, is an unexplored but critical task video surveillance, because it helps human observers quickly judge if false alarms or not. To describe human-understandable form for event recounting, learning generic knowledge about visual concepts (e.g., object action) crucial. Although convolutional neural networks (CNNs) have achieved promising results such concepts, remains open question as how effectively use CNNs detection, mainly due environment-dependent nature anomaly detection. In this paper, we tackle by integrating a CNN model detectors. Our approach first learns with multiple tasks exploit semantic information that useful detecting events. By appropriately plugging into detectors, can detect recount while taking advantage discriminative power CNNs. outperforms state-of-the-art on Avenue UCSD Ped2 benchmarks also produces recounting."
https://openalex.org/W2963672008,https://doi.org/10.18653/v1/w17-2619,Learning Joint Multilingual Sentence Representations with Neural Machine Translation,2017,"In this paper, we use the framework of neural machine translation to learn joint sentence representations across six very different languages. Our aim is that a representation which independent language, likely capture underlying semantics. We define new cross-lingual similarity measure, compare up 1.4M and study characteristics close sentences. provide experimental evidence sentences are in embedding space indeed semantically highly related, but often have quite structure syntax. These relations also hold when comparing"
https://openalex.org/W3035140194,https://doi.org/10.18653/v1/2020.acl-main.398,TaPas: Weakly Supervised Table Parsing via Pre-training,2020,"Answering natural language questions over tables is usually seen as a semantic parsing task. To alleviate the collection cost of full logical forms, one popular approach focuses on weak supervision consisting denotations instead forms. However, training parsers from poses difficulties, and in addition, generated forms are only used an intermediate step prior to retrieving denotation. In this paper, we present TAPAS, question answering without generating TAPAS trains supervision, predicts denotation by selecting table cells optionally applying corresponding aggregation operator such selection. extends BERT's architecture encode input, initializes effective joint pre-training text segments crawled Wikipedia, trained end-to-end. We experiment with three different datasets, find that outperforms or rivals models improving state-of-the-art accuracy SQA 55.1 67.2 performing par WIKISQL WIKITQ, but simpler model architecture. additionally transfer learning, which trivial our setting, yields 48.7 accuracy, 4.2 points above state-of-the-art."
https://openalex.org/W3106083666,https://doi.org/10.1007/s00429-015-1179-4,The white matter query language: a novel approach for describing human white matter anatomy,2016,"We have developed a novel method to describe human white matter anatomy using an approach that is both intuitive and simple use, which automatically extracts tracts from diffusion MRI volumes. Further, our simplifies the quantification statistical analysis of on large databases. This work reflects careful syntactical definition major fiber in brain based neuroanatomist's expert knowledge. The framework query language with near-to-English textual syntax. makes it possible construct dictionary anatomical definitions tracts. include adjacent gray regions, rules for spatial relations. label across subjects. After describing this method, we provide example its implementation where encode knowledge 10 association 15 projection per hemisphere, along 7 commissural Importantly, comparable accuracy manual labeling. Finally, present results applying create atlas 77 healthy subjects, use small proof-of-concept study detect changes characterize schizophrenia."
https://openalex.org/W1771830246,https://doi.org/10.1145/2884781.2884848,"On the ""naturalness"" of buggy code",2016,"Real software, the kind working programmers produce by kLOC to solve real-world problems, tends be “natural”, like speech or natural language; it highly repetitive and predictable. Researchers have captured this naturalness of software through statistical models used them good effect in suggestion engines, porting tools, coding standards checkers, idiom miners. This suggests that code appears improbable, surprising, a language model is “unnatural” some sense, thus possibly suspicious. In paper, we investigate hypothesis. We consider large corpus bug fix commits (ca. 7,139), from 10 different Java projects, focus on its statistics, evaluating buggy corresponding fixes. find with bugs more entropic (i.e. unnatural), becoming less so as are fixed. Ordering files for inspection their average entropy yields cost-effectiveness scores comparable popular defect prediction methods. At finer granularity, focusing lines similar well-known static finders (PMD, FindBugs) or- dering warnings these using an measure improves inspecting implicated warnings. may valid, simple way complement effectiveness PMD FindBugs, search-based bug-fixing methods benefit both fault-localization searching"
https://openalex.org/W2593481629,https://doi.org/10.1075/ijcl.22.3.02lov,The Spoken BNC2014,2017,"Abstract This paper introduces the Spoken British National Corpus 2014, an 11.5-million-word corpus of orthographically transcribed conversations among L1 speakers English from across UK, recorded in years 2012–2016. After showing that a survey recent history corpora spoken justifies compilation this new corpus, we describe main stages BNC2014’s creation: design, data and metadata collection, transcription, XML encoding, annotation. In doing so aim to (i) encourage users approach with sensitivity many methodological issues identified attempted overcome while compiling BNC2014, (ii) inform (future) compilers innovations implemented attempt make construction representing spontaneous speech informal contexts more tractable, both logistically practically, than past."
https://openalex.org/W2622627557,https://doi.org/10.1523/jneurosci.3267-16.2017,The Hierarchical Cortical Organization of Human Speech Processing,2017,"Speech comprehension requires that the brain extract semantic meaning from spectral features represented at cochlea. To investigate this process, we performed an fMRI experiment in which five men and two women passively listened to several hours of natural narrative speech. We then used voxelwise modeling predict BOLD responses based on three different feature spaces represent spectral, articulatory, properties The amount variance explained by each space was assessed using a separate validation dataset. Because some might be equally well more than one space, partitioning analysis determine fraction uniquely space. Consistent with previous studies, found speech involves hierarchical representations starting primary auditory areas moving laterally temporal lobe: are core A1, mixtures articulatory STG, STS, STS beyond. Our data also show both hemispheres actively involved perception interpretation. Further, as early hierarchy correlated representations. These results illustrate importance neurolinguistic research. methodology provides efficient way simultaneously test multiple specific hypotheses about without block designs segmented or synthetic speech.SIGNIFICANCE STATEMENT processing steps human transform sound into meaningful language, models set individual voxels recorded while subjects Both cerebral were large equal amounts. Also, transformation elements occurs cortical speech-processing stream. experimental analytical approaches important alternatives complements standard use designs, report laterality associated higher levels cortex reported here."
https://openalex.org/W2888161220,https://doi.org/10.18653/v1/d18-1002,Adversarial Removal of Demographic Attributes from Text Data,2018,"Recent advances in Representation Learning and Adversarial Training seem to succeed removing unwanted features from the learned representation. We show that demographic information of authors is encoded in—and can be recovered from—the intermediate representations by text-based neural classifiers. The implication decisions classifiers trained on textual data are not agnostic to—and likely condition on—demographic attributes. When attempting remove such using adversarial training, we find while component achieves chance-level development-set accuracy during a post-hoc classifier, sentences first part, still manages reach substantially higher classification accuracies same data. This behavior consistent across several tasks, properties datasets. explore techniques improve effectiveness component. Our main conclusion cautionary one: do rely training achieve invariant representation sensitive features."
https://openalex.org/W2912904516,https://doi.org/10.1162/tacl_a_00264,DREAM: A Challenge Data Set and Models for Dialogue-Based Reading Comprehension,2019,"We present DREAM, the first dialogue-based multiple-choice reading comprehension data set. Collected from English as a Foreign Language examinations designed by human experts to evaluate level of Chinese learners English, our set contains 10,197 questions for 6,444 dialogues. In contrast existing sets, DREAM is focus on in-depth multi-turn multi-party dialogue understanding. likely significant challenges systems: 84% answers are non-extractive, 85% require reasoning beyond single sentence, and 34% also involve commonsense knowledge. apply several popular neural models that primarily exploit surface information within text find them to, at best, just barely outperform rule-based approach. next investigate effects incorporating structure different kinds general world knowledge into both (neural non-neural) machine learning-based models. Experimental results show effectiveness available https://dataset.org/dream/ ."
https://openalex.org/W2956090150,https://doi.org/10.18653/v1/w18-5102,Hate Speech Dataset from a White Supremacy Forum,2018,"Hate speech is commonly defined as any communication that disparages a target group of people based on some characteristic such race, colour, ethnicity, gender, sexual orientation, nationality, religion, or other characteristic. Due to the massive rise user-generated web content social media, amount hate also steadily increasing. Over past years, interest in online detection and, particularly, automation this task has continuously grown, along with societal impact phenomenon. This paper describes dataset composed thousands sentences manually labelled containing not. The have been extracted from Stormfront, white supremacist forum. A custom annotation tool developed carry out manual labelling which, among things, allows annotators choose whether read context sentence before it. provides thoughtful qualitative and quantitative study resulting several baseline experiments different classification models. publicly available."
https://openalex.org/W2962713807,https://doi.org/10.18653/v1/n18-2093,TypeSQL: Knowledge-Based Type-Aware Neural Text-to-SQL Generation,2018,"Interacting with relational databases through natural language helps users any background easily query and analyze a vast amount of data. This requires system that understands users’ questions converts them to SQL queries automatically. In this paper, we present novel approach TypeSQL which formats the problem as slot filling task in more reasonable way. addition, utilizes type information better understand rare entities numbers questions. We experiment idea on WikiSQL dataset outperform prior art by 6% much shorter time. also show accessing content can significantly improve performance when are not well-formed. reach 82.6% accuracy, 17.5% absolute improvement compared previous content-sensitive model."
https://openalex.org/W2962916648,https://doi.org/10.1007/978-3-319-68288-4_37,Cross-Lingual Entity Alignment via Joint Attribute-Preserving Embedding,2017,"Entity alignment is the task of finding entities in two knowledge bases (KBs) that represent same real-world object. When facing KBs different natural languages, conventional cross-lingual entity methods rely on machine translation to eliminate language barriers. These approaches often suffer from uneven quality translations between languages. While recent embedding-based techniques encode and relationships do not need for alignment, a significant number attributes remain largely unexplored. In this paper, we propose joint attribute-preserving embedding model alignment. It jointly embeds structures into unified vector space further refines it by leveraging attribute correlations KBs. Our experimental results datasets show approach significantly outperforms state-of-the-art could be complemented with based translation."
https://openalex.org/W2963520511,https://doi.org/10.1609/aaai.v32i1.11923,Augmenting End-to-End Dialogue Systems With Commonsense Knowledge,2018,"Building dialogue systems that can converse naturally with humans is a challenging yet intriguing problem of artificial intelligence. In open-domain human-computer conversation, where the conversational agent expected to respond human utterances in an interesting and engaging way, commonsense knowledge has be integrated into model effectively. this paper, we investigate impact providing about concepts covered dialogue. Our represents first attempt integrating large base end-to-end models. retrieval-based scenario, propose jointly take account message content related for selecting appropriate response. experiments suggest knowledge-augmented models are superior their knowledge-free counterparts."
https://openalex.org/W2953022248,https://doi.org/10.48550/arxiv.1603.03925,Image Captioning with Semantic Attention,2016,"Automatically generating a natural language description of an image has attracted interests recently both because its importance in practical applications and it connects two major artificial intelligence fields: computer vision processing. Existing approaches are either top-down, which start from gist convert into words, or bottom-up, come up with words describing various aspects then combine them. In this paper, we propose new algorithm that combines through model semantic attention. Our learns to selectively attend concept proposals fuse them hidden states outputs recurrent neural networks. The selection fusion form feedback connecting the top-down bottom-up computation. We evaluate our on public benchmarks: Microsoft COCO Flickr30K. Experimental results show significantly outperforms state-of-the-art consistently across different evaluation metrics."
https://openalex.org/W2962804639,https://doi.org/10.1109/iccv.2017.529,WordSup: Exploiting Word Annotations for Character Based Text Detection,2017,"Imagery texts are usually organized as a hierarchy of several visual elements, i.e. characters, words, text lines and blocks. Among these character is the most basic one for various languages such Western, Chinese, Japanese, mathematical expression etc. It natural convenient to construct common detection engine based on detectors. However, training detectors requires vast location annotated which expensive obtain. Actually, existing real datasets mostly in word or line level. To remedy this dilemma, we propose weakly supervised framework that can utilize annotations, either tight quadrangles more loose bounding boxes, detector training. When applied scene detection, thus able train robust by exploiting annotations rich large-scale datasets, e.g. ICDAR15 COCO-text. The acts key role pipeline our engine. achieves state-of-the-art performance challenging benchmarks. We also demonstrate flexibility scenarios, including deformed math recognition."
https://openalex.org/W2962974924,https://doi.org/10.18653/v1/d17-1222,Deep Recurrent Generative Decoder for Abstractive Text Summarization,2017,We propose a new framework for abstractive text summarization based on sequence-to-sequence oriented encoder-decoder model equipped with deep recurrent generative decoder (DRGN). Latent structure information implied in the target summaries is learned latent random improving quality. Neural variational inference employed to address intractable posterior variables. Abstractive are generated both variables and discriminative deterministic states. Extensive experiments some benchmark datasets different languages show that DRGN achieves improvements over state-of-the-art methods.
https://openalex.org/W2963788376,https://doi.org/10.3115/v1/p15-2130,Multi-domain Dialog State Tracking using Recurrent Neural Networks,2015,"Dialog state tracking is a key component of many modern dialog systems, most which are designed with single, welldefined domain in mind. This paper shows that data drawn from different domains can be used to train general belief model operate across all these domains, exhibiting superior performance each the domainspecific models. We propose training procedure uses out-of-domain initialise models for entirely new domains. leads improvements regardless amount in-domain available model."
https://openalex.org/W2963971656,https://doi.org/10.1109/taslp.2017.2761547,Statistical Parametric Speech Synthesis Incorporating Generative Adversarial Networks,2018,"A method for statistical parametric speech synthesis incorporating generative adversarial networks (GANs) is proposed. Although powerful deep neural techniques can be applied to artificially synthesize waveform, the synthetic quality low compared with that of natural speech. One issues causing degradation an oversmoothing effect often observed in generated parameters. GAN introduced this paper consists two networks: a discriminator distinguish and samples, generator deceive discriminator. In proposed framework GANs, trained parameters, while acoustic models are minimize weighted sum conventional minimum generation loss deceiving Since objective GANs divergence (i.e., distribution difference) between effectively alleviates on We evaluated effectiveness text-to-speech voice conversion, found generate more spectral parameters $F_0$ than error training algorithm regardless its hyperparameter settings. Furthermore, we investigated various Wasserstein minimizing Earth-Mover's distance works best terms improving quality."
https://openalex.org/W2964313012,https://doi.org/10.1109/iccv.2017.608,Semantic Image Synthesis via Adversarial Learning,2017,"In this paper, we propose a way of synthesizing realistic images directly with natural language description, which has many useful applications, e.g. intelligent image manipulation. We attempt to accomplish such synthesis: given source and target text our model synthesizes meet two requirements: 1) being while matching the description; 2) maintaining other features that are irrelevant description. The should be able disentangle semantic information from modalities (image text), generate new combined semantics. To achieve this, proposed an end-to-end neural architecture leverages adversarial learning automatically learn implicit loss functions, optimized fulfill aforementioned requirements. have evaluated by conducting experiments on Caltech-200 bird dataset Oxford-102 flower dataset, demonstrated is capable match descriptions, still maintain original images."
https://openalex.org/W3045464143,https://doi.org/10.1162/tacl_a_00325,Topic Modeling in Embedding Spaces,2020,"Topic modeling analyzes documents to learn meaningful patterns of words. However, existing topic models fail interpretable topics when working with large and heavy-tailed vocabularies. To this end, we develop the embedded model (etm), a generative that marries traditional word embeddings. More specifically, etm each categorical distribution whose natural parameter is inner product between word’s embedding an its assigned topic. fit etm, efficient amortized variational inference algorithm. The discovers even vocabularies include rare words stop It outperforms document models, such as latent Dirichlet allocation, in terms both quality predictive performance."
https://openalex.org/W3098598077,https://doi.org/10.1109/icse.2017.9,Semantically Enhanced Software Traceability Using Deep Learning Techniques,2017,"In most safety-critical domains the need for traceability is prescribed by certifying bodies. Trace links are generally created among requirements, design, source code, test cases and other artifacts, however, creating such manually time consuming error prone. Automated solutions use information retrieval machine learning techniques to generate trace links, current fail understand semantics of software artifacts or integrate domain knowledge into tracing process therefore tend deliver imprecise inaccurate results. this paper, we present a solution that uses deep incorporate requirements artifact solution. We propose network architecture utilizes Word Embedding Recurrent Neural Network (RNN) models links. embedding learns word vectors represent corpus RNN these learn sentence artifacts. trained 360 different configurations using existing in Positive Train Control identified Bidirectional Gated Unit (BI-GRU) as best model task. BI-GRU significantly out-performed state-of-the-art methods including Vector Space Model Latent Semantic Indexing."
https://openalex.org/W2027176887,https://doi.org/10.1016/j.cortex.2015.02.014,A predictive coding framework for rapid neural dynamics during sentence-level language comprehension,2015,"There is a growing literature investigating the relationship between oscillatory neural dynamics measured using electroencephalography (EEG) and/or magnetoencephalography (MEG), and sentence-level language comprehension. Recent proposals have suggested strong link predictive coding accounts of hierarchical flow information in brain, beta gamma frequency ranges. We propose that findings relating oscillations to comprehension might be unified under such account. Our suggestion activity range may reflect both active maintenance current network configuration responsible for representing meaning construction, top-down propagation predictions hierarchically lower processing levels based on representation. In addition, we suggest low middle matching with bottom-up linguistic input, while evoked high prediction errors higher hierarchy. also discuss some implications this framework, outline ideas how these tested experimentally."
https://openalex.org/W2512532697,https://doi.org/10.18653/v1/w16-0425,Fracking Sarcasm using Neural Network,2016,"Precise semantic representation of a sentence and definitive information extraction are key steps in the accurate processing meaning, especially for figurative phenomena such as sarcasm, Irony, metaphor cause literal meanings to be discounted secondary or extended intentionally profiled. Semantic modelling faces new challenge social media, because grammatical inaccuracy is commonplace yet many previous state-of-the-art methods exploit structure. For sarcasm detection over media content, researchers so far have counted on Bag-of-Words(BOW), N-grams etc. In this paper, we propose neural network model task detection. We also review using Support Vector Machine (SVM) that employs constituency parsetrees fed labeled with syntactic information. The proposed composed Convolution Neural Network(CNN) followed by Long short term memory (LSTM) finally Deep network(DNN). outperforms textbased detection, yielding an F-score .92."
https://openalex.org/W2514518356,https://doi.org/10.1016/j.ultramic.2016.08.007,Current status and future directions for in situ transmission electron microscopy,2016,"This review article discusses the current and future possibilities for application of in situ transmission electron microscopy to reveal synthesis pathways functional mechanisms complex nanoscale materials. The findings a group scientists, representing academia, government labs private sector entities (predominantly commercial vendors) during workshop, held at Center Nanoscale Science Technology- National Institute Technology (CNST-NIST), are discussed. We provide comprehensive scientific needs instrument technique developments required meet them."
https://openalex.org/W2604205681,https://doi.org/10.1609/aaai.v31i1.10974,Coupled Multi-Layer Attentions for Co-Extraction of Aspect and Opinion Terms,2017,"The task of aspect and opinion terms co-extraction aims to explicitly extract describing features an entity expressing emotions from user-generated texts. To achieve this task, one effective approach is exploit relations between by parsing syntactic structure for each sentence. However, requires expensive effort highly depends on the quality results. In paper, we offer a novel deep learning model, named coupled multi-layer attentions. proposed model provides end-to-end solution does not require any parsers or other linguistic resources preprocessing. Specifically, attention network, where layer consists couple attentions with tensor operators. One extracting terms, while terms. They are learned interactively dually propagate information Through multiple layers, can further indirect more precise extraction. Experimental results three benchmark datasets in SemEval Challenge 2014 2015 show that our achieves state-of-the-art performances compared several baselines."
https://openalex.org/W2612228435,https://doi.org/10.18653/v1/p17-1167,Search-based Neural Structured Learning for Sequential Question Answering,2017,"Recent work in semantic parsing for question answering has focused on long and complicated questions, many of which would seem unnatural if asked a normal conversation between two humans. In an effort to explore conversational QA setting, we present more realistic task: sequences simple but inter-related questions. We collect dataset 6,066 that inquire about semi-structured tables from Wikipedia, with 17,553 question-answer pairs total. To solve this sequential task, propose novel dynamic neural framework trained using weakly supervised reward-guided search. Our model effectively leverages the context outperform state-of-the-art systems are designed answer highly complex"
https://openalex.org/W2963291843,https://doi.org/10.18653/v1/w16-6208,emoji2vec: Learning Emoji Representations from their Description,2016,"Many current natural language processing applications for social media rely on representation learning and utilize pre-trained word embeddings. There currently exist several publicly-available, sets of embeddings, but they contain few or no emoji representations even as usage in has increased. In this paper we release emoji2vec, embeddings all Unicode which are learned from their description the standard. The resulting can be readily used downstream alongside word2vec. We demonstrate, task sentiment analysis, that short descriptions outperforms a skip-gram model trained large collection tweets, while avoiding need contexts to appear frequently order estimate representation."
https://openalex.org/W2964144561,https://doi.org/10.18653/v1/k17-1045,Graph-based Neural Multi-Document Summarization,2017,"We propose a neural multi-document summarization (MDS) system that incorporates sentence relation graphs. employ Graph Convolutional Network (GCN) on the graphs, with embeddings obtained from Recurrent Neural Networks as input node features. Through multiple layer-wise propagation, GCN generates high-level hidden features for salience estimation. then use greedy heuristic to extract salient sentences while avoiding redundancy. In our experiments DUC 2004, we consider three types of graphs and demonstrate advantage combining relations in representation power deep networks. Our model improves upon traditional graph-based extractive approaches vanilla GRU sequence no graph, it achieves competitive results against other state-of-the-art systems."
https://openalex.org/W877909479,https://doi.org/10.1609/aaai.v29i1.9512,Jointly Modeling Deep Video and Compositional Text to Bridge Vision and Language in a Unified Framework,2015,"Recently, joint video-language modeling has been attracting more and attention. However, most existing approaches focus on exploring the language model upon a fixed visual model. In this paper, we propose unified framework that jointly models video corresponding text sentences. The consists of three parts: compositional semantics model, deep embedding our dependency-tree structure embeds sentence into continuous vector space, which preserves visually grounded meanings word order. leverage neural networks to capture essential semantic information from videos. minimize distance outputs in update these two jointly. Based parts, system is able accomplish tasks: 1) natural generation, 2) retrieval 3) retrieval. experiments, results show approach outperforms SVM, CRF CCA baselines predicting Subject-Verb-Object triplet better than tasks."
https://openalex.org/W2251849926,https://doi.org/10.18653/v1/d15-1106,Hierarchical Recurrent Neural Network for Document Modeling,2015,"This paper proposes a novel hierarchical recurrent neural network language model (HRNNLM) for document modeling. After establishing RNN to capture the coherence between sentences in document, HRNNLM integrates it as sentence history information into word level predict sequence with cross-sentence contextual information. A two-step training approach is designed, which sentence-level and word-level models are approximated convergence pipeline style. Examined by standard reordering scenario, proved its better accuracy modeling coherence. And at level, experimental results also indicate significant lower perplexity, followed practical translation result when applied Chinese-English reranking task."
https://openalex.org/W2955429306,https://doi.org/10.18653/v1/s19-2005,SemEval-2019 Task 3: EmoContext Contextual Emotion Detection in Text,2019,"In this paper, we present the SemEval-2019 Task 3 - EmoContext: Contextual Emotion Detection in Text. Lack of facial expressions and voice modulations make detecting emotions text a challenging problem. For instance, as humans, on reading “Why don’t you ever me!” can either interpret it sad or angry emotion same ambiguity exists for machines. However, context dialogue prove helpful detection emotion. task, given textual i.e. an utterance along with two previous turns context, goal was to infer underlying by choosing from four classes Happy, Sad, Angry Others. To facilitate participation dialogues user interaction conversational agent were taken annotated after several data processing steps. A training set 30160 dialogues, evaluation sets, Test1 Test2, containing 2755 5509 respectively released participants. total 311 teams made submissions task. The final leader-board evaluated Test2 set, highest ranked submission achieved 79.59 micro-averaged F1 score. Our analysis systems submitted task indicate that Bi-directional LSTM most common choice neural architecture used, had best performance Sad class, worst Happy class."
https://openalex.org/W2962986948,https://doi.org/10.1109/cvpr.2018.00527,An End-to-End TextSpotter with Explicit Alignment and Attention,2018,"Text detection and recognition in natural images have long been considered as two separate tasks that are processed sequentially. Training of a unified framework is non-trivial due to significant dif- ferences optimisation difficulties. In this work, we present conceptually simple yet efficient simultaneously processes the one shot. Our main contributions three-fold: 1) propose novel text-alignment layer allows it precisely compute convolutional features text instance ar- bitrary orientation, which key boost per- formance; 2) character attention mechanism introduced by using spatial information explicit supervision, leading large improvements recognition; 3) technologies, together with new RNN branch for word recognition, integrated seamlessly into single model end-to-end trainable. This work collaboratively shar- ing features, critical identify challenging instances. achieves impressive results on ICDAR2015 dataset, significantly advancing most recent results, F-measure from (0.54, 0.51, 0.47) (0.82, 0.77, 0.63), strong, weak generic lexicon respectively. Thanks joint training, our method can also serve good detec- tor achieving state-of-the-art performance datasets."
https://openalex.org/W2963099212,https://doi.org/10.18653/v1/d16-1229,Cultural Shift or Linguistic Drift? Comparing Two Computational Measures of Semantic Change,2016,"Words shift in meaning for many reasons, including cultural factors like new technologies and regular linguistic processes subjectification. Understanding the evolution of language culture requires disentangling these underlying causes. Here we show how two different distributional measures can be used to detect types semantic change. The first measure, which has been previous works, analyzes global shifts a word's semantics, it is sensitive changes due drift, such as generalization promise (""I promise."" -> ""It promised exciting.""). second develop here, focuses on local nearest neighbors; more shifts, change cell (""prison cell"" ""cell phone""). Comparing measurements made by methods allows researchers determine whether are or nature, distinction that essential work digital humanities historical linguistics."
https://openalex.org/W2963449390,https://doi.org/10.1109/cvpr.2017.551,Person Search with Natural Language Description,2017,"Searching persons in large-scale image databases with the query of natural language description has important applications video surveillance. Existing methods mainly focused on searching image-based or attribute-based queries, which have major limitations for a practical usage. In this paper, we study problem person search description. Given textual person, algorithm is required to rank all samples database then retrieve most relevant sample corresponding queried Since there no dataset benchmark available, collect detailed annotations and from various sources, termed as CUHK Person Description Dataset (CUHK-PEDES). A wide range possible models baselines been evaluated compared benchmark. An Recurrent Neural Network Gated Attention mechanism (GNA-RNN) proposed establish state-of-the art performance search."
https://openalex.org/W2963641561,https://doi.org/10.18653/v1/d16-1025,Neural versus Phrase-Based Machine Translation Quality: a Case Study,2016,"Within the field of Statistical Machine Translation (SMT), neural approach (NMT) has recently emerged as first technology able to challenge long-standing dominance phrase-based approaches (PBMT). In particular, at IWSLT 2015 evaluation campaign, NMT outperformed well established state-of-the-art PBMT systems on English-German, a language pair known be particularly hard because morphology and syntactic differences. To understand in what respects provides better translation quality than PBMT, we perform detailed analysis vs. SMT outputs, leveraging high post-edits performed by professional translators data. For time, our useful insights linguistic phenomena are best modeled models - such reordering verbs while pointing out other aspects that remain improved."
https://openalex.org/W2964316912,https://doi.org/10.18653/v1/n18-1109,Diverse Few-Shot Text Classification with Multiple Metrics,2018,"We study few-shot learning in natural language domains. Compared to many existing works that apply either metric-based or optimization-based meta-learning image domain with low inter-task variance, we consider a more realistic setting, where tasks are diverse. However, it imposes tremendous difficulties state-of-the-art algorithms since single metric is insufficient capture complex task variations domain. To alleviate the problem, propose an adaptive approach automatically determines best weighted combination from set of metrics obtained meta-training for newly seen task. Extensive quantitative evaluations on real-world sentiment analysis and dialog intent classification datasets demonstrate proposed method performs favorably against few shot terms predictive accuracy. make our code data available further study."
https://openalex.org/W3114632476,https://doi.org/10.3390/technologies9010002,A Survey on Contrastive Self-Supervised Learning,2020,"Self-supervised learning has gained popularity because of its ability to avoid the cost annotating large-scale datasets. It is capable adopting self-defined pseudolabels as supervision and use learned representations for several downstream tasks. Specifically, contrastive recently become a dominant component in self-supervised computer vision, natural language processing (NLP), other domains. aims at embedding augmented versions same sample close each while trying push away embeddings from different samples. This paper provides an extensive review methods that follow approach. The work explains commonly used pretext tasks setup, followed by architectures have been proposed so far. Next, we present performance comparison multiple such image classification, object detection, action recognition. Finally, conclude with limitations current need further techniques future directions make meaningful progress."
https://openalex.org/W2176118107,https://doi.org/10.1017/s0140525x15001247,"Gesture, sign, and language: The coming of age of sign language and gesture studies",2017,"Abstract How does sign language compare with gesture, on the one hand, and spoken other? Sign was once viewed as nothing more than a system of pictorial gestures without linguistic structure. More recently, researchers have argued that is no different from language, all same structures. The pendulum currently swinging back toward view gestural, or at least has gestural components. goal this review to elucidate relationships among language. We do so by taking close look not only how been studied over past 50 years, but also spontaneous accompany speech studied. conclude signers gesture just speakers do. Both produce imagistic along categorical signs words. Because present it difficult tell where stops begins, we suggest should be compared alone speech-plus-gesture. Although might easier (and, in some cases, preferable) blur distinction between argue distinguishing (or speech) essential predict certain types learning allows us understand conditions under which takes properties sign, gesture. end calling for new technology may help better calibrate borders"
https://openalex.org/W2492922441,https://doi.org/10.1007/s12559-016-9415-7,Multilingual Sentiment Analysis: State of the Art and Independent Comparison of Techniques,2016,"With the advent of Internet, people actively express their opinions about products, services, events, political parties, etc., in social media, blogs, and website comments. The amount research work on sentiment analysis is growing explosively. However, majority efforts are devoted to English-language data, while a great share information available other languages. We present state-of-the-art review multilingual analysis. More importantly, we compare our own implementation existing approaches common data. Precision observed experiments typically lower than one reported by original authors, which attribute lack detail presentation those approaches. Thus, works what they really offer reader, including whether allow for accurate reliable reproduction results."
https://openalex.org/W2606488938,https://doi.org/10.1016/j.neubiorev.2017.04.009,Deep temporal models and active inference,2017,"How do we navigate a deeply structured world? Why are you reading this sentence first - and did actually look at the fifth word? This review offers some answers by appealing to active inference based on deep temporal models. It builds previous formulations of simulate behavioural electrophysiological responses under hierarchical generative models state transitions. Inverting these corresponds sequential inference, such that any level entails sequence transitions in below. The aspect means evidence is accumulated over nested time scales, enabling inferences about narratives (i.e., scenes). We illustrate behaviour with Bayesian belief updating neuronal process theories epistemic foraging seen reading. These simulations reproduce perisaccadic delay period activity local field potentials empirically. Finally, exploit structure (e.g., font type) global semantic) violations; reproducing mismatch negativity P300 respectively."
https://openalex.org/W2791521493,https://doi.org/10.1109/access.2018.2814818,Convolutional Recurrent Deep Learning Model for Sentence Classification,2018,"As the amount of unstructured text data that humanity produces overall and on Internet grows, so does need to intelligently process it extract different types knowledge from it. Convolutional neural networks (CNNs) recurrent (RNNs) have been applied natural language processing systems with comparative, remarkable results. The CNN is a noble approach higher level features are invariant local translation. However, requires stacking multiple convolutional layers in order capture long-term dependencies, due locality pooling layers. In this paper, we describe joint RNN framework overcome problem. Briefly, use an unsupervised model train initial word embeddings further tuned by our deep learning network, then, pre-trained parameters network used initialize model. At final stage, proposed combines former information set feature maps learned layer dependencies via long-short-term memory. Empirically, show approach, slight hyperparameter tuning static vectors, achieves outstanding results sentiment analysis benchmarks. Our outperforms several existing approaches term accuracy; also competitive state-of-the-art Stanford Large Movie Review 93.3% accuracy, Sentiment Treebank 48.8% fine-grained 89.2% binary respectively. has significant role reducing number constructing followed as substitute for layer. were able reduce loss detailed, efficient fewer high performance."
https://openalex.org/W2808399042,https://doi.org/10.1145/3206025.3206064,Learning Joint Embedding with Multimodal Cues for Cross-Modal Video-Text Retrieval,2018,"Constructing a joint representation invariant across different modalities (e.g., video, language) is of significant importance in many multimedia applications. While there are number recent successes developing effective image-text retrieval methods by learning representations, the video-text task, however, has not been explored to its fullest extent. In this paper, we study how effectively utilize available multimodal cues from videos for cross-modal task. Based on our analysis, propose novel framework that simultaneously utilizes multi-modal features (different visual characteristics, audio inputs, and text) fusion strategy efficient retrieval. Furthermore, explore several loss functions training embedding modified pairwise ranking Experiments MSVD MSR-VTT datasets demonstrate method achieves performance gain compared state-of-the-art approaches."
https://openalex.org/W2951873305,https://doi.org/10.18653/v1/p19-1485,MultiQA: An Empirical Investigation of Generalization and Transfer in Reading Comprehension,2019,"A large number of reading comprehension (RC) datasets has been created recently, but little analysis done on whether they generalize to one another, and the extent which existing can be leveraged for improving performance new ones. In this paper, we conduct such an investigation over ten RC datasets, training or more source evaluating generalization, as well transfer a target dataset. We analyze factors that contribute show dataset transferring substantially improves performance, even in presence powerful contextual representations from BERT (Devlin et al., 2019). also find multiple leads robust generalization transfer, reduce cost example collection Following our analysis, propose MultiQA, BERT-based model, trained state-of-the-art five datasets. share infrastructure benefit research community."
https://openalex.org/W2962917899,https://doi.org/10.18653/v1/p19-1041,Disentangled Representation Learning for Non-Parallel Text Style Transfer,2019,"This paper tackles the problem of disentangling latent representations style and content in language models. We propose a simple yet effective approach, which incorporates auxiliary multi-task adversarial objectives, for prediction bag-of-words prediction, respectively. show, both qualitatively quantitatively, that are indeed disentangled space. representation learning can be applied to transfer on non-parallel corpora. achieve high performance terms accuracy, preservation, fluency, comparison various previous approaches."
https://openalex.org/W2963502184,https://doi.org/10.18653/v1/p16-1089,Siamese CBOW: Optimizing Word Embeddings for Sentence Representations,2016,"We present the Siamese Continuous Bag of Words (Siamese CBOW) model, a neural network for efficient estimation highquality sentence embeddings. Averaging embeddings words in has proven to be surprisingly successful and way obtaining However, word trained with methods currently available are not optimized task representation, and, thus, likely suboptimal. CBOW handles this problem by training directly purpose being averaged. The underlying learns predicting, from its surrounding sentences. show robustness model evaluating it on 20 datasets stemming wide variety sources."
https://openalex.org/W3104953317,https://doi.org/10.1145/3394486.3403172,LayoutLM: Pre-training of Text and Layout for Document Image Understanding,2020,"Pre-training techniques have been verified successfully in a variety of NLP tasks recent years. Despite the widespread use pre-training models for applications, they almost exclusively focus on text-level manipulation, while neglecting layout and style information that is vital document image understanding. In this paper, we propose \textbf{LayoutLM} to jointly model interactions between text across scanned images, which beneficial great number real-world understanding such as extraction from documents. Furthermore, also leverage features incorporate words' visual into LayoutLM. To best our knowledge, first time are learned single framework document-level pre-training. It achieves new state-of-the-art results several downstream tasks, including form (from 70.72 79.27), receipt 94.02 95.24) classification 93.07 94.42). The code pre-trained LayoutLM publicly available at \url{https://aka.ms/layoutlm}."
https://openalex.org/W2604698497,https://doi.org/10.18653/v1/w17-5526,Frames: a corpus for adding memory to goal-oriented dialogue systems,2017,"This paper proposes a new dataset, Frames, composed of 1369 human-human dialogues with an average 15 turns per dialogue. corpus contains goal-oriented between users who are given some constraints to book trip and assistants search database find appropriate trips. The exhibit complex decision-making behaviour which involve comparing trips, exploring different options, selecting among the trips that were discussed during To drive research on dialogue systems towards handling such behaviour, we have annotated released dataset propose in this task called frame tracking. consists keeping track semantic frames throughout each We rule-based baseline analyse tracking through baseline."
https://openalex.org/W2620558438,https://doi.org/10.1162/tacl_a_00063,Semantic Specialization of Distributional Word Vector Spaces using Monolingual and Cross-Lingual Constraints,2017,"We present Attract-Repel, an algorithm for improving the semantic quality of word vectors by injecting constraints extracted from lexical resources. Attract-Repel facilitates use mono- and cross-lingual resources, yielding semantically specialized vector spaces. Our evaluation shows that method can make existing lexicons to construct high-quality spaces a plethora different languages, facilitating transfer high- lower-resource ones. The effectiveness our approach is demonstrated with state-of-the-art results on similarity datasets in six languages. next show Attract-Repel-specialized boost performance downstream task dialogue state tracking (DST) across multiple Finally, we produced facilitate training multilingual DST models, which brings further improvements."
https://openalex.org/W2740840489,https://doi.org/10.18653/v1/k17-3001,CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies,2017,"The Conference on Computational Natural Language Learning (CoNLL) features a shared task, in which participants train and test their learning systems the same data sets. In 2017, task was devoted to dependency parsers for large number of languages, real-world setting without any gold-standard annotation input. All sets followed unified scheme, namely that Universal Dependencies. this paper, we define evaluation methodology, describe how were prepared, report analyze main results, provide brief categorization different approaches participating systems."
https://openalex.org/W2784010253,https://doi.org/10.1371/journal.pone.0203794,Automatic detection of cyberbullying in social media text,2018,"While social media offer great communication opportunities, they also increase the vulnerability of young people to threatening situations online. Recent studies report that cyberbullying constitutes a growing problem among youngsters. Successful prevention depends on adequate detection potentially harmful messages and information overload Web requires intelligent systems identify potential risks automatically. The focus this paper is automatic in text by modelling posts written bullies, victims, bystanders online bullying. We describe collection fine-grained annotation training corpus for English Dutch perform series binary classification experiments determine feasibility detection. make use linear support vector machines exploiting rich feature set investigate which sources contribute most particular task. Experiments holdout test reveal promising results cyberbullying-related posts. After optimisation hyperparameters, classifier yields an F1-score 64% 61% respectively, considerably outperforms baseline based keywords word unigrams."
https://openalex.org/W2804221886,https://doi.org/10.18653/v1/n18-1131,A Neural Layered Model for Nested Named Entity Recognition,2018,"Entity mentions embedded in longer entity are referred to as nested entities. Most named recognition (NER) systems deal only with the flat entities and ignore inner ones, which fails capture finer-grained semantic information underlying texts. To address this issue, we propose a novel neural model identify by dynamically stacking NER layers. Each layer is based on state-of-the-art that captures sequential context representation bidirectional Long Short-Term Memory (LSTM) feeds it cascaded CRF layer. Our merges output of LSTM current build new for detected subsequently them into next This allows our extract outer taking full advantage encoded their corresponding entities, an inside-to-outside way. stacks layers until no extracted. Extensive evaluation shows dynamic outperforms feature-based NER, achieving 74.7% 72.2% GENIA ACE2005 datasets, respectively, terms F-score."
https://openalex.org/W2954275542,https://doi.org/10.18653/v1/n19-1062,Black is to Criminal as Caucasian is to Police: Detecting and Removing Multiclass Bias in Word Embeddings,2019,"Online texts - across genres, registers, domains, and styles are riddled with human stereotypes, expressed in overt or subtle ways. Word embeddings, trained on these texts, perpetuate amplify propagate biases to machine learning models that use word embeddings as features. In this work, we propose a method debias multiclass settings such race religion, extending the work of (Bolukbasi et al., 2016) from binary setting, gender. Next, novel methodology for evaluation debiasing. We demonstrate our debiasing is robust maintains efficacy standard NLP tasks."
https://openalex.org/W2963481894,https://doi.org/10.18653/v1/w17-3006,One-step and Two-step Classification for Abusive Language Detection on Twitter,2017,"Automatic abusive language detection is a difficult but important task for online social media. Our research explores two-step approach of performing classification on and then classifying into specific types compares it with one-step doing one multi-class detecting sexist racist languages. With public English Twitter corpus 20 thousand tweets in the type sexism racism, our shows promising performance 0.827 F-measure by using HybridCNN 0.824 logistic regression two-steps."
https://openalex.org/W2963682821,https://doi.org/10.18653/v1/w15-3904,Boosting Named Entity Recognition with Neural Character Embeddings,2015,"Most state-of-the-art named entity recognition (NER) systems rely on handcrafted features and the output of other NLP tasks such as part-of-speech (POS) tagging text chunking. In this work we propose a language-independent NER system that uses automatically learned only. Our approach is based CharWNN deep neural network, which word-level character-level representations (embeddings) to perform sequential classification. We an extensive number experiments using two annotated corpora in different languages: HAREM I corpus, contains texts Portuguese; SPA CoNLL2002 Spanish. experimental results give evidence contribution character embeddings for NER. Moreover, demonstrate same network has been successfully applied POS can also achieve state-of-theart language-independet NER, hyperparameters, without any features. For outperforms by 7.9 points F1-score total scenario (ten NE classes). CoNLL-2002 state-ofthe-art 0.8 point F1."
https://openalex.org/W2963964898,https://doi.org/10.18653/v1/n18-2078,Exploiting Semantics in Neural Machine Translation with Graph Convolutional Networks,2018,"Semantic representations have long been argued as potentially useful for enforcing meaning preservation and improving generalization performance of machine translation methods. In this work, we are the first to incorporate information about predicate-argument structure source sentences (namely, semantic-role representations) into neural translation. We use Graph Convolutional Networks (GCNs) inject a semantic bias sentence encoders achieve improvements in BLEU scores over linguistic-agnostic syntax-aware versions on English–German language pair."
https://openalex.org/W2964235839,https://doi.org/10.18653/v1/d16-1120,Demographic Dialectal Variation in Social Media: A Case Study of African-American English,2016,"Though dialectal language is increasingly abundant on social media, few resources exist for developing NLP tools to handle such language. We conduct a case study of in online conversational text by investigating African-American English (AAE) Twitter. propose distantly supervised model identify AAE-like from demographics associated with geo-located messages, and we verify that this follows well-known AAE linguistic phenomena. In addition, analyze the quality existing identification dependency parsing text, demonstrating they perform poorly compared white speakers. also provide an ensemble classifier which eliminates disparity release new corpus tweets containing"
https://openalex.org/W3102725307,https://doi.org/10.18653/v1/2020.findings-emnlp.58,Revisiting Pre-Trained Models for Chinese Natural Language Processing,2020,"Bidirectional Encoder Representations from Transformers (BERT) has shown marvelous improvements across various NLP tasks, and consecutive variants have been proposed to further improve the performance of pre-trained language models. In this paper, we target on revisiting Chinese models examine their effectiveness in a non-English release model series community. We also propose simple but effective called MacBERT, which improves upon RoBERTa several ways, especially masking strategy that adopts MLM as correction (Mac). carried out extensive experiments eight tasks revisit existing well MacBERT. Experimental results show MacBERT could achieve state-of-the-art performances many ablate details with findings may help future research. Resources available: https://github.com/ymcui/MacBERT"
https://openalex.org/W2250807343,https://doi.org/10.18653/v1/d15-1031,Aligning Knowledge and Text Embeddings by Entity Descriptions,2015,"We study the problem of jointly embedding a knowledge base and text corpus. The key issue is alignment model making sure vectors entities, relations words are in same space. Wang et al. (2014a) rely on Wikipedia anchors, applicable scope quite limited. In this paper we propose new based descriptions without dependency anchors. require vector an entity not only to fit structured constraints KBs but also be equal computed from description. Extensive experiments show that, proposed approach consistently performs comparably or even better than method (2014a), which encouraging as do use any anchor information."
https://openalex.org/W2613831280,https://doi.org/10.1016/j.jbi.2017.05.002,Character-level neural network for biomedical named entity recognition,2017,"• I proposed a deep neural networks for Biomedical named entity recognition. A conditional random field was added to capture relationship between entities. Word vectors were initialized using pretrained word embeddings. Character-level of words useful handling out-of-vocabulary issues. recognition (BNER), which extracts important entities such as genes and proteins, is challenging task in automated systems that mine knowledge biomedical texts. The previous state-of-the-art required large amounts task-specific the form feature engineering, lexicons data pre-processing achieve high performance. In this paper, we introduce novel network architecture benefits from both word- character-level representations automatically, by combination bidirectional long short-term memory (LSTM) (CRF) eliminating need most engineering tasks. We evaluate our system on two datasets: JNLPBA corpus BioCreAtIvE II Gene Mention (GM) corpus. obtained performance outperforming systems. To best knowledge, are first investigate networks, CRF, embeddings representation recognizing"
https://openalex.org/W2733963201,https://doi.org/10.1038/s41598-017-05402-0,High-throughput Identification and Characterization of Two-dimensional Materials using Density functional theory,2017,"We introduce a simple criterion to identify two-dimensional (2D) materials based on the comparison between experimental lattice constants and mainly obtained from Materials-Project (MP) density functional theory (DFT) calculation repository. Specifically, if relative difference two for specific material is greater than or equal 5%, we predict them be good candidates 2D materials. have predicted at least 1356 such For all systems satisfying our criterion, manually create single layer calculate their energetics, structural, electronic, elastic properties both bulk cases. Currently database consists of 1012 430 materials, which 371 are common layer. The rest calculations underway. To validate calculated exfoliation energy suggested layered found that in 88.9% cases currently accepted was satisfied. Also, using molybdenum telluride as test case, performed X-ray diffraction Raman scattering experiments benchmark understand applicability limitations. data publicly available website http://www.ctcms.nist.gov/~knc6/JVASP.html."
https://openalex.org/W2734443755,https://doi.org/10.18653/v1/d17-1235,Generating High-Quality and Informative Conversation Responses with Sequence-to-Sequence Models,2017,"Sequence-to-sequence models have been applied to the conversation response generation problem where source sequence is history and target response. Unlike translation, responding inherently creative. The of long, informative, coherent, diverse responses remains a hard task. In this work, we focus on single turn setting. We add self-attention decoder maintain coherence in longer responses, propose practical approach, called glimpse-model, for scaling large datasets. introduce stochastic beam-search algorithm with segment-by-segment reranking which lets us inject diversity earlier process. trained combined data set over 2.3B messages mined from web. human evaluation studies, our method produces overall, higher proportion rated as acceptable excellent length increases, compared baseline sequence-to-sequence explicit length-promotion. A back-off strategy better full spectrum lengths."
https://openalex.org/W2927032858,https://doi.org/10.1186/s12911-018-0723-6,A clinical text classification paradigm using weak supervision and deep representation,2019,"Automatic clinical text classification is a natural language processing (NLP) technology that unlocks information embedded in narratives. Machine learning approaches have been shown to be effective for tasks. However, successful machine model usually requires extensive human efforts create labeled training data and conduct feature engineering. In this study, we propose paradigm using weak supervision deep representation reduce these efforts.We develop rule-based NLP algorithm automatically generate labels the data, then use pre-trained word embeddings as features models. Since trained on generated by automatic algorithm, process called supervision. We evaluat effectiveness two institutional case studies at Mayo Clinic: smoking status proximal femur (hip) fracture classification, one study public dataset: i2b2 2006 shared task. test four widely used models, namely, Support Vector (SVM), Random Forest (RF), Multilayer Perceptron Neural Networks (MLPNN), Convolutional (CNN), paradigm. Precision, recall, F1 score are metrics evaluate performance.CNN achieves best performance both tasks (F1 score: 0.92 Clinic 0.97 classification). show significantly outperform tf-idf topic modeling paradigm, CNN captures additional patterns from compared algorithms. also observe drawbacks of proposed more sensitive size might not complex multiclass tasks.The could creation engineering applying leveraging representation. The experimental experiments validated"
https://openalex.org/W2962808855,https://doi.org/10.24963/ijcai.2018/570,Reinforced Mnemonic Reader for Machine Reading Comprehension,2018,"In this paper, we introduce the Reinforced Mnemonic Reader for machine reading comprehension tasks, which enhances previous attentive readers in two aspects. First, a reattention mechanism is proposed to refine current attentions by directly accessing past that are temporally memorized multi-round alignment architecture, so as avoid problems of attention redundancy and deficiency. Second, new optimization approach, called dynamic-critical reinforcement learning, introduced extend standard supervised method. It always encourages predict more acceptable answer address convergence suppression problem occurred traditional learning algorithms. Extensive experiments on Stanford Question Answering Dataset (SQuAD) show our model achieves state-of-the-art results. Meanwhile, outperforms systems over 6% terms both Exact Match F1 metrics adversarial SQuAD datasets."
https://openalex.org/W2963769536,https://doi.org/10.18653/v1/p18-1157,Stochastic Answer Networks for Machine Reading Comprehension,2018,"We propose a simple yet robust stochastic answer network (SAN) that simulates multi-step reasoning in machine reading comprehension. Compared to previous work such as ReasoNet which used reinforcement learning determine the number of steps, unique feature is use kind prediction dropout on module (final layer) neural during training. show this trick improves robustness and achieves results competitive state-of-the-art Stanford Question Answering Dataset (SQuAD), Adversarial SQuAD, Microsoft MAchine Reading COmprehension (MS MARCO)."
https://openalex.org/W2964190861,https://doi.org/10.18653/v1/n18-1033,Classical Structured Prediction Losses for Sequence to Sequence Learning,2018,"There has been much recent work on training neural attention models at the sequence-level using either reinforcement learning-style methods or by optimizing beam. In this paper, we survey a range of classical objective functions that have widely used to train linear for structured prediction and apply them sequence models. Our experiments show these losses can perform surprisingly well slightly outperforming beam search optimization in like setup. We also report new state art results both IWSLT’14 German-English translation as Gigaword abstractive summarization. On large WMT’14 English-French task, achieves 41.5 BLEU which is par with art."
https://openalex.org/W3173151551,https://doi.org/10.1109/tkde.2021.3090866,Self-supervised Learning: Generative or Contrastive,2021,"Deep supervised learning has achieved great success in the last decade. However, its deficiencies of dependence on manual labels and vulnerability to attacks have driven people explore a better solution. As an alternative, self-supervised attracts many researchers for soaring performance representation several years. Self-supervised leverages input data itself as supervision benefits almost all types downstream tasks. In this survey, we take look into new methods computer vision, natural language processing, graph learning. We comprehensively review existing empirical summarize them three main categories according their objectives: generative, contrastive, generative-contrastive (adversarial). further investigate related theoretical analysis work provide deeper thoughts how works. Finally, briefly discuss open problems future directions An outline slide survey is provided."
https://openalex.org/W2140610559,https://doi.org/10.3115/v1/p15-1144,Sparse Overcomplete Word Vector Representations,2015,"Current distributed representations of words show little resemblance to theories lexical semantics. The former are dense and uninterpretable, the latter largely based on familiar, discrete classes (e.g., supersenses) relations synonymy hypernymy). We propose methods that transform word vectors into sparse (and optionally binary) vectors. resulting more similar interpretable features typically used in NLP, though they discovered automatically from raw corpora. Because highly sparse, computationally easy work with. Most importantly, we find outperform original benchmark tasks."
https://openalex.org/W2162018888,https://doi.org/10.3758/s13428-015-0647-3,The Novel Object and Unusual Name (NOUN) Database: A collection of novel images for use in experimental research,2016,"Many experimental research designs require images of novel objects. Here we introduce the Novel Object and Unusual Name (NOUN) Database. This database contains 64 primary object additional exemplars for ten basic- nine global-level categories. The objects' novelty was confirmed by both self-report a lack consensus on questions that required participants to name identify We also found correlated with qualifying naming responses pertaining colors. results from similarity sorting task (and subsequent multidimensional scaling analysis ratings) demonstrated objects are complex distinct entities vary along several featural dimensions beyond simply shape color. A final experiment item comprised sub- superordinate These may be useful in variety settings, particularly developmental psychology other language, categorization, perception, visual memory, related domains."
https://openalex.org/W2512597464,https://doi.org/10.18653/v1/p16-1218,Graph-based Dependency Parsing with Bidirectional LSTM,2016,"In this paper, we propose a neural network model for graph-based dependency parsing which utilizes Bidirectional LSTM (BLSTM) to capture richer contextual information instead of using high-order factorization, and enable our use much fewer features than previous work. addition, an effective way learn sentence segment embedding on sentence-level based extra forward network. Although uses only first-order experiments English Peen Treebank Chinese Penn show that could be competitive with higher-order models state-of-the-art models."
https://openalex.org/W2546521646,https://doi.org/10.1016/j.knosys.2016.11.001,Data classification using evidence reasoning rule,2017,"In Dempster-Shafer evidence theory (DST) based classifier design, Dempster's combination (DC) rule is commonly used as a multi-attribute to combine collected from different attributes. The main aim of this paper present classification method using novel i.e., the reasoning (ER) rule. As an improvement DC rule, newly proposed ER defines reliability and weight evidence. former indicates ability attribute or its provide correct assessment for problem, latter reflects relative importance in comparison with other when they need be combined. rule-based procedure expatiated acquisition estimation It purely data-driven approach without making any assumptions about relationships between attributes class memberships, specific statistic distributions data. Experiential results on five popular benchmark databases taken University California Irvine (UCI) machine learning database show high accuracy that competitive classical mainstream classifiers."
https://openalex.org/W2743028754,https://doi.org/10.1186/s12859-017-1776-8,A neural network multi-task learning approach to biomedical named entity recognition,2017,"Named Entity Recognition (NER) is a key task in biomedical text mining. Accurate NER systems require task-specific, manually-annotated datasets, which are expensive to develop and thus limited size. Since such datasets contain related but different information, an interesting question whether it might be possible use them together improve performance. To investigate this, we supervised, multi-task, convolutional neural network models apply large number of varied existing named entity datasets. Additionally, investigated the effect dataset size on performance both single- multi-task settings.We present single-task model for NER, Multi-output Dependent model. We three 15 containing multiple entities including Anatomy, Chemical, Disease, Gene/Protein Species. Each represent task. The results from then compared evidence benefits Multi-task Learning. With observed average F-score improvement 0.8% when baseline 78.4%. Although there was significant drop one dataset, improves significantly five by up 6.3%. For 0.4% There were no drops any six 1.1%. experiments found that as decreased, multi-output model's increased model's. Using 50, 25 10% training data resulted approximately 3.4, 8 16.7% respectively 0.2, 3.0 9.8% model.Our show that, average, produced better than trained single dataset. also Learning beneficial small Across various settings improvements significant, demonstrating benefit this"
https://openalex.org/W2800516701,https://doi.org/10.1037/bul0000158,Predicting while comprehending language: A theory and review.,2018,"Researchers agree that comprehenders regularly predict upcoming language, but they do not always on what prediction is (and how to differentiate it from integration) or constitutes evidence for it. After defining prediction, we show occurs at all linguistic levels semantics form, and then propose a theory of which mechanisms use predict. We argue most effectively using their production system (i.e., prediction-by-production): They covertly imitate the form speaker's utterance construct representation underlying communicative intention. Comprehenders can run this intention through own prepare predicted utterance. But doing so takes time resources, vary in extent preparation, with many groups (non-native speakers, illiterates, children, older adults) less than typical native young adults. thus prediction-by-production an optional mechanism, augmented by based association. Support our proposal comes areas research (electrophysiological, eye-tracking, behavioral studies reading, spoken language processing context visual environments, speech processing, dialogue). (PsycINFO Database Record"
https://openalex.org/W2885775891,https://doi.org/10.1007/978-3-030-01234-2_29,A Joint Sequence Fusion Model for Video Question Answering and Retrieval,2018,"We present an approach named JSFusion (Joint Sequence Fusion) that can measure semantic similarity between any pairs of multimodal sequence data (e.g. a video clip and language sentence). Our matching network consists two key components. First, the Joint Semantic Tensor composes dense pairwise representation into 3D tensor. Then, Convolutional Hierarchical Decoder computes their score by discovering hidden hierarchical matches modalities. Both modules leverage attention mechanisms learn to promote well-matched patterns while prune out misaligned ones in bottom-up manner. Although is universal model be applicable data, this work focuses on video-language tasks including retrieval QA. evaluate three VQA LSMDC, for which our achieves best performance reported so far. also perform multiple-choice movie MSR-VTT dataset, outperforms many state-of-the-art methods."
https://openalex.org/W2891417293,https://doi.org/10.18653/v1/d18-1157,RESIDE: Improving Distantly-Supervised Neural Relation Extraction using Side Information,2018,"Distantly-supervised Relation Extraction (RE) methods train an extractor by automatically aligning relation instances in a Knowledge Base (KB) with unstructured text. In addition to instances, KBs often contain other relevant side information, such as aliases of relations (e.g., founded and co-founded are for the founderOfCompany). RE models usually ignore readily available information. this paper, we propose RESIDE, distantly-supervised neural extraction method which utilizes additional information from improved extraction. It uses entity type alias imposing soft constraints while predicting relations. RESIDE employs Graph Convolution Networks (GCN) encode syntactic text improves performance even when limited is available. Through extensive experiments on benchmark datasets, demonstrate RESIDE’s effectiveness. We have made source code encourage reproducible research."
https://openalex.org/W2950768109,https://doi.org/10.18653/v1/p19-1282,Is Attention Interpretable?,2019,"Attention mechanisms have recently boosted performance on a range of NLP tasks. Because attention layers explicitly weight input components’ representations, it is also often assumed that can be used to identify information models found important (e.g., specific contextualized word tokens). We test whether assumption holds by manipulating weights in already-trained text classification and analyzing the resulting differences their predictions. While we observe some ways which higher correlate with greater impact model predictions, find many this does not hold, i.e., where gradient-based rankings better predict effects than magnitudes. conclude while noisily predicts overall importance model, no means fail-safe indicator."
https://openalex.org/W2962676330,https://doi.org/10.48550/arxiv.1709.04109,Empower Sequence Labeling with Task-Aware Neural Language Model,2017,"Linguistic sequence labeling is a general modeling approach that encompasses variety of problems, such as part-of-speech tagging and named entity recognition. Recent advances in neural networks (NNs) make it possible to build reliable models without handcrafted features. However, many cases, hard obtain sufficient annotations train these models. In this study, we develop novel framework extract abundant knowledge hidden raw texts empower the task. Besides word-level contained pre-trained word embeddings, character-aware language are incorporated character-level knowledge. Transfer learning techniques further adopted mediate different components guide model towards key Comparing previous methods, task-specific allows us adopt more concise conduct efficient training. Different from most transfer proposed does not rely on any additional supervision. It extracts self-contained order information training sequences. Extensive experiments benchmark datasets demonstrate effectiveness leveraging efficiency co-training. For example, CoNLL03 NER task, completes about 6 hours single GPU, reaching F1 score 91.71$\pm$0.10 using extra annotation."
https://openalex.org/W2962960733,https://doi.org/10.1109/icmla.2018.00120,Automated Vulnerability Detection in Source Code Using Deep Representation Learning,2018,"Increasing numbers of software vulnerabilities are discovered every year whether they reported publicly or internally in proprietary code. These can pose serious risk exploit and result system compromise, information leaks, denial service. We leveraged the wealth C C++ open-source code available to develop a largescale function-level vulnerability detection using machine learning. To supplement existing labeled datasets, we compiled vast dataset millions functions it with carefully-selected findings from three different static analyzers that indicate potential exploits. Using these developed fast scalable tool based on deep feature representation learning directly interprets lexed source evaluated our both real packages NIST SATE IV benchmark dataset. Our results demonstrate is promising approach for automated detection."
https://openalex.org/W2963409068,https://doi.org/10.1109/cvpr.2017.378,MDNet: A Semantically and Visually Interpretable Medical Image Diagnosis Network,2017,"The inability to interpret the model prediction in semantically and visually meaningful ways is a well-known shortcoming of most existing computer-aided diagnosis methods. In this paper, we propose MDNet establish direct multimodal mapping between medical images diagnostic reports that can read images, generate reports, retrieve by symptom descriptions, visualize attention, provide justifications network process. includes an image language model. proposed enhance multi-scale feature ensembles utilization efficiency. model, integrated with our improved attention mechanism, aims explore discriminative descriptions from learn sentence words pixels. overall trained end-to-end using developed optimization strategy. Based on pathology bladder cancer its (BCIDR) dataset, conduct sufficient experiments demonstrate outperforms comparative baselines. obtains state-of-the-art performance two CIFAR datasets as well."
https://openalex.org/W2963955958,https://doi.org/10.1109/cvpr.2016.247,Less is More: Zero-Shot Learning from Online Textual Documents with Noise Suppression,2016,"Classifying a visual concept merely from its associated online textual source, such as Wikipedia article, is an attractive research topic in zero-shot learning because it alleviates the burden of manually collecting semantic attributes. Several recent works have pursued this approach by exploring various ways connecting and text domains. This paper revisits idea stepping further to consider one important factor: representation usually too noisy for application. consideration motivates us design simple-but-effective method capable suppressing noise text. More specifically, we propose $l_{2,1}$-norm based objective function which can simultaneously suppress signal learn match document features. We also develop optimization algorithm efficiently solve resulting problem. By conducting experiments on two large datasets, demonstrate that proposed significantly outperforms competing methods rely information sources but without explicit suppression. make in-depth analysis provide insight what kind documents useful learning."
https://openalex.org/W2990818246,https://doi.org/10.1109/iccv.2019.00902,Entangled Transformer for Image Captioning,2019,"In image captioning, the typical attention mechanisms are arduous to identify equivalent visual signals especially when predicting highly abstract words. This phenomenon is known as semantic gap between vision and language. problem can be overcome by providing attributes that homologous Thanks inherent recurrent nature gated operating mechanism, Recurrent Neural Network (RNN) its variants dominating architectures in captioning. However, designing elaborate integrate inputs attributes, RNN-like become unflexible due their complexities. this paper, we investigate a Transformer-based sequence modeling framework, built only with layers feedforward layers. To bridge gap, introduce EnTangled Attention (ETA) enables Transformer exploit information simultaneously. Furthermore, Gated Bilateral Controller (GBC) proposed guide interactions multimodal information. We name our model ETA-Transformer. Remarkably, ETA-Transformer achieves state-of-the-art performance on MSCOCO captioning dataset. The ablation studies validate improvements of modules."
https://openalex.org/W1962025484,https://doi.org/10.1109/cvpr.2015.7299046,Transferring a semantic representation for person re-identification and search,2015,"Learning semantic attributes for person re-identification and description-based search has gained increasing interest due to attributes' great potential as a pose view-invariant representation. However, existing attribute-centric approaches have thus far underperformed state-of-the-art conventional approaches. This is their nonscalable need extensive domain (camera) specific annotation. In this paper we present new attribute learning approach search. Our model trained on fashion photography datasets - either weakly or strongly labelled. It can then be transferred adapted provide powerful description of surveillance detections, without requiring any supervision. The resulting representation useful both unsupervised supervised re-identification, achieving near performance respectively. Furthermore, it allows integrated within the same framework."
https://openalex.org/W2281963941,https://doi.org/10.14569/ijacsa.2015.060712,Survey on Chatbot Design Techniques in Speech Conversation Systems,2015,"Human-Computer Speech is gaining momentum as a technique of computer interaction. There has been recent upsurge in speech based search engines and assistants such Siri, Google Chrome Cortana. Natural Language Processing (NLP) techniques NLTK for Python can be applied to analyse speech, intelligent responses found by designing an engine provide appropriate human like responses. This type programme called Chatbot, which the focus this study. paper presents survey on used design Chatbots comparison made between different from nine carefully selected papers according main methods adopted. These are representative significant improvements last decade. The discusses similarities differences examines particular Loebner prize-winning Chatbots."
https://openalex.org/W2336246663,https://doi.org/10.1007/s10726-016-9479-5,Simplified Neutrosophic Linguistic Multi-criteria Group Decision-Making Approach to Green Product Development,2017,"For many companies, green product development has become a key strategic consideration due to regulatory requirements and market trends. In this paper, the life cycle assessment technique is used develop an innovative multi-criteria group decision-making approach that incorporates power aggregation operators TOPSIS-based QUALIFLEX method in order solve design selection problems using neutrosophic linguistic information. Differences semantics as well risk preferences of decision-makers are considered proposed method. The practicality effectiveness then demonstrated through illustrative example, which select optimum design, followed by sensitivity comparative analyses."
https://openalex.org/W2405884322,https://doi.org/10.18653/v1/p16-1035,Query Expansion with Locally-Trained Word Embeddings,2016,"Continuous space word embeddings have received a great deal of attention in the natural language processing and machine learning communities for their ability to model term similarity other relationships. We study use relatedness context query expansion ad hoc information retrieval. demonstrate that such as word2vec GloVe, when trained globally, underperform corpus specific retrieval tasks. These results suggest tasks benefiting from global may also benefit local embeddings."
https://openalex.org/W2462891382,https://doi.org/10.1016/j.patrec.2016.06.012,Representation learning for very short texts using weighted word embedding aggregation,2016,"Short text messages such as tweets are very noisy and sparse in their use of vocabulary. Traditional textual representations, tf-idf, have difficulty grasping the semantic meaning texts, which is important applications event detection, opinion mining, news recommendation, etc. We constructed a method based on word embeddings frequency information to arrive at low-dimensional representations for short texts designed capture similarity. For this purpose we weight-based model learning procedure novel median-based loss function. This paper discusses details our optimization methods, together with experimental results both Wikipedia Twitter data. find that outperforms baseline approaches experiments, it generalizes well different without retraining. Our therefore capable retaining most text, applicable out-of-the-box."
https://openalex.org/W2474574787,https://doi.org/10.1109/cvpr.2016.321,Learning Aligned Cross-Modal Representations from Weakly Aligned Data,2016,"People can recognize scenes across many different modalities beyond natural images. In this paper, we investigate how to learn cross-modal scene representations that transfer modalities. To study problem, introduce a new dataset. While convolutional neural networks categorize well, they also an intermediate representation not aligned modalities, which is undesirable for crossmodal applications. We present methods regularize so have shared agnostic of the modality. Our experiments suggest our help retrieval. Moreover, visualizations units emerge in tend activate on consistent concepts independently"
https://openalex.org/W2887712318,https://doi.org/10.1109/tip.2018.2855422,Video Captioning by Adversarial LSTM,2018,"In this paper, we propose a novel approach to video captioning based on adversarial learning and Long-Short Term Memory (LSTM). With solution concept aim at compensating for the deficiencies of LSTM-based methods that generally show potential effectively handle temporal nature data when generating captions, but also typically suffer from exponential error accumulation. Specifically, adopt standard Generative Adversarial Network (GAN) architecture, characterized by an interplay two competing processes: ""generator"", which generates textual sentences given visual content video, ""discriminator"" controls accuracy generated sentences. The discriminator acts as ""adversary"" towards generator with its controlling mechanism helps become more accurate. For module, take existing using LSTM network. discriminator, realization specifically tuned problem taking both features input. This leads our proposed LSTM-GAN system experimentally significantly outperform public datasets."
https://openalex.org/W2950457956,https://doi.org/10.18653/v1/p19-1081,OpenDialKG: Explainable Conversational Reasoning with Attention-based Walks over Knowledge Graphs,2019,"We study a conversational reasoning model that strategically traverses through large-scale common fact knowledge graph (KG) to introduce engaging and contextually diverse entities attributes. For this study, we collect new Open-ended Dialog KG parallel corpus called OpenDialKG, where each utterance from 15K human-to-human role-playing dialogs is manually annotated with ground-truth reference corresponding paths 1M+ facts. then propose the DialKG Walker learns symbolic transitions of dialog contexts as structured traversals over KG, predicts natural given previous via novel domain-agnostic, attention-based path decoder. Automatic human evaluations show our can retrieve more human-like responses than state-of-the-art baselines or rule-based models, in both in-domain cross-domain tasks. The proposed also generates walk for entity retrieved, providing way explain reasoning."
https://openalex.org/W2964289193,https://doi.org/10.18653/v1/p18-1118,Document Context Neural Machine Translation with Memory Networks,2018,"We present a document-level neural machine translation model which takes both source and target document context into account using memory networks. the problem as structured prediction with interdependencies among observed hidden variables, i.e., sentences their unobserved translations in document. The resulting is tackled equipped two components, one each for side, to capture documental interdependencies. train end-to-end, propose an iterative decoding algorithm based on block coordinate descent. Experimental results of English from French, German, Estonian documents show that our effective exploiting context, statistically significantly outperforms previous work terms BLEU METEOR."
https://openalex.org/W2184378182,https://doi.org/10.1093/jamia/ocv044,RobotReviewer: evaluation of a system for automatically assessing bias in clinical trials,2016,"Abstract Objective To develop and evaluate RobotReviewer, a machine learning (ML) system that automatically assesses bias in clinical trials. From (PDF-formatted) trial report, the should determine risks of for domains defined by Cochrane Risk Bias (RoB) tool, extract supporting text these judgments. Methods We algorithmically annotated 12,808 PDFs using data from Database Systematic Reviews (CDSR). Trials were labeled as being at low or high/unclear risk each domain, sentences informative not. This dataset was used to train multi-task ML model. estimated accuracy judgments versus humans comparing trials with two more independent RoB assessments CDSR. Twenty blinded experienced reviewers rated relevance text, output equivalent (human-extracted) Results By retrieving top 3 candidate per document (top3 recall), best relevant than CDSR, but not significantly (60.4% ‘highly relevant' v 56.5% reviews; difference +3.9%, [−3.2% +10.9%]). Model less accurate those published reviews, though &amp;lt;10% (overall 71.0% 78.3% CDSR). Conclusion assessment may be automated reasonable accuracy. Automatically identified is equal quality manually technology could substantially reduce reviewer workload expedite evidence syntheses."
https://openalex.org/W2252217313,https://doi.org/10.18653/v1/s15-2047,SemEval-2015 Task 3: Answer Selection in Community Question Answering,2015,"Community Question Answering (cQA) provides new interesting research directions to the traditional (QA) field, e.g., exploitation of interaction between users and structure related posts. In this context, we organized SemEval2015 Task 3 on Answer Selection in cQA, which included two subtasks: (a) classifying answers as good, bad, or potentially relevant with respect question, (b) answering a YES/NO question yes, no, unsure, based list all answers. We set subtask A for Arabic English relatively different cQA domains, i.e., Qatar Living website English, Quran-related Arabic. used crowdsourcing Amazon Mechanical Turk label large training dataset, released community. Thirteen teams participated challenge total 61 submissions: 24 primary 37 contrastive. The best systems achieved an official score (macro-averaged F1) 57.19 63.7 subtasks B, 78.55 A."
https://openalex.org/W2796350793,https://doi.org/10.7554/elife.33468,Large-scale replication study reveals a limit on probabilistic prediction in language comprehension,2018,"Do people routinely pre-activate the meaning and even phonological form of upcoming words? The most acclaimed evidence for prediction comes from a 2005 Nature Neuroscience publication by DeLong, Urbach Kutas, who observed graded modulation electrical brain potentials (N400) to nouns preceding articles probability that use word continue sentence fragment (‘cloze’). In our direct replication study spanning 9 laboratories (N=334), pre-registered replication-analyses exploratory Bayes factor analyses successfully replicated noun-results but, crucially, not article-results. Pre-registered single-trial also yielded statistically significant effect but articles. Exploratory Bayesian showed article-effect may be non-zero is likely far smaller than originally reported too small observe without very large sample sizes. Our results do support view readers predictable words."
https://openalex.org/W2888541716,https://doi.org/10.18653/v1/d18-1398,Meta-Learning for Low-Resource Neural Machine Translation,2018,"In this paper, we propose to extend the recently introduced model-agnostic meta-learning algorithm (MAML, Finn, et al., 2017) for low-resource neural machine translation (NMT). We frame as a problem where learn adapt languages based on multilingual high-resource language tasks. use universal lexical representation (Gu 2018b) overcome input-output mismatch across different languages. evaluate proposed strategy using eighteen European (Bg, Cs, Da, De, El, Es, Et, Fr, Hu, It, Lt, Nl, Pl, Pt, Sk, Sl, Sv and Ru) source tasks five diverse (Ro,Lv, Fi, Tr Ko) target show that approach significantly outperforms multilingual, transfer learning (Zoph 2016) enables us train competitive NMT system with only fraction of training examples. For instance, can achieve high 22.04 BLEU Romanian-English WMT’16 by seeing 16,000 translated words (~600 parallel sentences)"
https://openalex.org/W2962795068,https://doi.org/10.18653/v1/p16-1157,Cross-lingual Models of Word Embeddings: An Empirical Comparison,2016,"Despite interest in using cross-lingual knowledge to learn word embeddings for various tasks, a systematic comparison of the possible approaches is lacking literature. We perform an extensive evaluation four popular inducing embeddings, each requiring different form supervision, on typographically language pairs. Our setup spans including intrinsic mono-lingual and similarity, extrinsic downstream semantic syntactic applications. show that models which require expensive almost always better, but cheaply supervised often prove competitive certain tasks."
https://openalex.org/W2962953307,https://doi.org/10.18653/v1/d18-1421,Paraphrase Generation with Deep Reinforcement Learning,2018,"Automatic generation of paraphrases from a given sentence is an important yet challenging task in natural language processing (NLP). In this paper, we present deep reinforcement learning approach to paraphrase generation. Specifically, propose new framework for the task, which consists generator and evaluator, both are learned data. The generator, built as sequence-to-sequence model, can produce sentence. constructed matching judge whether two sentences each other. first trained by then further fine-tuned reward evaluator. For methods based on supervised inverse respectively, depending type available training Experimental results datasets demonstrate proposed models (the generators) more accurate outperform state-of-the-art automatic evaluation human evaluation."
https://openalex.org/W2963034998,https://doi.org/10.18653/v1/p18-1090,Unpaired Sentiment-to-Sentiment Translation: A Cycled Reinforcement Learning Approach,2018,"The goal of sentiment-to-sentiment “translation” is to change the underlying sentiment a sentence while keeping its content. main challenge lack parallel data. To solve this problem, we propose cycled reinforcement learning method that enables training on unpaired data by collaboration between neutralization module and an emotionalization module. We evaluate our approach two review datasets, Yelp Amazon. Experimental results show significantly outperforms state-of-the-art systems. Especially, proposed substantially improves content preservation performance. BLEU score improved from 1.64 22.46 0.56 14.06 respectively."
https://openalex.org/W2101217916,https://doi.org/10.18653/v1/s15-2080,SemEval-2015 Task 11: Sentiment Analysis of Figurative Language in Twitter,2015,"This report summarizes the objectives and evaluation of SemEval 2015 task on sentiment analysis figurative language Twitter (Task 11). is first wholly dedicated to analyzing Twitter. Specifically, three broad classes are considered: irony, sarcasm metaphor. Gold standard sets 8000 training tweets 4000 test were annotated using workers crowdsourcing platform CrowdFlower. Participating systems required provide a fine-grained score an 11-point scale (-5 +5, including 0 for neutral intent) each tweet, evaluated against gold both Cosinesimilarity Mean-Squared-Error measure."
https://openalex.org/W2251994258,https://doi.org/10.18653/v1/w15-3001,Findings of the 2015 Workshop on Statistical Machine Translation,2015,"This paper presents the results of WMT15 shared tasks, which included a standard news translation task, metrics tuning task for run-time estimation machine quality, and an automatic post-editing task. year, 68 systems from 24 institutions were submitted to ten directions in An additional 7 anonymized included, then evaluated both automatically manually. The quality had three subtasks, with total 10 teams, submitting 34 entries. pilot postediting 4"
https://openalex.org/W2406624406,https://doi.org/10.1016/j.knosys.2016.05.040,A hybrid approach to the sentiment analysis problem at the sentence level,2016,"The objective of this article is to present a hybrid approach the Sentiment Analysis problem at sentence level. This new method uses natural language processing (NLP) essential techniques, sentiment lexicon enhanced with assistance SentiWordNet, and fuzzy sets estimate semantic orientation polarity its intensity for sentences, which provides foundation computing sentiments. proposed applied three different data-sets results achieved are compared those obtained using Naive Bayes Maximum Entropy techniques. It demonstrated that presented more accurate precise than both when latter utilised in isolation. In addition, it shown datasets containing snippets, performs similarly state art"
https://openalex.org/W2883284975,https://doi.org/10.7717/peerj-cs.156,Comparison and benchmark of name-to-gender inference services,2018,"The increased interest in analyzing and explaining gender inequalities tech, media, academia highlights the need for accurate inference methods to predict a person’s from their name. Several such services exist that provide access large databases of names, often enriched with information social media profiles, culture-specific rules, insights sociolinguistics. We compare benchmark five name-to-gender by applying them classification test data set consisting 7,076 manually labeled names. compiled names are analyzed characterized according geographical cultural origin. define series performance metrics quantify various types errors, parameter tuning procedure search optimal values services’ free parameters. Finally, we perform benchmarks all under study regarding several scenarios where particular metric is be optimized."
https://openalex.org/W2888236192,https://doi.org/10.18653/v1/k18-1050,End-to-End Neural Entity Linking,2018,"Entity Linking (EL) is an essential task for semantic text understanding and information extraction. Popular methods separately address the Mention Detection (MD) Disambiguation (ED) stages of EL, without leveraging their mutual dependency. We here propose first neural end-to-end EL system that jointly discovers links entities in a document. The main idea to consider all possible spans as potential mentions learn contextual similarity scores over entity candidates are useful both MD ED decisions. Key components context-aware mention embeddings, embeddings probabilistic - map, demanding other engineered features. Empirically, we show our method significantly outperforms popular systems on Gerbil platform when enough training data available. Conversely, if testing datasets follow different annotation conventions compared set (e.g. queries/ tweets vs news documents), model coupled with traditional NER offers best or second accuracy."
https://openalex.org/W2807333695,https://doi.org/10.18653/v1/s18-1005,SemEval-2018 Task 3: Irony Detection in English Tweets,2018,"This paper presents the first shared task on irony detection: given a tweet, automatic natural language processing systems should determine whether tweet is ironic (Task A) and which type of (if any) expressed B). The tweets were collected using irony-related hashtags (i.e. #irony, #sarcasm, #not) subsequently manually annotated to minimise amount noise in corpus. Prior distributing data, that used collect removed from For both tasks, training corpus 3,834 was provided, as well test set containing 784 tweets. Our tasks received submissions 43 teams for binary classification Task A 31 multiclass B. highest scores obtained subtasks are respectively F1= 0.71 0.51 demonstrate fine-grained much more challenging than detection."
https://openalex.org/W2949176808,https://doi.org/10.1093/bioinformatics/bty449,Transfer learning for biomedical named entity recognition with neural networks,2018,"The explosive increase of biomedical literature has made information extraction an increasingly important tool for research. A fundamental task is the recognition named entities in text (BNER) such as genes/proteins, diseases and species. Recently, a domain-independent method based on deep learning statistical word embeddings, called long short-term memory network-conditional random field (LSTM-CRF), been shown to outperform state-of-the-art entity-specific BNER tools. However, this dependent gold-standard corpora (GSCs) consisting hand-labeled entities, which tend be small but highly reliable. An alternative GSCs are silver-standard (SSCs), generated by harmonizing annotations several automatic annotation systems. SSCs typically contain more noise than have advantage containing many training examples. Ideally, these could combined achieve benefits both, opportunity transfer learning. In work, we analyze what extent improves upon results BNER.We demonstrate that transferring neural network (DNN) trained large, noisy SSC smaller, reliable GSC significantly BNER. Compared baseline evaluated 23 covering four different entity classes, average reduction error approximately 11%. We found especially beneficial target datasets with number labels (approximately 6000 or less).Source code LSTM-CRF available at https://github.com/Franck-Dernoncourt/NeuroNER/ links https://github.com/BaderLab/Transfer-Learning-BNER-Bioinformatics-2018/.Supplementary data Bioinformatics online."
https://openalex.org/W2988421999,https://doi.org/10.18653/v1/d19-5801,MRQA 2019 Shared Task: Evaluating Generalization in Reading Comprehension,2019,"We present the results of Machine Reading for Question Answering (MRQA) 2019 shared task on evaluating generalization capabilities reading comprehension systems. In this task, we adapted and unified 18 distinct question answering datasets into same format. Among them, six were made available training, development, rest hidden final evaluation. Ten teams submitted systems, which explored various ideas including data sampling, multi-task learning, adversarial training ensembling. The best system achieved an average F1 score 72.5 12 held-out datasets, 10.7 absolute points higher than our initial baseline based BERT."
https://openalex.org/W3012624518,https://doi.org/10.1073/pnas.1915768117,Racial disparities in automated speech recognition,2020,"Automated speech recognition (ASR) systems, which use sophisticated machine-learning algorithms to convert spoken language text, have become increasingly widespread, powering popular virtual assistants, facilitating automated closed captioning, and enabling digital dictation platforms for health care. Over the last several years, quality of these systems has dramatically improved, due both advances in deep learning collection large-scale datasets used train systems. There is concern, however, that tools do not work equally well all subgroups population. Here, we examine ability five state-of-the-art ASR systems—developed by Amazon, Apple, Google, IBM, Microsoft—to transcribe structured interviews conducted with 42 white speakers 73 black speakers. In total, this corpus spans US cities consists 19.8 h audio matched on age gender speaker. We found exhibited substantial racial disparities, an average word error rate (WER) 0.35 compared 0.19 trace disparities underlying acoustic models as race gap was large a subset identical phrases individuals our corpus. conclude proposing strategies—such using more diverse training include African American Vernacular English—to reduce performance differences ensure technology inclusive."
https://openalex.org/W3035119815,https://doi.org/10.1109/jbhi.2020.3001216,Deep Sentiment Classification and Topic Discovery on Novel Coronavirus or COVID-19 Online Discussions: NLP Using LSTM Recurrent Neural Network Approach,2020,"Internet forums and public social media, such as online healthcare forums, provide a convenient channel for users (people/patients) concerned about health issues to discuss share information with each other. In late December 2019, an outbreak of novel coronavirus (infection from which results in the disease named COVID-19) was reported, and, due rapid spread virus other parts world, World Health Organization declared state emergency. this paper, we used automated extraction COVID-19-related discussions media natural language process (NLP) method based on topic modeling uncover various related COVID-19 opinions. Moreover, also investigate how use LSTM recurrent neural network sentiment classification comments. Our findings shed light importance using opinions suitable computational techniques understand surrounding guide decision-making. addition, experiments demonstrated that research model achieved accuracy 81.15% - higher than several well-known machine-learning algorithms COVID-19-Sentiment Classification."
https://openalex.org/W3102887392,https://doi.org/10.1109/tmm.2018.2832602,Predicting Visual Features From Text for Image and Video Caption Retrieval,2018,"This paper strives to find amidst a set of sentences the one best describing content given image or video. Different from existing works, which rely on joint subspace for their and video caption retrieval, we propose do so in visual space exclusively. Apart this conceptual novelty, contribute \emph{Word2VisualVec}, deep neural network architecture that learns predict feature representation textual input. Example captions are encoded into embedding based multi-scale sentence vectorization further transferred choice via simple multi-layer perceptron. We generalize Word2VisualVec by predicting text both 3-D convolutional features as well visual-audio representation. Experiments Flickr8k, Flickr30k, Microsoft Video Description dataset very recent NIST TrecVid challenge retrieval detail Word2VisualVec's properties, its benefit over embeddings, potential multimodal query composition state-of-the-art results."
https://openalex.org/W3124180615,https://doi.org/10.1007/s10664-017-9546-9,Sentiment Polarity Detection for Software Development,2018,"The role of sentiment analysis is increasingly emerging to study software developers' emotions by mining crowd-generated content within social engineering tools. However, off-the-shelf tools have been trained on non-technical domains and general-purpose media, thus resulting in misclassifications technical jargon problem reports. Here, we present Senti4SD, a classifier specifically support communication channels. Senti4SD validated using gold standard Stack Overflow questions, answers, comments manually annotated for polarity. It exploits suite both lexicon- keyword-based features, as well semantic features based word embedding. With respect mainstream tool, which use baseline, reduces the neutral positive posts emotionally negative. To encourage replications, release lab package including classifier, embedding space, with annotation guidelines."
https://openalex.org/W2463895987,https://doi.org/10.18653/v1/d16-1157,Charagram: Embedding Words and Sentences via Character n-grams,2016,"We present Charagram embeddings, a simple approach for learning character-based compositional models to embed textual sequences. A word or sentence is represented using character n-gram count vector, followed by single nonlinear transformation yield low-dimensional embedding. use three tasks evaluation: similarity, and part-of-speech tagging. demonstrate that embeddings outperform more complex architectures based on character-level recurrent convolutional neural networks, achieving new state-of-the-art performance several similarity tasks."
https://openalex.org/W2561299349,https://doi.org/10.1111/cogs.12461,A Neurocomputational Model of the N400 and the P600 in Language Processing,2017,"Ten years ago, researchers using event-related brain potentials (ERPs) to study language comprehension were puzzled by what looked like a Semantic Illusion: Semantically anomalous, but structurally well-formed sentences did not affect the N400 component-traditionally taken reflect semantic integration-but instead produced P600 effect, which is generally linked syntactic processing. This finding led considerable amount of debate, and number complex processing models have been proposed as an explanation. What these in common that they postulate two or more separate streams, order reconcile Illusion other semantically induced effects with traditional interpretations P600. Recently, however, multi-stream called into question, simpler single-stream model has proposed. According this alternative model, component reflects retrieval word meaning from memory, indexes integration unfolding utterance interpretation. In present paper, we provide support for ""Retrieval-Integration (RI)"" account instantiating it neurocomputational model. first successfully simulate amplitude comprehension, simulations proof concept RI patterns modulations."
https://openalex.org/W2740433069,https://doi.org/10.18653/v1/w17-4739,The University of Edinburgh's Neural MT Systems for WMT17,2017,"This paper describes the University of Edinburgh's submissions to WMT17 shared news translation and biomedical tasks. We participated in 12 directions for news, translating between English Czech, German, Latvian, Russian, Turkish Chinese. For task we submitted systems Polish Romanian. Our are neural machine trained with Nematus, an attentional encoder-decoder. follow our setup from last year build BPE-based models parallel back-translated monolingual training data. Novelties this include use deep architectures, layer normalization, more compact due weight tying improvements BPE segmentations. perform extensive ablative experiments, reporting on effectivenes different ensembling techniques."
https://openalex.org/W2798304389,https://doi.org/10.18653/v1/p18-1192,"Syntax for Semantic Role Labeling, To Be, Or Not To Be",2018,"Semantic role labeling (SRL) is dedicated to recognizing the predicate-argument structure of a sentence. Previous studies have shown syntactic information has remarkable contribution SRL performance. However, such perception was challenged by few recent neural models which give impressive performance without backbone. This paper intends quantify importance dependency in deep learning framework. We propose an enhanced argument model companying with extended korder pruning algorithm for effectively exploiting information. Our achieves state-of-the-art results on CoNLL-2008, 2009 benchmarks both English and Chinese, showing quantitative significance syntax together thorough empirical survey over existing models."
https://openalex.org/W2962911926,https://doi.org/10.18653/v1/d18-1503,The Importance of Being Recurrent for Modeling Hierarchical Structure,2018,"Recent work has shown that recurrent neural networks (RNNs) can implicitly capture and exploit hierarchical information when trained to solve common natural language processing tasks (Blevins et al., 2018) such as modeling (Linzen 2016; Gulordava machine translation (Shi 2016). In contrast, the ability model structured data with non-recurrent received little attention despite their success in many NLP (Gehring 2017; Vaswani 2017). this work, we compare two architectures—recurrent versus non-recurrent—with respect structure find recurrency is indeed important for purpose. The code used our experiments available at https://github.com/ ketranm/fan_vs_rnn"
https://openalex.org/W2963631961,https://doi.org/10.1109/taffc.2018.2874986,Survey on Emotional Body Gesture Recognition,2021,"Automatic emotion recognition has become a trending research topic in the past decade. While works based on facial expressions or speech abound, recognizing affect from body gestures remains less explored topic. We present new comprehensive survey hoping to boost field. first introduce emotional as component of what is commonly known ”body language” and comment general aspects gender differences culture dependence. then define complete framework for automatic gesture recognition. person detection static dynamic pose estimation methods both RGB 3D. recent literature related representation learning images emotionally expressive gestures. also discuss multi-modal approaches that combine face with improved pre-processing methodologies (e.g., human estimation) are nowadays mature technologies fully developed robust large scale analysis, we show quantity labelled data scarce. There no agreement clearly defined output spaces representations shallow largely naive geometrical representations."
https://openalex.org/W2963908984,https://doi.org/10.1109/icfhr.2016.0060,PHOCNet: A Deep Convolutional Neural Network for Word Spotting in Handwritten Documents,2016,"In recent years, deep convolutional neural networks have achieved state of the art performance in various computer vision tasks such as classification, detection or segmentation. Due to their outstanding performance, CNNs are more and used field document image analysis well. this work, we present a CNN architecture that is trained with recently proposed PHOC representation. We show empirically our able outperform state-of-the-art results for word spotting benchmarks while exhibiting short training test times."
https://openalex.org/W3038033387,https://doi.org/10.18653/v1/2020.acl-demos.12,Multilingual Universal Sentence Encoder for Semantic Retrieval,2020,"We introduce two pre-trained retrieval focused multilingual sentence encoding models, respectively based on the Transformer and CNN model architectures. The models embed text from 16 languages into a single semantic space using multi-task trained dual-encoder that learns tied representations translation bridge tasks (Chidambaram al., 2018). provide performance is competitive with state-of-the-art on: (SR), pair bitext (BR) question answering (ReQA). On English transfer learning tasks, our sentence-level embeddings approach, in some cases exceed, of monolingual, only, embedding models. Our are made available for download TensorFlow Hub."
https://openalex.org/W3102483398,https://doi.org/10.18653/v1/2020.emnlp-main.484,"XGLUE: A New Benchmark Datasetfor Cross-lingual Pre-training, Understanding and Generation",2020,"In this paper, we introduce XGLUE, a new benchmark dataset to train large-scale cross-lingual pre-trained models using multilingual and bilingual corpora, evaluate their performance across diverse set of tasks. Comparing GLUE (Wang et al.,2019), which is labeled in English includes natural language understanding tasks only, XGLUE has three main advantages: (1) it provides two corpora with different sizes for pre-training; (2) 11 diversified that cover both generation scenarios; (3) each task, data multiple languages. We extend recent model Unicoder (Huang al., 2019) tasks, evaluated on as strong baseline. also the base versions (12-layer) Multilingual BERT, XLM XLM-R comparison."
https://openalex.org/W1610821757,https://doi.org/10.1155/2015/918710,"GNormPlus: An Integrative Approach for Tagging Genes, Gene Families, and Protein Domains",2015,"The automatic recognition of gene names and their associated database identifiers from biomedical text has been widely studied in recent years, as these tasks play an important role many downstream text-mining applications. Despite significant previous research, only a small number tools are publicly available typically restricted to detecting mention level or document identifiers. In this work, we report GNormPlus: end-to-end open source system that handles both identifier detection. We created new corpus 694 PubMed articles support our development GNormPlus, containing manual annotations for not identifiers, but also closely related concepts useful name disambiguation, such families protein domains. GNormPlus integrates several advanced techniques, including SimConcept resolving composite names. As result, compares favorably other state-of-the-art methods when evaluated on two used public benchmarking datasets, achieving 86.7% F1-score the BioCreative II Gene Normalization task dataset 50.1% III dataset. code its annotated freely available, results applying entire accessible through web-based tool PubTator."
https://openalex.org/W2251507550,https://doi.org/10.18653/v1/d15-1242,Specializing Word Embeddings for Similarity or Relatedness,2015,"We demonstrate the advantage of specializing semantic word embeddings for either similarity or relatedness. compare two variants retrofitting and a joint-learning approach, find that all three yield specialized spaces capture human intuitions regarding relatedness better than unspecialized spaces. also show using in NLP tasks applications leads to clear improvements, document classification synonym selection, which rely on but not both."
https://openalex.org/W2252278997,https://doi.org/10.18653/v1/w15-4322,Multimedia Lab $@$ ACL WNUT NER Shared Task: Named Entity Recognition for Twitter Microposts using Distributed Word Representations,2015,"Due to the short and noisy nature of Twitter microposts, detecting named entities is often a cumbersome task. As part ACL2015 Named Entity Recognition (NER) shared task, we present semisupervised system that detects 10 types entities. To end, leverage 400 million microposts generate powerful word embeddings as input features use neural network execute classification. further boost performance, employ dropout train leaky Rectified Linear Units (ReLUs). Our achieved fourth position in final ranking, without using any kind hand-crafted such lexical or gazetteers."
https://openalex.org/W2577646479,https://doi.org/10.1136/bmjopen-2016-012012,Natural language processing to extract symptoms of severe mental illness from clinical text: the Clinical Record Interactive Search Comprehensive Data Extraction (CRIS-CODE) project,2017,"OBJECTIVES: We sought to use natural language processing develop a suite of models capture key symptoms severe mental illness (SMI) from clinical text, facilitate the secondary healthcare data in research. DESIGN: Development and validation information extraction applications for ascertaining SMI routine health records using Clinical Record Interactive Search (CRIS) resource; description their distribution corpus discharge summaries. SETTING: Electronic large provider serving geographic catchment 1.2 million residents four boroughs south London, UK. PARTICIPANTS: The derived was described 23 128 summaries 7962 patients who had received an diagnosis, 13 496 7575 non-SMI diagnosis. OUTCOME MEASURES: Fifty were identified by team psychiatrists based on salience linguistic consistency records, broadly categorised under positive, negative, disorganisation, manic catatonic subgroups. Text each symptom generated TextHunter tool CRIS database. RESULTS: extracted 46 with median F1 score 0.88. Four performed poorly excluded. From summaries, it possible extract symptomatology 87% 60% CONCLUSIONS: This work demonstrates possibility automatically extracting broad range English text Descriptive also indicated that most cut across diagnoses, rather than being restricted particular groups."
https://openalex.org/W2739992143,https://doi.org/10.1145/3077136.3080822,Neural Rating Regression with Abstractive Tips Generation for Recommendation,2017,"Recently, some E-commerce sites launch a new interaction box called Tips on their mobile apps. Users can express experience and feelings or provide suggestions using short texts typically several words one sentence. In essence, writing tips giving numerical rating are two facets of user's product assessment action, expressing the user feelings. Jointly modeling these is helpful for designing better recommendation system. While existing models integrate text information such as item specifications reviews into latent factors improving prediction, no works consider quality. We propose deep learning based framework named NRT which simultaneously predict precise ratings generate abstractive with good linguistic quality simulating For generation, gated recurrent neural networks employed to ""translate"" representations concise Extensive experiments benchmark datasets from different domains show that achieves significant improvements over state-of-the-art methods. Moreover, generated vividly"
https://openalex.org/W2886799640,https://doi.org/10.1007/978-3-030-01225-0_47,Instance-Level Human Parsing via Part Grouping Network,2018,"Instance-level human parsing towards real-world analysis scenarios is still under-explored due to the absence of sufficient data resources and technical difficulty in multiple instances a single pass. Several related works all follow ""parsing-by-detection"" pipeline that heavily relies on separately trained detection models localize then performs for each instance sequentially. Nonetheless, two discrepant optimization targets lead suboptimal representation learning error accumulation final results. In this work, we make first attempt explore detection-free Part Grouping Network (PGN) efficiently people an image Our PGN reformulates instance-level as twinned sub-tasks can be jointly learned mutually refined via unified network: 1) semantic part segmentation assigning pixel (e.g., face, arms); 2) instance-aware edge group parts into distinct person instances. Thus shared intermediate would endowed with capabilities both characterizing fine-grained inferring belongings part. Finally, simple partition process employed get results during inference. We conducted experiments PASCAL-Person-Part dataset our outperforms state-of-the-art methods. Furthermore, show its superiority newly collected multi-person (CIHP) including 38,280 diverse images, which largest so far facilitate more advanced analysis. The CIHP benchmark source code are available at http://sysu-hcp.net/lip/."
https://openalex.org/W2888456631,https://doi.org/10.18653/v1/d18-1039,Contextual Parameter Generation for Universal Neural Machine Translation,2018,"We propose a simple modification to existing neural machine translation (NMT) models that enables using single universal model translate between multiple languages while allowing for language specific parameterization, and can also be used domain adaptation. Our approach requires no changes the architecture of standard NMT system, but instead introduces new component, contextual parameter generator (CPG), generates parameters system (e.g., weights in network). This accepts source target embeddings as input, encoder decoder, respectively. The rest remains unchanged is shared across all languages. show how this use monolingual data training perform zero-shot translation. further it able surpass state-of-the-art performance both IWSLT-15 IWSLT-17 datasets learned are uncover interesting relationships"
https://openalex.org/W2897630418,https://doi.org/10.3758/s13428-018-1115-7,"The “Small World of Words” English word association norms for over 12,000 cue words",2019,"Word associations have been used widely in psychology, but the validity of their application strongly depends on number cues included study and extent to which they probe all known by an individual. In this work, we address both issues introducing a new English word association dataset. We describe collection for over 12,000 cue words, currently largest such English-language resource world. Our procedure allowed subjects provide multiple responses each cue, permits us measure weak associations. evaluate utility dataset several different contexts, including lexical decision semantic categorization. also show that measures based mechanism spreading activation derived from are highly predictive direct judgments similarity. Finally, comparison with existing sets further highlights systematic improvements provided through these norms."
https://openalex.org/W2932414082,https://doi.org/10.1109/cvpr.2019.00200,All About Structure: Adapting Structural Information Across Domains for Boosting Semantic Segmentation,2019,"In this paper we tackle the problem of unsupervised domain adaptation for task semantic segmentation, where attempt to transfer knowledge learned upon synthetic datasets with ground-truth labels real-world images without any annotation. With hypothesis that structural content is most informative and decisive factor segmentation can be readily shared across domains, propose a Domain Invariant Structure Extraction (DISE) framework disentangle into domain-invariant structure domain-specific texture representations, which further realize image-translation domains enable label improve performance. Extensive experiments verify effectiveness our proposed DISE model demonstrate its superiority over several state-of-the-art approaches."
https://openalex.org/W2963842551,https://doi.org/10.1162/tacl_a_00029,Learning to Remember Translation History with a Continuous Cache,2018,"Existing neural machine translation (NMT) models generally translate sentences in isolation, missing the opportunity to take advantage of document-level information. In this work, we propose augment NMT with a very light-weight cache-like memory network, which stores recent hidden representations as history. The probability distribution over generated words is updated online depending on history retrieved from memory, endowing capability dynamically adapt time. Experiments multiple domains different topics and styles show effectiveness proposed approach negligible impact computational cost."
https://openalex.org/W2964053384,https://doi.org/10.18653/v1/d17-1091,Learning to Paraphrase for Question Answering,2017,"Question answering (QA) systems are sensitive to the many different ways natural language expresses same information need. In this paper we turn paraphrases as a means of capturing knowledge and present general framework which learns felicitous for various QA tasks. Our method is trained end-to-end using question-answer pairs supervision signal. A question its serve input neural scoring model assigns higher weights linguistic expressions most likely yield correct answers. We evaluate our approach on over Freebase answer sentence selection. Experimental results three datasets show that consistently improves performance, achieving competitive despite use simple models."
https://openalex.org/W3098654368,https://doi.org/10.18653/v1/p16-1068,Automatic Text Scoring Using Neural Networks,2016,"Automated Text Scoring (ATS) provides a cost-effective and consistent alternative to human marking. However, in order achieve good performance, the predictive features of system need be manually engineered by experts. We introduce model that forms word representations learning extent which specific words contribute text's score. Using Long-Short Term Memory networks represent meaning texts, we demonstrate fully automated framework is able excellent results over similar approaches. In an attempt make our more interpretable, inspired recent advances visualizing neural networks, novel method for identifying regions text has found discriminative."
https://openalex.org/W3156636935,https://doi.org/10.18653/v1/2021.emnlp-main.552,SimCSE: Simple Contrastive Learning of Sentence Embeddings,2021,"This paper presents SimCSE, a simple contrastive learning framework that greatly advances state-of-the-art sentence embeddings. We first describe an unsupervised approach, which takes input and predicts itself in objective, with only standard dropout used as noise. method works surprisingly well, performing on par previous supervised counterparts. find acts minimal data augmentation, removing it leads to representation collapse. Then, we propose incorporates annotated pairs from natural language inference datasets into our by using ""entailment"" positives ""contradiction"" hard negatives. evaluate SimCSE semantic textual similarity (STS) tasks, models BERT base achieve average of 76.3% 81.6% Spearman's correlation respectively, 4.2% 2.2% improvement compared the best results. also show -- both theoretically empirically objective regularizes pre-trained embeddings' anisotropic space be more uniform, better aligns positive when signals are available."
https://openalex.org/W2168681026,https://doi.org/10.18653/v1/s15-2079,UNITN: Training Deep Convolutional Neural Network for Twitter Sentiment Classification,2015,"This paper describes our deep learning system for sentiment analysis of tweets. The main contribution this work is a process to initialize the parameter weights convolutional neural network, which crucial train an accurate model while avoiding need inject any additional features. Briefly, we use unsupervised language word embeddings that are further tuned by on distant supervised corpus. At final stage, pre-trained parameters network used then trained training data from Semeval-2015. According results official test sets, ranks 1st in phrase-level subtask A (among 11 teams) and 2nd messagelevel B 40 teams). Interestingly, computing average rank over all six sets (official five progress sets) puts both subtasks B."
https://openalex.org/W2590462354,https://doi.org/10.1093/jamia/ocw180,Deep learning for pharmacovigilance: recurrent neural network architectures for labeling adverse drug reactions in Twitter posts,2017,"Abstract Objective Social media is an important pharmacovigilance data source for adverse drug reaction (ADR) identification. Human review of social infeasible due to quantity, thus natural language processing techniques are necessary. includes informal vocabulary and irregular grammar, which challenge methods. Our objective develop a scalable, deep-learning approach that exceeds state-of-the-art ADR detection performance in media. Materials Methods We developed recurrent neural network (RNN) model labels words input sequence with membership tags. The only features word-embedding vectors, can be formed through task-independent pretraining or during training. Results best-performing RNN used pretrained word embeddings created from large, non–domain-specific Twitter dataset. It achieved approximate match F-measure 0.755 identification on the dataset, compared 0.631 baseline lexicon system 0.65 conditional random field model. Feature analysis indicated semantic information boosted sensitivity and, combined contextual awareness captured RNN, precision. Discussion required no task-specific feature engineering, suggesting generalizability additional sequence-labeling tasks. Learning curve showed our reached optimal fewer training examples than other models. Conclusions significantly improved by using contextually aware unlabeled datasets. reduces manual data-labeling requirements scalable large"
https://openalex.org/W2952179106,https://doi.org/10.18653/v1/p19-1074,DocRED: A Large-Scale Document-Level Relation Extraction Dataset,2019,"Multiple entities in a document generally exhibit complex inter-sentence relations, and cannot be well handled by existing relation extraction (RE) methods that typically focus on extracting intra-sentence relations for single entity pairs. In order to accelerate the research document-level RE, we introduce DocRED, new dataset constructed from Wikipedia Wikidata with three features: (1) DocRED annotates both named is largest human-annotated RE plain text; (2) requires reading multiple sentences extract infer their synthesizing all information of document; (3) along data, also offer large-scale distantly supervised which enables adopted weakly scenarios. verify challenges implement recent state-of-the-art conduct thorough evaluation these DocRED. Empirical results show challenging methods, indicates remains an open problem further efforts. Based detailed analysis experiments, discuss promising directions future research."
https://openalex.org/W2963188990,https://doi.org/10.18653/v1/p18-1104,MojiTalk: Generating Emotional Responses at Scale,2018,"Generating emotional language is a key step towards building empathetic natural processing agents. However, major challenge for this line of research the lack large-scale labeled training data, and previous studies are limited to only small sets human annotated sentiment labels. Additionally, explicitly controlling emotion generated text also difficult. In paper, we take more radical approach: exploit idea leveraging Twitter data that naturally with emojis. We collect large corpus conversations include emojis in response assume convey underlying emotions sentence. investigate several conditional variational autoencoders on these conversations, which allow us use control text. Experimentally, show our quantitative qualitative analyses proposed models can successfully generate high-quality abstractive conversation responses accordance designated emotions."
https://openalex.org/W2963607157,https://doi.org/10.18653/v1/d18-1208,Content Selection in Deep Learning Models of Summarization,2018,"We carry out experiments with deep learning models of summarization across the domains news, personal stories, meetings, and medical articles in order to understand how content selection is performed. find that many sophisticated features state art extractive summarizers do not improve performance over simpler models. These results suggest it easier create a summarizer for new domain than previous work suggests bring into question benefit those have massive datasets (i.e., news). At same time, they important questions research summarization; namely, forms sentence representations or external knowledge sources are needed better suited sumarization task."
https://openalex.org/W2972498556,https://doi.org/10.18653/v1/w19-4808,Analyzing the Structure of Attention in a Transformer Language Model,2019,"The Transformer is a fully attention-based alternative to recurrent networks that has achieved state-of-the-art results across range of NLP tasks. In this paper, we analyze the structure attention in language model, GPT-2 small pretrained model. We visualize for individual instances and interaction between syntax over large corpus. find targets different parts speech at layer depths within aligns with dependency relations most strongly middle layers. also deepest layers model capture distant relationships. Finally, extract exemplar sentences reveal highly specific patterns targeted by particular heads."
https://openalex.org/W3101767658,https://doi.org/10.1145/3159652.3159703,Dynamic Word Embeddings for Evolving Semantic Discovery,2018,"Word evolution refers to the changing meanings and associations of words throughout time, as a byproduct human language evolution. By studying word evolution, we can infer social trends constructs over different periods history. However, traditional techniques such representation learning do not adequately capture evolving structure vocabulary. In this paper, develop dynamic statistical model learn time-aware vector representation. We propose that simultaneously learns embeddings solves resulting ""alignment problem"". This is trained on crawled NYTimes dataset. Additionally, multiple intuitive evaluation strategies temporal embeddings. Our qualitative quantitative tests indicate our method only reliably captures but also consistently outperforms state-of-the-art embedding approaches both semantic accuracy alignment quality."
https://openalex.org/W2251066368,https://doi.org/10.18653/v1/d15-1243,Evaluation of Word Vector Representations by Subspace Alignment,2015,"Unsupervisedly learned word vectors have proven to provide exceptionally effective features in many NLP tasks. Most common intrinsic evaluations of vector quality measure correlation with similarity judgments. However, these often correlate poorly how well the representations perform as downstream evaluation We present QVEC—a computationally inexpensive embeddings based on alignment a matrix extracted from manually crafted lexical resources—that obtains strong performance battery semantic tasks.1"
https://openalex.org/W2304113845,https://doi.org/10.48550/arxiv.1603.06393,Incorporating Copying Mechanism in Sequence-to-Sequence Learning,2016,"We address an important problem in sequence-to-sequence (Seq2Seq) learning referred to as copying, which certain segments the input sequence are selectively replicated output sequence. A similar phenomenon is observable human language communication. For example, humans tend repeat entity names or even long phrases conversation. The challenge with regard copying Seq2Seq that new machinery needed decide when perform operation. In this paper, we incorporate into neural network-based and propose a model called CopyNet encoder-decoder structure. can nicely integrate regular way of word generation decoder mechanism choose sub-sequences put them at proper places Our empirical study on both synthetic data sets real world demonstrates efficacy CopyNet. outperform RNN-based remarkable margins text summarization tasks."
https://openalex.org/W2768567289,https://doi.org/10.1148/radiol.2017171115,Deep Learning to Classify Radiology Free-Text Reports,2017,"Purpose To evaluate the performance of a deep learning convolutional neural network (CNN) model compared with traditional natural language processing (NLP) in extracting pulmonary embolism (PE) findings from thoracic computed tomography (CT) reports two institutions. Materials and Methods Contrast material-enhanced CT examinations chest performed between January 1, 1998, 2016, were selected. Annotations by human radiologists made for three categories: presence, chronicity, location PE. Classification CNN an unsupervised algorithm obtaining vector representations words was open-source application PeFinder. Sensitivity, specificity, accuracy, F1 scores both PeFinder internal external validation sets determined. Results The demonstrated accuracy 99% area under curve value 0.97. For report data, had statistically significant larger score (0.938) than did (0.867) when classifying as either PE positive or negative, but no difference sensitivity, found. statistical Conclusion A can classify radiology free-text equivalent to beyond that existing NLP model. © RSNA, 2017 Online supplemental material is available this article."
https://openalex.org/W2799044502,https://doi.org/10.18653/v1/p18-1088,Target-Sensitive Memory Networks for Aspect Sentiment Classification,2018,"Aspect sentiment classification (ASC) is a fundamental task in analysis. Given an aspect/target and sentence, the classifies polarity expressed on target sentence. Memory networks (MNs) have been used for this recently achieved state-of-the-art results. In MNs, attention mechanism plays crucial role detecting context given target. However, we found important problem with current MNs performing ASC task. Simply improving will not solve it. The referred to as target-sensitive sentiment, which means that of (detected) dependent it cannot be inferred from alone. To tackle problem, propose memory (TMNs). Several alternative techniques are designed implementation TMNs their effectiveness experimentally evaluated."
https://openalex.org/W2806532810,https://doi.org/10.1162/coli_a_00322,A Structured Review of the Validity of BLEU,2018,"The BLEU metric has been widely used in NLP for over 15 years to evaluate systems, especially machine translation and natural language generation. I present a structured review of the evidence on whether is valid evaluation technique—in other words, scores correlate with real-world utility user-satisfaction systems; this covers 284 correlations reported 34 papers. Overall, supports using diagnostic MT systems (which what it was originally proposed for), but does not support outside MT, individual texts, or scientific hypothesis testing."
https://openalex.org/W2891359673,https://doi.org/10.18653/v1/d18-1280,ICON: Interactive Conversational Memory Network for Multimodal Emotion Detection,2018,"Emotion recognition in conversations is crucial for building empathetic machines. Present works this domain do not explicitly consider the inter-personal influences that thrive emotional dynamics of dialogues. To end, we propose Interactive COnversational memory Network (ICON), a multimodal emotion detection framework extracts features from conversational videos and hierarchically models self- inter-speaker into global memories. Such memories generate contextual summaries which aid predicting orientation utterance-videos. Our model outperforms state-of-the-art networks on multiple classification regression tasks two benchmark datasets."
https://openalex.org/W2922158773,https://doi.org/10.18653/v1/w18-6312,Attaining the Unattainable? Reassessing Claims of Human Parity in Neural Machine Translation,2018,"We reassess a recent study (Hassan et al., 2018) that claimed machine translation (MT) has reached human parity for the of news from Chinese into English, using pairwise ranking and considering three variables were not taken account in previous study: language which source side test set was originally written, proficiency evaluators, provision inter-sentential context. If we consider only original text (i.e. translated another language, or translationese), then find evidence showing been achieved. compare judgments professional translators against those non-experts discover experts result higher inter-annotator agreement better discrimination between translations. In addition, analyse translations identify important issues. Finally, based on these findings, provide recommendations future evaluations MT."
https://openalex.org/W2950752421,https://doi.org/10.48550/arxiv.1506.01057,A Hierarchical Neural Autoencoder for Paragraphs and Documents,2015,"Natural language generation of coherent long texts like paragraphs or longer documents is a challenging problem for recurrent networks models. In this paper, we explore an important step toward task: training LSTM (Long-short term memory) auto-encoder to preserve and reconstruct multi-sentence paragraphs. We introduce model that hierarchically builds embedding paragraph from embeddings sentences words, then decodes the original paragraph. evaluate reconstructed using standard metrics ROUGE Entity Grid, showing neural models are able encode in way syntactic, semantic, discourse coherence. While only first generating text units models, our work has potential significantly impact natural summarization\footnote{Code three described paper can be found at www.stanford.edu/~jiweil/ ."
https://openalex.org/W2964263523,https://doi.org/10.24963/ijcai.2018/556,Co-training Embeddings of Knowledge Graphs and Entity Descriptions for Cross-lingual Entity Alignment,2018,"Multilingual knowledge graph (KG) embeddings provide latent semantic representations of entities and structured with cross-lingual inferences, which benefit various knowledge-driven NLP tasks. However, precisely learning such inferences is usually hindered by the low coverage entity alignment in many KGs. Since multilingual KGs also literal descriptions entities, this paper, we introduce an embedding-based approach leverages a weakly aligned KG for semi-supervised using descriptions. Our performs co-training two embedding models, i.e. model description model. The models are trained on large Wikipedia-based trilingual dataset where most unknown to training. Experimental results show that performance proposed task improves at each iteration co-training, eventually reaches stage it significantly surpasses previous approaches. We our has promising abilities zero-shot alignment, completion."
https://openalex.org/W3015453090,https://doi.org/10.18653/v1/2020.acl-main.447,S2ORC: The Semantic Scholar Open Research Corpus,2020,"We introduce S2ORC, a large corpus of 81.1M English-language academic papers spanning many disciplines. The consists rich metadata, paper abstracts, resolved bibliographic references, as well structured full text for 8.1M open access papers. Full is annotated with automatically-detected inline mentions citations, figures, and tables, each linked to their corresponding objects. In we aggregate from hundreds publishers digital archives into unified source, create the largest publicly-available collection machine-readable date. hope this resource will facilitate research development tools tasks mining over text."
https://openalex.org/W3035050380,https://doi.org/10.18653/v1/2020.acl-main.552,Extractive Summarization as Text Matching,2020,"This paper creates a paradigm shift with regard to the way we build neural extractive summarization systems. Instead of following commonly used framework extracting sentences individually and modeling relationship between sentences, formulate task as semantic text matching problem, in which source document candidate summaries will be (extracted from original text) matched space. Notably, this is well-grounded our comprehensive analysis inherent gap sentence-level summary-level extractors based on property dataset. Besides, even instantiating simple form model, have driven state-of-the-art result CNN/DailyMail new level (44.41 ROUGE-1). Experiments other five datasets also show effectiveness framework. We believe power matching-based has not been fully exploited. To encourage more instantiations future, released codes, processed dataset, well generated https://github.com/maszhongming/MatchSum."
https://openalex.org/W2190506272,https://doi.org/10.1109/icassp.2016.7472619,Deep convolutional acoustic word embeddings using word-pair side information,2016,"Recent studies have been revisiting whole words as the basic modelling unit in speech recognition and query applications, instead of phonetic units. Such whole-word segmental systems rely on a function that maps variable-length segment to vector fixed-dimensional space; resulting acoustic word embeddings need allow for accurate discrimination between different types, directly embedding space. We compare several old new approaches task. Our best approach uses side information form known pairs train Siamese convolutional neural network (CNN): pair tied networks take two segments input produce their embeddings, trained with hinge loss separates same-word different-word by some margin. A classifier CNN performs similarly, but requires much stronger supervision. Both types CNNs yield large improvements over previously published results"
https://openalex.org/W2302963717,https://doi.org/10.1162/tacl_a_00088,Transforming Dependency Structures to Logical Forms for Semantic Parsing,2016,"The strongly typed syntax of grammar formalisms such as CCG, TAG, LFG and HPSG offers a synchronous framework for deriving syntactic structures semantic logical forms. In contrast—partly due to the lack strong type system—dependency are easy annotate have become widely used form analysis many languages. However, system makes formal mechanism forms from dependency challenging. We address this by introducing robust based on lambda calculus neo-Davidsonian trees. These then parsing natural language Freebase. Experiments Free917 Web-Questions datasets show that our representation is superior original trees it outperforms CCG-based task. Compared prior work, we obtain strongest result date competitive results WebQuestions."
https://openalex.org/W2415204069,https://doi.org/10.48550/arxiv.1605.09090,"Learning Natural Language Inference using Bidirectional LSTM model and
  Inner-Attention",2016,"In this paper, we proposed a sentence encoding-based model for recognizing text entailment. our approach, the encoding of is two-stage process. Firstly, average pooling was used over word-level bidirectional LSTM (biLSTM) to generate first-stage representation. Secondly, attention mechanism employed replace on same better representations. Instead using target attend words in source sentence, utilized sentence's representation appeared itself, which called ""Inner-Attention"" paper . Experiments conducted Stanford Natural Language Inference (SNLI) Corpus has proved effectiveness mechanism. With less number parameters, outperformed existing best approach by large margin."
https://openalex.org/W2582664174,https://doi.org/10.1017/s1351324916000383,Natural language processing in mental health applications using non-clinical texts,2017,"Abstract Natural language processing (NLP) techniques can be used to make inferences about peoples’ mental states from what they write on Facebook, Twitter and other social media. These then create online pathways direct people health information assistance also generate personalized interventions. Regrettably, the computational methods collect, process utilize writing data, as well evaluations of these techniques, are still dispersed in literature. This paper provides a taxonomy data sources that have been for support intervention. Specifically, we review how media detect emotions identify who may need psychological assistance; labeling diagnosis; finally, discuss ways personalize The overarching aim this scoping is highlight areas research where NLP has applied literature help develop common draws together fields health, human-computer interaction NLP."
https://openalex.org/W2963541420,https://doi.org/10.1609/aaai.v32i1.11925,Table-to-Text Generation by Structure-Aware Seq2seq Learning,2018,"Table-to-text generation aims to generate a description for factual table which can be viewed as set of field-value records. To encode both the content and structure table, we propose novel structure-aware seq2seq architecture consists field-gating encoder generator with dual attention. In encoding phase, update cell memory LSTM unit by field gate its corresponding value in order incorporate information into representation. decoding attention mechanism contains word level is proposed model semantic relevance between generated table. We conduct experiments on \texttt{WIKIBIO} dataset over 700k biographies infoboxes from Wikipedia. The visualizations case studies show that our capable generating coherent informative descriptions based comprehensive understanding Automatic evaluations also outperforms baselines great margin. Code this work available https://github.com/tyliupku/wiki2bio."
https://openalex.org/W2963628345,https://doi.org/10.18653/v1/p16-1039,Neural Word Segmentation Learning for Chinese,2016,"Most previous approaches to Chinese word segmentation formalize this problem as a character-based sequence labeling task where only contextual information within fixed sized local windows and simple interactions between adjacent tags can be captured. In paper, we propose novel neural framework which thoroughly eliminates context utilize complete history. Our model employs gated combination network over characters produce distributed representations of candidates, are then given long short-term memory (LSTM) language scoring model. Experiments on the benchmark datasets show that without help feature engineering most existing approaches, our models achieve competitive or better performances with state-of-the-art methods."
https://openalex.org/W3034758614,https://doi.org/10.1109/cvpr42600.2020.01075,ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks,2020,"We present ALFRED (Action Learning From Realistic Environments and Directives), a benchmark for learning mapping from natural language instructions egocentric vision to sequences of actions household tasks. includes long, compositional tasks with non-reversible state changes shrink the gap between research benchmarks real-world applications. consists expert demonstrations in interactive visual environments 25k directives. These directives contain both high-level goals like “Rinse off mug place it coffee maker.” low-level “Walk maker on right.” are more complex terms sequence length, action space, than existing vision- and-language task datasets. show that baseline model based recent embodied vision-and-language performs poorly ALFRED, suggesting there is significant room developing innovative grounded understanding models this benchmark."
https://openalex.org/W3094221957,https://doi.org/10.2196/21978,Public Perception of the COVID-19 Pandemic on Twitter: Sentiment Analysis and Topic Modeling Study,2020,"Background COVID-19 is a scientifically and medically novel disease that not fully understood because it has yet to be consistently deeply studied. Among the gaps in research on outbreak, there lack of sufficient infoveillance data. Objective The aim this study was increase understanding public awareness pandemic trends uncover meaningful themes concern posted by Twitter users English language during pandemic. Methods Data mining conducted collect total 107,990 tweets related between December 13 March 9, 2020. analyses included frequency keywords, sentiment analysis, topic modeling identify explore discussion topics over time. A natural processing approach latent Dirichlet allocation algorithm were used most common tweet as well categorize clusters based keyword analysis. Results results indicate three main aspects regarding First, trend spread symptoms can divided into stages. Second, analysis showed people have negative outlook toward COVID-19. Third, modeling, relating outbreak categories: emergency, how control COVID-19, reports Conclusions Sentiment produce useful information about social media alternative perspectives investigate crisis, which created considerable awareness. This shows good communication channel for both These findings help health departments communicate alleviate specific concerns disease."
https://openalex.org/W3098267758,https://doi.org/10.18653/v1/2020.emnlp-main.346,AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts,2020,"The remarkable success of pretrained language models has motivated the study what kinds knowledge these learn during pretraining. Reformulating tasks as fill-in-the-blanks problems (e.g., cloze tests) is a natural approach for gauging such knowledge, however, its usage limited by manual effort and guesswork required to write suitable prompts. To address this, we develop AutoPrompt, an automated method create prompts diverse set tasks, based on gradient-guided search. Using show that masked (MLMs) have inherent capability perform sentiment analysis inference without additional parameters or finetuning, sometimes achieving performance par with recent state-of-the-art supervised models. We also our elicit more accurate factual from MLMs than manually created LAMA benchmark, can be used relation extractors effectively extraction These results demonstrate automatically generated are viable parameter-free alternative existing probing methods, LMs become sophisticated capable, potentially replacement finetuning."
https://openalex.org/W1146351399,https://doi.org/10.1080/23273798.2015.1072223,Is prediction necessary to understand language? Probably not,2016,"ABSTRACTSome recent theoretical accounts in the cognitive sciences suggest that prediction is necessary to understand language. Here we evaluate this proposal. We consider arguments provides a unified principle of human mind and it pervades cortical function. discuss whether evidence abilities detect statistical regularities necessarily for predictive processing suggestions language learning. point out not all users appear predict suboptimal input makes often very challenging. Prediction, moreover, strongly context-dependent impeded by resource limitations. also argue may be problematic most experimental comes from “prediction-encouraging” set-ups. conclude languages can learned understood absence prediction. Claims i..."
https://openalex.org/W2151703435,https://doi.org/10.1007/s10115-015-0882-z,Word network topic model: a simple but general solution for short and imbalanced texts,2016,"The short text has been the prevalent format for information of Internet in recent decades, especially with development online social media, whose millions users generate a vast number messages everyday. Although sophisticated signals delivered by make it promising source topic modeling, its extreme sparsity and imbalance brings unprecedented challenges to conventional models like LDA variants. Aiming at presenting simple but general solution modeling texts, we present word co-occurrence network based model named WNTM tackle simultaneously. Different from previous approaches, distribution over topics each instead learning document, which successfully enhance semantic density data space without importing too much time or complexity. Meanwhile, rich contextual preserved word-word also guarantees sensitivity identifying rare convincing quality. Furthermore, employing same Gibbs sampling makes easily be extended various application scenarios. Extensive validations on both normal texts testify outperformance as compared baseline methods. And finally demonstrate potential precisely discovering newly emerging unexpected events Weibo pretty early stages."
https://openalex.org/W2190675355,https://doi.org/10.1016/j.chb.2015.11.040,Beyond positive or negative: Qualitative sentiment analysis of social media reactions to unexpected stressful events,2016,"Sentiment analysis techniques are increasingly used to grasp reactions from social media users unexpected and potentially stressful events. This paper argues that, alongside assessments of the affective valence content as negative or positive, there is a need for deeper understanding context in which expressed specific functions that users' emotional states may reflect. To demonstrate this, we present qualitative expressions on Twitter collected Germany during 2011 EHEC food contamination incident based coding scheme developed Skinner et?al.'s (2003) coping classification framework. Affective were found be diverse not only terms but also adaptive they served: beyond positive tone, some people perceived outbreak threat while others challenge cope with. We discuss how this sentiment can allow better way overall situation - resources individuals experience having with emerging demands. Qualitative indicates underlying emotions.We analysed Germany.Expressions show events, negative.The helps diagnose if challenge.The useful effective crisis communication crises."
https://openalex.org/W2238966896,https://doi.org/10.1148/rg.2016150080,Natural Language Processing Technologies in Radiology Research and Clinical Applications,2016,"The migration of imaging reports to electronic medical record systems holds great potential in terms advancing radiology research and practice by leveraging the large volume data continuously being updated, integrated, shared. However, there are significant challenges as well, largely due heterogeneity how these formatted. Indeed, although is movement toward structured reporting (ie, hierarchically itemized with use standardized terminology), majority remain unstructured free-form language. To effectively ""mine"" datasets for hypothesis testing, a robust strategy extracting necessary information needed. Manual extraction time-consuming often unmanageable task. ""Intelligent"" search engines that instead rely on natural language processing (NLP), computer-based approach analyzing text or speech, can be used automate this mining overall goal NLP translate human into format fixed collection elements), each set choices its value, easily manipulated computer programs (among other things) order subcategories query presence absence finding. authors review fundamentals describe various techniques constitute radiology, along some key applications."
https://openalex.org/W2251785914,https://doi.org/10.18653/v1/d15-1284,Humor Recognition and Humor Anchor Extraction,2015,"Humor is an essential component in personal communication. How to create computational models discover the structures behind humor, recognize humor and even extract anchors remains a challenge. In this work, we first identify several semantic design sets of features for each structure, next employ approach humor. Furthermore, develop simple effective method that enable sentence. Experiments conducted on two datasets demonstrate our recognizer automatically distinguishing between humorous non-humorous texts extracted correlate quite well with human annotations."
https://openalex.org/W2587277634,https://doi.org/10.5244/c.30.136,Deep Sign: Hybrid CNN-HMM for Continuous Sign Language Recognition,2016,"This paper introduces the end-to-end embedding of a CNN into HMM, while interpreting outputs in Bayesian fashion. The hybrid CNN-HMM combines strong discriminative abilities CNNs with sequence modelling capabilities HMMs. Most current approaches field gesture and sign language recognition disregard necessity dealing data both for training evaluation. With our presented we are able to improve over state-of-the-art on three challenging benchmark continuous tasks by between 15% 38% relative up 13.3% absolute."
https://openalex.org/W2664496537,https://doi.org/10.1162/coli_a_00302,Multiword Expression Processing: A Survey,2017,"Multiword expressions (MWEs) are a class of linguistic forms spanning conventional word boundaries that both idiosyncratic and pervasive across different languages. The structure processing depends on the clear distinction between words phrases has to be re-thought accommodate MWEs. issue MWE handling is crucial for NLP applications, where it raises number challenges. emergence solutions in absence guiding principles motivates this survey, whose aim not only provide focused review processing, but also clarify nature interactions downstream applications. We propose conceptual framework within which challenges research contributions can positioned. It offers shared understanding what meant by “MWE processing,” distinguishing subtasks discovery identification. elucidates two use cases: Parsing machine translation. Many approaches literature differentiated according how timed with respect underlying cases. discuss such orchestration choices affect scope MWE-aware systems. For each cases, we conclude open issues perspectives."
https://openalex.org/W2737990573,https://doi.org/10.1016/j.asej.2017.04.007,Sentiment analysis in Arabic: A review of the literature,2017,"Abstract Within the last couple of years, Sentiment Analysis in Arabic has gained a considerable interest from research community. In this respect, objective paper is to provide review major works that have dealt with area language. A thorough investigation available literature revealed were mainly concentrated on dealing specific tasks. To end, they used three different approaches, namely supervised, unsupervised and hybrid. The results these studies achieved are interesting but divergent. This divergence relatively due type approach opted for, task being analysed as well specificities intricacies variety understudy."
https://openalex.org/W2741040846,https://doi.org/10.18653/v1/p17-1106,Visualizing and Understanding Neural Machine Translation,2017,"While neural machine translation (NMT) has made remarkable progress in recent years, it is hard to interpret its internal workings due the continuous representations and non-linearity of networks. In this work, we propose use layer-wise relevance propagation (LRP) compute contribution each contextual word arbitrary hidden states attention-based encoder-decoder framework. We show that visualization with LRP helps NMT analyze errors."
https://openalex.org/W2898856000,https://doi.org/10.18653/v1/n19-1380,Cross-lingual Transfer Learning for Multilingual Task Oriented Dialog,2019,"One of the first steps in utterance interpretation pipeline many task-oriented conversational AI systems is to identify user intents and corresponding slots. Since data collection for machine learning models this task time-consuming, it desirable make use existing a high-resource language train low-resource languages. However, development such has largely been hindered by lack multilingual training data. In paper, we present new set 57k annotated utterances English (43k), Spanish (8.6k) Thai (5k) across domains weather, alarm, reminder. We evaluate three different cross-lingual transfer methods: (1) translating data, (2) using pre-trained embeddings, (3) novel method translation encoder as contextual word representations. find that given several hundred examples target language, latter two methods outperform Further, very settings, representations give better results than static embeddings. also compare monolingual resources form ELMo just small amounts outperforms all methods, which highlights need more sophisticated methods."
https://openalex.org/W2963119602,https://doi.org/10.18653/v1/s17-2094,BB_twtr at SemEval-2017 Task 4: Twitter Sentiment Analysis with CNNs and LSTMs,2017,In this paper we describe our attempt at producing a state-of-the-art Twitter sentiment classifier using Convolutional Neural Networks (CNNs) and Long Short Term Memory (LSTMs) networks. Our system leverages large amount of unlabeled data to pre-train word embeddings. We then use subset the fine tune embeddings distant supervision. The final CNNs LSTMs are trained on SemEval-2017 dataset where fined tuned again. To boost performances ensemble several together. approach achieved first rank all five English subtasks amongst 40 teams.
https://openalex.org/W2963327605,https://doi.org/10.1109/cvpr.2018.00163,Edit Probability for Scene Text Recognition,2018,"We consider the scene text recognition problem under attention-based encoder-decoder framework, which is state of art. The existing methods usually employ a frame-wise maximal likelihood loss to optimize models. When we train model, misalignment between ground truth strings and attention's output sequences probability distribution, caused by missing or superfluous characters, will confuse mislead training process, consequently make costly degrade accuracy. To handle this problem, propose novel method called edit (EP) for recognition. EP tries effectively estimate generating string from sequence distribution conditioned on input image, while considering possible occurrences missing/superfluous characters. advantage lies in that process can focus missing, unrecognized thus impact be alleviated even overcome. conduct extensive experiments standard benchmarks, including IIIT-5K, Street View Text ICDAR datasets. Experimental results show substantially boost performance."
https://openalex.org/W2963525668,https://doi.org/10.5244/c.31.114,"Exploring the structure of a real-time, arbitrary neural artistic stylization network",2017,"In this paper, we present a method which combines the flexibility of neural algorithm artistic style with speed fast transfer networks to allow real-time stylization using any content/style image pair. We build upon recent work leveraging conditional instance normalization for multi-style by learning predict parameters directly from image. The model is successfully trained on corpus roughly 80,000 paintings and able generalize previously unobserved. demonstrate that learned embedding space smooth contains rich structure organizes semantic information associated in an entirely unsupervised manner."
https://openalex.org/W2964345285,https://doi.org/10.18653/v1/p19-1426,Bridging the Gap between Training and Inference for Neural Machine Translation,2019,"Neural Machine Translation (NMT) generates target words sequentially in the way of predicting next word conditioned on context words. At training time, it predicts with ground truth as while at inference has to generate entire sequence from scratch. This discrepancy fed leads error accumulation among way. Furthermore, word-level requires strict matching between generated and which overcorrection over different but reasonable translations. In this paper, we address these issues by sampling not only also predicted model during training, where is selected a sentence-level optimum. Experiment results Chinese->English WMT'14 English->German translation tasks demonstrate that our approach can achieve significant improvements multiple datasets."
https://openalex.org/W3172642864,https://doi.org/10.18653/v1/2021.naacl-main.185,It’s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners,2021,"When scaled to hundreds of billions parameters, pretrained language models such as GPT-3 (Brown et al., 2020) achieve remarkable few-shot performance. However, enormous amounts compute are required for training and applying big models, resulting in a large carbon footprint making it difficult researchers practitioners use them. We show that performance similar can be obtained with much “greener” their parameter count is several orders magnitude smaller. This achieved by converting textual inputs into cloze questions contain task description, combined gradient-based optimization; exploiting unlabeled data gives further improvements. identify key factors successful natural understanding small models."
https://openalex.org/W2105103432,https://doi.org/10.3115/v1/p15-2017,Language Models for Image Captioning: The Quirks and What Works,2015,"Two recent approaches have achieved state-of-the-art results in image captioning. The first uses a pipelined process where set of candidate words is generated by convolutional neural network (CNN) trained on images, and then maximum entropy (ME) language model used to arrange these into coherent sentence. second the penultimate activation layer CNN as input recurrent (RNN) that generates caption sequence. In this paper, we compare merits different modeling for time using same state-ofthe-art input. We examine issues approaches, including linguistic irregularities, repetition, data overlap. By combining key aspects ME RNN methods, achieve new record performance over previously published benchmark COCO dataset. However, gains see BLEU do not translate human judgments."
https://openalex.org/W2190333735,https://doi.org/10.1016/j.jbi.2015.07.020,Annotating longitudinal clinical narratives for de-identification: The 2014 i2b2/UTHealth corpus,2015,"Display Omitted De-identification shared task for longitudinal clinical records.Protected Health Information in records replaced with realistic surrogates.First corpus of its kind available distribution.Used Track 1 the 2014 i2b2/UTHealth NLP task. The natural language processing featured a track focused on de-identification medical records. For this track, we de-identified set 1304 describing 296 patients. This was under broad interpretation HIPAA guidelines using double-annotation followed by arbitration, rounds sanity checking, and proof reading. average token-based F1 measure annotators compared to gold standard 0.927. resulting annotations were used both de-identify data All annotated private health information surrogates automatically then read over corrected manually. is first made research. task, during which systems achieved mean F-measure 0.872 maximum 0.964 entity-based micro-averaged evaluations."
https://openalex.org/W2799072540,https://doi.org/10.18653/v1/p18-2077,Simpler but More Accurate Semantic Dependency Parsing,2018,"While syntactic dependency annotations concentrate on the surface or functional structure of a sentence, semantic aim to capture between-word relationships that are more closely related meaning using graph-structured representations. We extend LSTM-based parser Dozat and Manning (2017) train generate these graph structures. The resulting system its own achieves state-of-the-art performance, beating previous, substantially complex by 0.6% labeled F1. Adding linguistically richer input representations pushes margin even higher, allowing us beat it 1.9%"
https://openalex.org/W2946609015,https://doi.org/10.18653/v1/p19-1472,HellaSwag: Can a Machine Really Finish Your Sentence?,2019,"Recent work by Zellers et al. (2018) introduced a new task of commonsense natural language inference: given an event description such as “A woman sits at piano,” machine must select the most likely followup: “She sets her fingers on keys.” With introduction BERT, near human-level performance was reached. Does this mean that machines can perform human level inference? In paper, we show inference still proves difficult for even state-of-the-art models, presenting HellaSwag, challenge dataset. Though its questions are trivial humans (>95% accuracy), models struggle (<48%). We achieve via Adversarial Filtering (AF), data collection paradigm wherein series discriminators iteratively adversarial set machine-generated wrong answers. AF to be surprisingly robust. The key insight is scale up length and complexity dataset examples towards critical ‘Goldilocks’ zone generated text ridiculous humans, yet often misclassified models. Our construction resulting difficulty, sheds light inner workings deep pretrained More broadly, it suggests path forward NLP research, in which benchmarks co-evolve with evolving way, so present ever-harder challenges."
https://openalex.org/W2962749380,https://doi.org/10.1109/cvpr.2016.320,Seeing through the Human Reporting Bias: Visual Classifiers from Noisy Human-Centric Labels,2016,"When human annotators are given a choice about what to label in an image, they apply their own subjective judgments on ignore and mention. We refer these noisy human-centric annotations as exhibiting reporting bias. Examples of such include image tags keywords found photo sharing sites, or datasets containing captions. In this paper, we use for learning visually correct classifiers. Such do not consistent vocabulary, miss significant amount the information present however, demonstrate that noise exhibits structure can be modeled. propose algorithm decouple bias from grounded labels. Our results highly interpretable what's versus worth saying. algorithm's efficacy along variety metrics datasets, including MS COCO Yahoo Flickr 100M.We show improvements over traditional algorithms both classification captioning, doubling performance existing methods some cases."
https://openalex.org/W2963251942,https://doi.org/10.18653/v1/p16-1160,A Character-level Decoder without Explicit Segmentation for Neural Machine Translation,2016,"The existing machine translation systems, whether phrase-based or neural, have relied almost exclusively on word-level modelling with explicit segmentation. In this paper, we ask a fundamental question: can neural generate character sequence without any segmentation? To answer question, evaluate an attention-based encoder‐ decoder subword-level encoder and character-level four language pairs‐En-Cs, En-De, En-Ru En-Fi‐ using the parallel corpora from WMT’15. Our experiments show that models outperform ones all of pairs. Furthermore, ensembles state-of-the-art non-neural systems En-Cs, En-De En-Fi perform comparably En-Ru."
https://openalex.org/W2963546833,https://doi.org/10.18653/v1/w16-0106,Neural Generative Question Answering,2016,"This paper presents an end-to-end neural network model, named Neural Generative Question Answering (GENQA), that can generate answers to simple factoid questions, based on the facts in a knowledge-base. More specifically, model is built encoder-decoder framework for sequence-to-sequence learning, while equipped with ability enquire knowledge-base, and trained corpus of question-answer pairs, their associated triples Empirical study shows proposed effectively deal variations questions answers, right natural by referring The experiment question answering demonstrates outperform embedding-based QA as well dialogue same data."
https://openalex.org/W2963924212,https://doi.org/10.18653/v1/d17-1314,DOC: Deep Open Classification of Text Documents,2017,"Traditional supervised learning makes the closed-world assumption that classes appeared in test data must have training. This also applies to text or classification. As is used increasingly dynamic open environments where some new/test documents may not belong any of training classes, identifying these novel during classification presents an important problem. problem called open-world paper proposes a deep based approach. It outperforms existing state-of-the-art techniques dramatically."
https://openalex.org/W2105637130,https://doi.org/10.1093/jamia/ocv034,Toward high-throughput phenotyping: unbiased automated feature extraction and selection from knowledge sources,2015,"Analysis of narrative (text) data from electronic health records (EHRs) can improve population-scale phenotyping for clinical and genetic research. Currently, selection text features algorithms is slow laborious, requiring extensive iterative involvement by domain experts. This paper introduces a method to develop in an unbiased manner automatically extracting selecting informative features, which be comparable expert-curated ones classification accuracy.Comprehensive medical concepts were collected publicly available knowledge sources automated, fashion. Natural language processing (NLP) revealed the occurrence patterns these EHR notes, enabled phenotype classification. When combined with additional codified penalized logistic regression model was trained classify target phenotype.The authors applied our identify patients rheumatoid arthritis coronary artery disease cases among those large multi-institutional EHR. The area under receiver operating characteristic curves (AUC) classifying RA CAD using models automated 0.951 0.929, respectively, compared AUCs 0.938 0.929 features.Models NLP selected through unbiased, procedure achieved or slightly higher accuracy than features. majority interpretable.The proposed feature extraction method, generating highly accurate improved efficiency, significant step toward high-throughput phenotyping."
https://openalex.org/W2295584157,https://doi.org/10.3115/v1/n15-1028,Deep Multilingual Correlation for Improved Word Embeddings,2015,"Word embeddings have been found useful for many NLP tasks, including part-of-speech tagging, named entity recognition, and parsing. Adding multilingual context when learning can improve their quality, example via canonical correlation analysis (CCA) on embeddingsfromtwo languages. In this paper, we extend idea to learn deep non-linear transformations of word the two languages, using recently proposed analysis. The resulting embeddings, evaluated multiple bigram similarity consistently over monolingual transformed with linear CCA."
https://openalex.org/W2951211142,https://doi.org/10.18653/v1/p19-1213,Ranking Generated Summaries by Correctness: An Interesting but Challenging Application for Natural Language Inference,2019,"While recent progress on abstractive summarization has led to remarkably fluent summaries, factual errors in generated summaries still severely limit their use practice. In this paper, we evaluate produced by state-of-the-art models via crowdsourcing and show that such occur frequently, particular with more models. We study whether textual entailment predictions can be used detect if they reduced reranking alternative predicted summaries. That leads an interesting downstream application for our experiments, find out-of-the-box trained NLI datasets do not yet offer the desired performance task therefore release annotations as additional test data future extrinsic evaluations of NLI."
https://openalex.org/W2952594430,https://doi.org/10.18653/v1/p19-1527,Neural Architectures for Nested NER through Linearization,2019,"We propose two neural network architectures for nested named entity recognition (NER), a setting in which entities may overlap and also be labeled with more than one label. encode the labels using linearized scheme. In our first proposed approach, are modeled as multilabels corresponding to Cartesian product of standard LSTM-CRF architecture. second one, NER is viewed sequence-to-sequence problem, input sequence consists tokens output labels, hard attention on word whose label being predicted. The methods outperform state art four corpora: ACE-2004, ACE-2005, GENIA Czech CNEC. enrich recently published contextual embeddings: ELMo, BERT Flair, reaching further improvements corpora. addition, we report flat state-of-the-art results CoNLL-2002 Dutch Spanish CoNLL-2003 English."
