{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1YIME0qmzf2dm3MAcQURmcBqtOFEnxsjc",
      "authorship_tag": "ABX9TyNQZt6y8iZ2nI6BZwnNtFG8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AI-Growth-Lab/SciNerTopic/blob/main/notebooks/Sci_NERTopic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sci-NERTopic\n",
        "\n",
        "This notebook is a friendly demo of the Sci-NERTopic modeling technique finetuned for the analysis of scientific documents with transformers. It is optimized to work with document abstracts. You should be able to run this notebook on a free Google Colab GPU instance. It is inspired by BERTopic and adopts the use of Sentence Transformers in combination with UMAP and HDBSCAN. In addition, a fine-tuned Named Entity Recognition (NER) model - retrained SciBert on the SciERC corpus - is used to extract various classes scientific keywords (in this example `Issue` and `Method` from analysed abstracts.\n",
        "\n",
        "The techniques performs following stepps:\n",
        "\n",
        "- Load up NER and SBERT models (from HuggingFace)\n",
        "- Extract and aggregate NER keywords\n",
        "- Embed all documents using SBERT\n",
        "- Reduce dimensionality with UMAP\n",
        "- Cluster embeddings with HDBSCAN\n",
        "- use c-TF-IDF with NER keywords\n",
        "\n",
        "In this notebook you can use either a pre-extracted dataset (~1000 documents about NLP extracted from OpenAlex), explore another subject area on OpenAlex (extract documents using their API) or upload your own data (e.g. extract from Web Of Science or Scopus)."
      ],
      "metadata": {
        "id": "jpdC7kbUl6s1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "0nLUHQZulyv5"
      },
      "outputs": [],
      "source": [
        "#@title ##Install and import requirements\n",
        "\n",
        "!pip install -U sentence-transformers simpletransformers umap-learn hdbscan -qqqq\n",
        "\n",
        "from simpletransformers.ner import NERModel, NERArgs\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "import itertools\n",
        "import io\n",
        "from google.colab import files\n",
        "\n",
        "import requests, json\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import altair as alt\n",
        "\n",
        "import umap\n",
        "import hdbscan\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "from gensim.models.tfidfmodel import TfidfModel\n",
        "from gensim.matutils import corpus2csc, corpus2dense\n",
        "\n",
        "import logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "transformers_logger = logging.getLogger(\"transformers\")\n",
        "transformers_logger.setLevel(logging.WARNING)\n",
        "\n",
        "\n",
        "def kwReshaper(column, end_tags):\n",
        "  \"\"\"\n",
        "  column = pd.column or iterable where ents are stored\n",
        "  end_tag = tuple containing comma-separated strings with end tags of NER\n",
        "  Extracts comma-separated keywords from CONLL format \n",
        "  as produced by SciBERT\n",
        "  \"\"\"\n",
        "  extracted_kws = []\n",
        "  for i in range(len(column)):\n",
        "\n",
        "    abs = column[i]\n",
        "    abs_gen = (_ for _ in abs)\n",
        "\n",
        "    keywords = []\n",
        "    term = []\n",
        "\n",
        "    try:\n",
        "      while abs_gen:\n",
        "        t = next(abs_gen)\n",
        "        if t[1] == 'O':\n",
        "          continue\n",
        "        elif t[1].endswith(end_tags):\n",
        "          while True:\n",
        "            term.append(t[0].strip(',.'))\n",
        "            t = next(abs_gen)\n",
        "            if t[1].startswith('B'):\n",
        "              term.append(t[0].strip(',.'))\n",
        "              keywords.append(' '.join(term))\n",
        "              term = []\n",
        "            if t[1] == 'O':\n",
        "              keywords.append(' '.join(term))\n",
        "              term = []\n",
        "              break\n",
        "    except StopIteration: \n",
        "      extracted_kws.append(keywords)\n",
        "      keywords = []\n",
        "      continue\n",
        "  return extracted_kws\n",
        "\n",
        "\n",
        "!mkdir outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Selecting Transformer Models"
      ],
      "metadata": {
        "id": "gwxIk0DPDvIN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ###Select and load NER model\n",
        "\n",
        "#@markdown `pretrained_ner_model_name_or_path` (huggingface or local). For this NER model, please keep the labels as specified.\n",
        "pretrained_ner_model_name_or_path = \"RJuro/SciNERTopic\" #@param {type:\"string\"}\n",
        "ner_labels = ['B-Material', 'O', 'B-OtherScientificTerm', 'I-OtherScientificTerm', 'B-Generic', 'B-Method', 'I-Method', 'B-Task', 'I-Task', 'I-Material', 'B-Metric', 'I-Metric', 'I-Generic'] #@param\n",
        "\n",
        "model = NERModel(\n",
        "    \"bert\", pretrained_ner_model_name_or_path, labels=ner_labels\n",
        ")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "AsWx8TQQ4-9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ###Select and load sentence transformer model\n",
        "\n",
        "#@markdown `pretrained_sbert_model_name_or_path` (huggingface or local). We recommend `allenai-specter`for scientific documents. Consider `AI-Growth-Lab/PatentSBERTa`for patent documents.  [Read more here](https://www.sbert.net/docs/pretrained_models.html) about available pretrained models.\n",
        "pretrained_sbert_model_name_or_path = \"allenai-specter\" #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "model_st = SentenceTransformer(pretrained_sbert_model_name_or_path)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Gnax6WYJkAP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Data and Selecting Key Columns\n",
        "\n",
        "This notebook accepts any tabular data loaded as a `pandas.DataFrame`.\n",
        "You need one column with the documents to analyse e.g. `Abstract`. Additionally, consider having a column with `Title` and publication `Year` for your documents."
      ],
      "metadata": {
        "id": "rrRxUvzEEF5p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load remote file - dataframe of 247 publications records on NLP research from Openalex\n",
        "data = pd.read_csv('https://raw.githubusercontent.com/AI-Growth-Lab/SciNerTopic/main/data/nlp_openalex.csv')"
      ],
      "metadata": {
        "id": "tRJiy5RtuRyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load Data from OpenAlex\n",
        "\n",
        "#@markdown You can check out the list of concepts witht heir IDs [here](https://docs.google.com/spreadsheets/d/1LBFHjPt4rj_9r0t0TTAlT68NwOtNH8Z21lBMsJDMoZg/edit#gid=575855905), e.g., NLP c204321447 \n",
        "# specify endpoint\n",
        "endpoint = 'works'\n",
        "concept = \"'c204321447'\" #@param {type:\"string\"}\n",
        "oa = True #@param {type:\"boolean\"}\n",
        "nDocs = 2302 #@param {type:\"slider\", min:200, max:3000, step:1}\n",
        "from_pub_date = \"2017-01-01\" #@param {type:\"date\"}\n",
        "#@markdown Enter your email for API call to OpenAlex\n",
        "email = 'test@test.com'#@param {type:\"string\"} \n",
        "\n",
        "\n",
        "def OA(oa):\n",
        "  if True:\n",
        "    return 'true'\n",
        "  else:\n",
        "    return 'false'\n",
        "\n",
        "\n",
        "\n",
        "oa_str = OA(oa)\n",
        "\n",
        "# build the 'filter' parameter\n",
        "filters = \",\".join((\n",
        "    f'concepts.id:{concept}',\n",
        "    'is_paratext:false', \n",
        "    f'from_publication_date:{from_pub_date}',\n",
        "    f'is_oa:{oa_str}'\n",
        "))\n",
        "\n",
        "# put the URL together\n",
        "filtered_works_url = f'https://api.openalex.org/{endpoint}?mailto={email}&filter={filters}'\n",
        "print(f'complete URL with filters:\\n{filtered_works_url}')\n",
        "\n",
        "\n",
        "paging_param = 'per-page=100&cursor=*'\n",
        "\n",
        "works_query = f'{filtered_works_url}&{paging_param}'\n",
        "\n",
        "response = requests.get(works_query)\n",
        "meta = json.loads(response.text)['meta']\n",
        "next_cursor = meta['next_cursor']\n",
        "results_alx = json.loads(response.text)['results']\n",
        "\n",
        "\n",
        "cycles = math.floor((meta['count'] - 100) / meta['per_page'])+1\n",
        "if cycles > 30:\n",
        "  cycles = int(nDocs/100)\n",
        "\n",
        "df_input = []\n",
        "\n",
        "for result in results_alx:\n",
        "  if result['abstract_inverted_index']:\n",
        "    abs = ' '.join(result['abstract_inverted_index'].keys())\n",
        "    df_input.append((result['id'], result['doi'],result['title'],result['publication_year'],abs))\n",
        "\n",
        "for cycle in range(cycles):\n",
        "  cycle_query = f'{works_query[:-1]}{next_cursor}'\n",
        "  response = requests.get(cycle_query)\n",
        "  meta = json.loads(response.text)['meta']\n",
        "  next_cursor = meta['next_cursor']\n",
        "  results_alx = json.loads(response.text)['results']\n",
        "  for result in results_alx:\n",
        "    if result['abstract_inverted_index']:\n",
        "      abs = ' '.join(result['abstract_inverted_index'].keys())\n",
        "      df_input.append((result['id'], result['doi'],result['title'],result['publication_year'],abs))\n",
        "\n",
        "\n",
        "data = pd.DataFrame(df_input, columns=['id','doi','title','publication_year','abstract'])\n",
        "\n",
        "print(f'Downloaded {str(len(data))} documents')\n",
        "\n",
        "data.head()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "xcLgDZ_oEuoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title load local file (optional)\n",
        "#@markdown Run this cell to upload local CSV file for analysis. Files will be deleted after restart of the notebook\n",
        "#@markdown: specify the separator used in your CSV-\n",
        "loc_sep = \",\" #@param {type:\"string\"}\n",
        "\n",
        "data = files.upload()\n",
        "key = list(data.keys())[0]\n",
        "data = pd.read_csv(io.BytesIO(data[key]), sep=loc_sep)\n",
        "\n",
        "data.info()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "XWjEpgSb5vEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Inspect DF fields\n",
        "\n",
        "data.info()"
      ],
      "metadata": {
        "id": "iMnJmYoAZSY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ### Select columns containing the text corpus, title and years\n",
        "\n",
        "#@markdown Dataframe column containing the text\n",
        "txt_col = \"abstract\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ### Optional\n",
        "#@markdown Dataframe column containing a title\n",
        "tit_col = \"title\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown Dataframe column containing publication year\n",
        "y_col = \"publication_year\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown Dataframe column containing citation counts\n",
        "cit_col = \"Cited by\" #@param {type:\"string\"}\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "s07Ry7Y0DkRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Extract and append keywords categories\n",
        "\n",
        "data.dropna(subset=[txt_col], inplace=True)\n",
        "data.index = range(len(data))\n",
        "\n",
        "ixs = []\n",
        "txts = []\n",
        "\n",
        "for txt in data[txt_col]:\n",
        "  sents = sent_tokenize(txt)\n",
        "  ixs.append(len(sents))\n",
        "  txts.extend(sents)\n",
        "\n",
        "p, r = model.predict(txts)\n",
        "\n",
        "joiner = (_ for _ in p)\n",
        "\n",
        "abstracts = []\n",
        "n_predictions = []\n",
        "\n",
        "for i in ixs:\n",
        "  pred_1 = [next(joiner) for _ in range(i)]\n",
        "  pred_1 = list(itertools.chain(*pred_1))\n",
        "  abstracts.append(list(itertools.chain(*[pr.items() for pr in pred_1])))\n",
        "\n",
        "data['ents'] = abstracts\n",
        "data.index = range(len(data))\n",
        "\n",
        "extracted_kws = kwReshaper(data.ents, ('Method','OtherScientificTerm','Task'))\n",
        "extracted_kw_M = kwReshaper(data.ents, ('Method'))\n",
        "extracted_kw_T = kwReshaper(data.ents, ('Task'))\n",
        "extracted_kw_O = kwReshaper(data.ents, ('OtherScientificTerm'))\n",
        "\n",
        "data['ner-keywords'] = extracted_kws\n",
        "data['ner-keywords-Method'] = extracted_kw_M\n",
        "data['ner-keywords-Task'] = extracted_kw_T\n",
        "data['ner-keywords-OtherSci'] = extracted_kw_O"
      ],
      "metadata": {
        "cellView": "form",
        "id": "QUnFh0Wn2Njc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Create text embeddings \n",
        "embeddings = model_st.encode(data[txt_col], convert_to_tensor=True, show_progress_bar=True)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "8b5hD1iXOHnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown parameters for the topic model\n",
        "\n",
        "\n",
        "#@markdown [n_neighbors](https://umap-learn.readthedocs.io/en/latest/parameters.html#n-neighbors) controls how UMAP balances local versus global structure in the data. Low values of n_neighbors will force UMAP to concentrate on very local structure, which in turn will lead to a higher number of specific topics but potentially also higher numbers of unassigned observations. Consider increasing this parameter with corpus size.\n",
        "n_neighbors = 5 #@param {type:\"slider\", min:2, max:25, step:1}\n",
        "\n",
        "#@markdown [min_cluster_size](https://hdbscan.readthedocs.io/en/latest/parameter_selection.html#selecting-min-cluster-size) controls the minimum size of topics identified by the HDBSAN clustering. Higher values may be useful for larger corpus sizes. However, they can lead to higher numbers of unassigned values.\n",
        "min_cluster_size = 15 #@param {type:\"slider\", min:2, max:100, step:1}\n",
        "\n",
        "#@markdown [min_samples](https://hdbscan.readthedocs.io/en/latest/parameter_selection.html#selecting-min-samples) The larger the value of `min_samples` you provide, the more conservative the clustering – more points will be declared as noise. Thus lower numbers will result in fewer unassigned documents, but potentially less precisely defined topics. \n",
        "min_samples = 5 #@param {type:\"slider\", min:1, max:5, step:1}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "lqcRDloPOKKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Execute dimensionality reduction and clustering\n",
        "\n",
        "kws_lab = ['ner-keywords', 'ner-keywords-Method', 'ner-keywords-Task']\n",
        "eid = []\n",
        "df_outs = []\n",
        "\n",
        "\n",
        "umap_reducer_abs = umap.UMAP(random_state=42, n_components=2, n_neighbors=n_neighbors)\n",
        "embeddings_abs = embeddings.detach().cpu().numpy()\n",
        "embeddings_abs_red = umap_reducer_abs.fit_transform(embeddings_abs)\n",
        "\n",
        "for lab in kws_lab:\n",
        "  data[lab] = data[lab].map(lambda t: [x.lower() for x in t])\n",
        "\n",
        "clusterer_abs = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, min_samples=min_samples)\n",
        "clusterer_abs.fit(embeddings_abs_red)\n",
        "clusters_col = list(set(clusterer_abs.labels_))\n",
        "\n",
        "data['topic'] = clusterer_abs.labels_\n",
        "cluster_share = data['topic'].value_counts(normalize=True)\n",
        "cluster_share = [cluster_share[i] for i in clusters_col]\n",
        "\n",
        "cluster_size = data['topic'].value_counts(normalize=False)\n",
        "cluster_size = [cluster_size[i] for i in clusters_col]"
      ],
      "metadata": {
        "cellView": "form",
        "id": "isvx5kBEGtmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Keyword Filtering and Number of Keywords\n",
        "\n",
        "\n",
        "#@markdown how often should keywords appear ovall to be considered for topic-labeling\n",
        "no_below = 3 #@param {type:\"slider\", min:2, max:100, step:1}\n",
        "\n",
        "#@markdown shore of documents in the corpus containing a term. General keywords appearing in too many documents will be discounted but may introduce ambiguity. Lower numbers will lead to fewer general keywords.\n",
        "no_above = 0.1 #@param {type:\"slider\", min:0.1, max:0.9, step:0.05}\n",
        "\n",
        "#@markdown Cut-off number of Top Keywords to keep as topic desctiptors\n",
        "n_kws = 10 #@param {type:\"slider\", min:3, max:100, step:1}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "oDtKZbnegdOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Rank NER keywords and display topic stats\n",
        "\n",
        "top_kw = []\n",
        "for lab in kws_lab:\n",
        "  # Generate a dictionary and filter\n",
        "  dictionary = Dictionary(data[lab])\n",
        "  dictionary.filter_extremes(no_below=no_below, no_above=no_below)\n",
        "\n",
        "  # construct corpus using this dictionary\n",
        "  corpus = [dictionary.doc2bow(word) for word in [doc for doc in data[lab]]]\n",
        "  # Create and fit a new TfidfModel using the corpus: tfidf\n",
        "  tfidf = TfidfModel(corpus)\n",
        "  # transform corpus to TFIDF\n",
        "  corpus_tfidf = tfidf[corpus]\n",
        "  # Let's check out the topics by getting \"top-tfidf\" for the different clusters (and we need to transponse)\n",
        "  tfidf_matrix = corpus2dense(corpus_tfidf, len(dictionary)).T\n",
        "\n",
        "  top_kw_i = []\n",
        "  for i in clusters_col:\n",
        "    cluster_index = data[data['topic'] == i].index\n",
        "    topk = np.flip(np.argsort(np.sum(tfidf_matrix[cluster_index,:], axis=0)))[:n_kws]\n",
        "    top_kw_i.append([dictionary[x] for x in topk])\n",
        "    #print(str(i) + str([dictionary[x] for x in topk]))\n",
        "  top_kw.append(top_kw_i)\n",
        "\n",
        "df_out = pd.DataFrame(zip(clusters_col, cluster_size, cluster_share, top_kw[0],top_kw[1],top_kw[2]))\n",
        "df_out.columns = ['topic','topic_size', 'topic_size_pct', 'top_kw_all', 'top_kw_method', 'top_kw_issue']\n",
        "\n",
        "\n",
        "topic_centers = []\n",
        "for topic in df_out['topic'].values:\n",
        "  t_ix = data[data['topic']==topic].index\n",
        "  topic_centers.append(np.median(embeddings_abs_red[t_ix], axis=0))\n",
        "topic_centers = np.vstack(topic_centers)\n",
        "\n",
        "\n",
        "emb_df = pd.DataFrame(np.hstack([clusterer_abs.labels_.reshape(-1,1),embeddings_abs_red]), columns=['topic','x','y'])\n",
        "topic_centers_df = pd.DataFrame(topic_centers, columns=['x_med','y_med'])\n",
        "topic_centers_df['topic'] = df_out['topic'].values\n",
        "emb_df = pd.merge(emb_df,topic_centers_df, how='left')\n",
        "emb_df['dist_topic_cent'] = emb_df.apply(lambda t: np.linalg.norm(np.array([t['x'],t['y']]) - np.array([t['x_med'],t['y_med']])), axis=1)\n",
        "\n",
        "data['dist_topic_cent'] = emb_df['dist_topic_cent']\n",
        "\n",
        "data.to_csv('outputs/data.csv', index=None)\n",
        "df_out.to_csv('outputs/df_out.csv', index=None)\n",
        "\n",
        "df_out"
      ],
      "metadata": {
        "id": "um_Xyc7Zma-y",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Plot Topic-Map\n",
        "df_out['desc'] = [', '.join(t) for t in df_out['top_kw_all']]\n",
        "df_out['x'] = topic_centers[:,0]\n",
        "df_out['y'] = topic_centers[:,1]\n",
        "\n",
        "df_out_viz = df_out[~df_out['topic'].isin([0,-1])]\n",
        "\n",
        "# plot\n",
        "chart_map = alt.Chart(df_out_viz).mark_circle(size=60).encode(\n",
        "    x='x',\n",
        "    y='y',\n",
        "    size=alt.Size(\"topic_size_pct:Q\", scale=alt.Scale(range=[5/ df_out_viz.topic_size_pct.min(), 1000/ df_out_viz.topic_size_pct.max()]), title='Topic Size (share)'),\n",
        "    tooltip=[alt.Tooltip('topic:Q', title='Topic N'),\n",
        "        alt.Tooltip('desc:O', title='Top NER Keywords'),\n",
        "        alt.Tooltip('topic_size_pct:Q', title='Topic Size (share)')]\n",
        ").properties(\n",
        "    title=f'NERTopic-mapplot - {len(df_out_viz)} Topics',\n",
        "    width=800,\n",
        "    height=600\n",
        ").interactive()\n",
        "\n",
        "chart_map.save('outputs/map_plot.html')\n",
        "\n",
        "chart_map"
      ],
      "metadata": {
        "cellView": "form",
        "id": "UEPrsiMHkcT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown print 10 closest article titles wrt topic\n",
        "\n",
        "top_p = 1 #@param {type:\"number\"}\n",
        "\n",
        "for title in data.query(f'topic == {top_p}').sort_values('dist_topic_cent')[tit_col][:10]:\n",
        "  print(title + '\\n')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "6dMnHtlKzpt3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown print title of 10 most cited articles within topic\n",
        "\n",
        "top_p = 1 #@param {type:\"number\"}\n",
        "\n",
        "for row in data.query(f'topic == {top_p}').sort_values(cit_col, ascending=False)[:10].iterrows():\n",
        "  print(row[1][tit_col]+ '; N Citations ' + str(row[1][cit_col]) + '\\n')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "7NKJcUv_2rl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Plot topics over time (punchcard plot)\n",
        "\n",
        "#@markdown Plot width\n",
        "dyn_p_width = 800 #@param {type:\"number\"}\n",
        "\n",
        "ctab1 = pd.crosstab(data['topic'], data[y_col], normalize='columns').stack().reset_index()\n",
        "ctab1.columns = ['topic','year','YearlyShare']\n",
        "ctab2 = pd.crosstab(data['topic'], data[y_col], normalize='index')\n",
        "ctab2 = ctab2.stack().reset_index()\n",
        "ctab2.columns = ['topic','year','TopicShare']\n",
        "ctab2['YearlyShare'] = ctab1['YearlyShare']\n",
        "ctab2 = ctab2.merge(df_out[['topic','top_kw_all','topic_size_pct', 'topic_size']], left_on='topic', right_on='topic')\n",
        "ctab2['topic_size_pct'] = ctab2['topic_size_pct']*100\n",
        "ctab2['desc'] = [', '.join(t) for t in ctab2['top_kw_all']]\n",
        "\n",
        "dyanamic_chart = alt.Chart(ctab2).mark_point(filled=True).encode(\n",
        "    y=alt.X('topic:O',title='Topic Number'),\n",
        "    x=alt.Y('year:O', title='Year'),\n",
        "    color=alt.Color('max(YearlyShare):Q', scale=alt.Scale(scheme=\"inferno\")),\n",
        "    size=alt.Size(\"TopicShare:Q\", scale=alt.Scale(range=[0, 600])),\n",
        "    order=alt.Order(\"TopicShare:Q\", sort=\"descending\"),\n",
        "    tooltip=[\n",
        "        alt.Tooltip('TopicShare:Q', title='topic distr. over time'),\n",
        "        alt.Tooltip('topic:O', title='Topic'),\n",
        "        alt.Tooltip('year:O', title='Year'),\n",
        "        alt.Tooltip('topic_size:Q', title='Topic Size (N)'),\n",
        "        alt.Tooltip('topic_size_pct:Q', title='Topic Size in %'),\n",
        "        alt.Tooltip('desc:O', title='Topic descr.')\n",
        "    ]\n",
        ").properties(width=800)\n",
        "\n",
        "dyanamic_chart.save('outputs/dynamic_plot.html')\n",
        "dyanamic_chart\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "q9ZEjB0g7XzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### Download Processed Data, Summaries and Plots\n",
        "\n",
        "!zip -r outputs.zip outputs\n",
        "\n",
        "files.download('outputs.zip') "
      ],
      "metadata": {
        "cellView": "form",
        "id": "0LZbgyEAMpWP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}